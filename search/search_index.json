{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Icechunk - Open-source, cloud-native transactional tensor storage engine","text":"<p>Home</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Home / contributing</p>"},{"location":"contributing/#contributing","title":"Contributing","text":"<p>\ud83d\udc4b Hi! Thanks for your interest in contributing to Icechunk!</p> <p>Icechunk is an open source (Apache 2.0) project and welcomes contributions in the form of:</p> <ul> <li>Usage questions - open a GitHub issue</li> <li>Bug reports - open a GitHub issue</li> <li>Feature requests - open a GitHub issue</li> <li>Documentation improvements - open a GitHub pull request</li> <li>Bug fixes and enhancements - open a GitHub pull request</li> </ul>"},{"location":"contributing/#development","title":"Development","text":""},{"location":"contributing/#python-development-workflow","title":"Python Development Workflow","text":"<p>Create / activate a virtual environment:</p> VenvConda / Mamba <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>mamba create -n icechunk python=3.12 rust zarr\nmamba activate icechunk\n</code></pre> <p>Install <code>maturin</code>:</p> <pre><code>pip install maturin\n</code></pre> <p>Build the project in dev mode:</p> <pre><code>maturin develop\n</code></pre> <p>or build the project in editable mode:</p> <pre><code>pip install -e icechunk@.\n</code></pre>"},{"location":"contributing/#rust-development-worflow","title":"Rust Development Worflow","text":"<p>TODO</p>"},{"location":"contributing/#roadmap","title":"Roadmap","text":"<p>The initial release of Icechunk is just the beginning. We have a lot more planned for the format and the API. </p>"},{"location":"contributing/#core-format","title":"Core format","text":"<p>The core format is where we\u2019ve put most of our effort to date and we plan to continue work in this area. Leading up to the 1.0 release, we will be focused on stabilizing data structures for snapshots, chunk manifests, attribute files and references. We\u2019ll also document and add more mechanisms for on-disk format evolution. The intention is to guarantee that any new version of Icechunk can always read repositories generated with any previous versions. We expect to evolve the spec and the Rust implementation as we stabilize things.</p>"},{"location":"contributing/#optimizations","title":"Optimizations","text":"<p>While the initial performance benchmarks of Icechunk are very encouraging, we know that we have only scratched the surface of what is possible. We are looking forward to investing in a number of optimizations that will really make Icechunk fly!</p> <ul> <li>Chunk compaction on write</li> <li>Request batching and splitting</li> <li>Manifest compression and serialization improvements</li> <li>Manifest split heuristics</li> <li>Bringing parts of the codec pipeline to the Rust side</li> <li>Better caching, in memory and optionally on local disk</li> <li>Performance statistics, tests, baseline and evolution</li> </ul>"},{"location":"contributing/#other-utilities","title":"Other Utilities","text":"<p>On top of the foundation of the Icechunk format, we are looking to build a suite of other utilities that operate on data stored in Icechunk. Some examples:</p> <ul> <li>Garbage collection - version controlled data has the potential to accumulate data that is no longer needed but is still included in the store. A garbage collection process will allow users to safely cleanup data from old versions of an Icechunk dataset.</li> <li>Chunk compaction - data written by Zarr may result in many small chunks in object storage. A chunk compaction service will allow users to retroactively compact small chunks into larger objects (similar to Zarr\u2019s sharding format), resulting in potential performance improvements and fewer objects in storage.</li> <li>Manifest optimization - knowing how the data is queried would allow to optimize the shape and splits of the chunk manifests, in such a way as to minimize the amount of data needed to execute the most frequent queries.</li> </ul>"},{"location":"contributing/#zarr-related","title":"Zarr-related","text":"<p>We\u2019re very excited about a number of extensions to Zarr that would work great with Icechunk. </p> <ul> <li>Variable length chunks</li> <li>Chunk-level statistics</li> </ul>"},{"location":"contributing/#miscellaneous","title":"Miscellaneous","text":"<p>There\u2019s much more than what we\u2019ve written above on the roadmap. Some examples:</p> <ul> <li>Distributed write support with <code>dask.array</code></li> <li>Multi-language support (R, Julia, \u2026)</li> <li>Exposing high level API (groups and arrays) to Python users</li> <li>Make more details of the format accessible through configuration</li> <li>Improve Xarray backend to integrate more directly with Icechunk</li> </ul>"},{"location":"faq/","title":"Icechunk - Frequenctly Asked Questions","text":"<p>Home / faq</p>"},{"location":"faq/#faq","title":"FAQ","text":""},{"location":"faq/#why-was-icechunk-created","title":"Why was Icechunk created?","text":"<p>Icechunk was created by Earthmover as the open-source format for its cloud data platform Arraylake.</p> <p>Icechunk builds on the successful Zarr project. Zarr is a great foundation for storing and querying large multidimensional array data in a flexible, scalable way. But when people started using Zarr together with cloud object storage in a collaborative way, it became clear that Zarr alone could not offer the sort of consistency many users desired. Icechunk makes Zarr work a little bit more like a database, enabling different users / processes to safely read and write concurrently, while still only using object storage as a persistence layer.</p> <p>Another motivation for Icechunk was the success of Kerchunk. The Kerchunk project showed that it was possible to map many existing archival formats (e.g. HDF5, NetCDF, GRIB) to the Zarr data model without actually rewriting any bytes, by creating \"virtual\" Zarr datasets referencing binary chunks inside other files. Doing this at scale requires tracking millions of \"chunk references.\" Icechunk's storage model allows for these virtual chunks to be stored seamlessly alongside native Zarr chunks.</p> <p>Finally, Icechunk provides a universal I/O layer for cloud object storage, implementing numerous performance optimizations designed to accelerate data-intensive applications.</p> <p>Solving these problems in one go via a powerful, open-source, Rust-based library will bring massive benefits to the cloud-native scientific data community.</p>"},{"location":"faq/#where-does-the-name-icechunk-come-from","title":"Where does the name \"Icechunk\" come from?","text":"<p>Icechunk was inspired partly by Apache Iceberg, a popular cloud-native table format. However, instead of storing tabular data, Icechunk stores multidimensional arrays, for which the individual unit of storage is the chunk.</p>"},{"location":"faq/#when-should-i-use-icechunk","title":"When should I use Icechunk?","text":"<p>Here are some scenarios where it makes sense to use Icechunk:</p> <ul> <li>You want to store large, dynamically evolving multi-dimensional array (a.k.a. tensor) in cloud object storage.</li> <li>You want to allow multiple uncoordinated processes to access your data at the same time (like a database).</li> <li>You want to be able to safely roll back failed updates or revert Zarr data to an earlier state.</li> <li>You want to use concepts from data version control (e.g. tagging, branching, snapshots) with Zarr data.</li> <li>You want to achieve cloud-native performance on archival file formats (HDF5, NetCDF, GRIB) by exposing them as virtual Zarr datasets and need to store chunk references in a a robust, scalable, interoperable way.</li> <li>You want to get the best possible performance for reading / writing tensor data in AI / ML workflows.</li> </ul>"},{"location":"faq/#what-are-the-downsides-to-using-icechunk","title":"What are the downsides to using Icechunk?","text":"<p>As with all things in technology, the benefits of Icechunk come with some tradeoffs:</p> <ul> <li>There may be slightly higher cold-start latency to opening Icechunk datasets compared with regular Zarr.</li> <li>The on-disk format is less transparent than regular Zarr.</li> <li>The process for distributed writes is more complex to coordinate.</li> </ul> <p>Warning</p> <p>Another downside of Icechunk in its current state is its immaturity. The library is very new, likely contains bugs, and is not recommended for production usage at this point in time.</p>"},{"location":"faq/#what-is-icechunks-relationship-to-zarr","title":"What is Icechunk's relationship to Zarr?","text":"<p>The Zarr format and protocol is agnostic to the underlying storage system (\"store\" in Zarr terminology) and communicates with the store via a simple key / value interface. Zarr tells the store which keys and values it wants to get or set, and it's the store's job to figure out how to persist or retrieve the required bytes.</p> <p>Most existing Zarr stores have a simple 1:1 mapping between Zarr's keys and the underlying file / object names. For example, if Zarr asks for a key call <code>myarray/c/0/0</code>, the store may just look up a key of the same name in an underlying cloud object storage bucket.</p> <p>Icechunk is a storage engine which creates a layer of indirection between the Zarr keys and the actual files in storage. A Zarr library doesn't have to know explicitly how Icechunk works or how it's storing data on disk. It just gets / sets keys as it would with any store. Icechunk figures out how to materialize these keys based on its storage schema.</p> <ul> <li> <p>Standard Zarr + Fsspec</p> <p>In standard Zarr usage (without Icechunk), fsspec sits between the Zarr library and the object store, translating Zarr keys directly to object store keys.</p> <pre><code>flowchart TD\n    zarr-python[Zarr Library] &lt;-- key / value--&gt; icechunk[fsspec]\n    icechunk &lt;-- key / value --&gt; storage[(Object Storage)]\n</code></pre> </li> <li> <p>Zarr + Icechunk</p> <p>With Icechunk, the Icechunk library intercepts the Zarr keys and translates them to the Icechunk schema, storing data in object storage using its own format. </p> <pre><code>flowchart TD\n    zarr-python[Zarr Library] &lt;-- key / value--&gt; icechunk[Icechunk Library]\n    icechunk &lt;-- icechunk data / metadata files --&gt; storage[(Object Storage)]\n</code></pre> </li> </ul> <p>Implementing Icechunk this way allows Icechunk's specification to evolve independently from Zarr's, maintaining interoperability while enabling rapid iteration and promoting innovation on the I/O layer.</p>"},{"location":"faq/#is-icechunk-part-of-the-zarr-spec","title":"Is Icechunk part of the Zarr Spec?","text":"<p>No. At the moment, the Icechunk spec is completely independent of the Zarr spec.</p> <p>In the future, we may choose to propose Icechunk as a Zarr extension. However, because it sits below Zarr in the stack, it's not immediately clear how to do that. </p>"},{"location":"faq/#should-i-implement-icechunk-on-my-own-based-on-the-spec","title":"Should I implement Icechunk on my own based on the spec?","text":"<p>No, we do not recommend implementing Icechunk independently of the existing Rust library. There are two reasons for this:</p> <ol> <li>The spec has not yet been stabilized and is still evolving rapidly.</li> <li>It's probably much easier to bind to the Rust library from your language of choice,    rather than re-implement from scratch.</li> </ol> <p>We welcome contributions from folks interested in developing Icechunk bindings for other languages!</p>"},{"location":"faq/#is-icechunk-stable","title":"Is Icechunk stable?","text":"<p>The Icechunk library is reasonably well-tested and performant. The Rust-based core library provides a solid foundation of correctness, safety, and speed.</p> <p>However, the actual on disk format is still evolving and may change from one alpha release to the next. Until Icechunk reaches v1.0, we can't commit to long-term stability of the on-disk format. This means Icechunk can't yet be used for production uses which require long-term persistence of data.</p> <p>\ud83d\ude05 Don't worry! We are working as fast as we can and aim to release v1.0 soon!</p>"},{"location":"faq/#is-icechunk-fast","title":"Is Icechunk fast?","text":"<p>We have not yet begun the process of optimizing Icechunk for performance. Our focus so far has been on correctness and delivering the features needed for full interoperability with Zarr and Xarray.</p> <p>However, preliminary investigations indicate that Icechunk is at least as fast as the existing Zarr / Dask / fsspec stack and in many cases achieves significantly lower latency and higher throughput. Furthermore, Icechunk achieves this without using Dask, by implementing its own asynchronous multithreaded I/O pipeline.</p>"},{"location":"faq/#how-does-icechunk-compare-to-x","title":"How does Icechunk compare to X?","text":""},{"location":"faq/#array-formats","title":"Array Formats","text":"<p>Array formats are file formats for storing multi-dimensional array (tensor) data. Icechunk is an array format. Here is how Icechunk compares to other popular array formats.</p>"},{"location":"faq/#hdf5","title":"HDF5","text":"<p>HDF5 (Hierarchical Data Format version 5) is a popular format for storing scientific data. HDF is widely used in high-performance computing.</p> <ul> <li> <p>Similarities</p> <p>Icechunk and HDF5 share the same data model: multidimensional arrays and metadata organized into a hierarchical tree structure. This data model can accomodate a wide range of different use cases and workflows.</p> <p>Both Icechunk and HDF5 use the concept of \"chunking\" to split large arrays into smaller storage units.</p> </li> <li> <p>Differences</p> <p>HDF5 is a monolithic file format designed first and foremost for POSIX filesystems. All of the chunks in an HDF5 dataset live within a single file. The size of an HDF5 dataset is limited to the size of a single file. HDF5 relies on the filesystem for consistency and is not designed for multiple concurrent yet uncoordinated readers and writers.</p> <p>Icechunk spreads chunks over many files and is designed first and foremost for cloud object storage. Icechunk can accommodate datasets of arbitrary size. Icechunk's optimistic concurrency design allows for safe concurrent access for uncoordinated readers and writers.</p> </li> </ul>"},{"location":"faq/#netcdf","title":"NetCDF","text":"<p>NetCDF (Network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.</p> <p>NetCDF4 uses HDF5 as its underlying file format. Therefore, the similarities and differences with Icechunk are fundamentally the same.</p> <p>Icechunk can accommodate the NetCDF data model. It's possible to write NetCDF compliant data in Icechunk using Xarray.</p>"},{"location":"faq/#zarr","title":"Zarr","text":"<p>Icechunk works together with Zarr. (See What is Icechunk's relationship to Zarr? for more detail.)</p> <p>Compared to regular Zarr (without Icechunk), Icechunk offers many benefits, including</p> <ul> <li>Serializable isolation of updates via transactions</li> <li>Data version control (snapshots, branches, tags)</li> <li>Ability to store references to chunks in external datasets (HDF5, NetCDF, GRIB, etc.)</li> <li>A Rust-optimized I/O layer for communicating with object storage</li> </ul>"},{"location":"faq/#cloud-optimized-geotiff-cog","title":"Cloud Optimized GeoTiff (CoG)","text":"<p>A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing \u200bHTTP GET range requests to ask for just the parts of a file they need.</p> <p>CoG has become very popular in the geospatial community as a cloud-native format for raster data. A CoG file contains a single image (possibly with multiple bands), sharded into chunks of an appropriate size. A CoG also contains \"overviews,\" lower resolution versions of the same data. Finally, a CoG contains relevant geospatial metadata regarding projection, CRS, etc. which allow georeferencing of the data.</p> <p>Data identical to what is found in a CoG can be stored in the Zarr data model and therefore in an Icechunk repo. Furthermore, Zarr / Icechunk can accommodate rasters of arbitrarily large size and facilitate massive-scale concurrent writes (in addition to reads); A CoG, in contrast, is limited to a single file and thus has limitations on scale and write concurrency.</p> <p>However, Zarr and Icechunk currently do not offer the same level of broad geospatial interoperability that CoG does. The GeoZarr project aims to change that.</p>"},{"location":"faq/#tiledb-embedded","title":"TileDB Embedded","text":"<p>TileDB Embedded is an innovative array storage format that bears many similarities to both Zarr and Icechunk. Like TileDB Embedded, Icechunk aims to provide database-style features on top of the array data model. Both technologies use an embedded / serverless architecture, where client processes interact directly with data files in storage, rather than through a database server. However, there are a number of difference, enumerated below.</p> <p>The following table compares Zarr + Icechunk with TileDB Embedded in a few key areas</p> feature Zarr + Icechunk TileDB Embedded Comment atomicity atomic updates can span multiple arrays and groups array fragments limited to a single array Icechunk's model allows a writer to stage many updates across interrelated arrays into a single transaction. concurrency and isolation serializable isolation of transactions eventual consistency While both formats enable lock-free concurrent reading and writing, Icechunk can catch (and potentially reject) inconsistent, out-of order updates. versioning snapshots, branches, tags linear version history Icechunk's data versioning model is closer to Git's. unit of storage chunk tile (basically the same thing) minimum write chunk cell TileDB allows atomic updates to individual cells, while Zarr requires writing an entire chunk. sparse arrays Zar + Icechunk do not currently support sparse arrays. virtual chunk references Icechunk enables references to chunks in other file formats (HDF5, NetCDF, GRIB, etc.), while TileDB does not. <p>Beyond this list, there are numerous differences in the design, file layout, and implementation of Icechunk and TileDB embedded which may lead to differences in suitability and performance for different workfloads.</p>"},{"location":"faq/#safetensors","title":"SafeTensors","text":"<p>SafeTensors is a format developed by HuggingFace for storing tensors (arrays) safely, in contrast to Python pickle objects.</p> <p>By the same criteria Icechunk and Zarr are also \"safe\", in that it is impossible to trigger arbitrary code execution when reading data.</p> <p>SafeTensors is a single-file format, like HDF5, SafeTensors optimizes for a simple on-disk layout that facilitates mem-map-based zero-copy reading in ML training pipleines, assuming that the data are being read from a local POSIX filesystem Zarr and Icechunk instead allow for flexible chunking and compression to optimize I/O against object storage.</p>"},{"location":"faq/#tabular-formats","title":"Tabular Formats","text":"<p>Tabular formats are for storing tabular data. Tabular formats are extremely prevalent in general-purpose data analytics but are less widely used in scientific domains.  The tabular data model is different from Icechunk's multidimensional array data model, and so a direct comparison is not always apt. However, Icechunk is inspired by many tabular file formats, and there are some notable similarities.</p>"},{"location":"faq/#apache-parquet","title":"Apache Parquet","text":"<p>Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides high performance compression and encoding schemes to handle complex data in bulk and is supported in many programming language and analytics tools.</p> <p>Parquet employs many of the same core technological concepts used in Zarr + Icechunk such as chunking, compression, and efficient metadata access in a cloud context. Both formats support a range of different numerical data types. Both are \"columnar\" in the sense that different columns / variables / arrays can be queried efficiently without having to fetch unwanted data from other columns. Both also support attaching arbitrary key-value metadata to variables. Parquet supports \"nested\" types like variable-length lists, dicts, etc. that are currently unsupported in Zarr (but may be possible in the future).</p> <p>In general, Parquet and other tabular formats can't be substituted for Zarr / Icechunk, due to the lack of multidimensional array support. On the other hand, tabular data can be modeled in Zarr / Icechunk in a relatively straightforward way: each column as a 1D array, and a table / dataframe as a group of same-sized 1D arrays.</p>"},{"location":"faq/#apache-iceberg","title":"Apache Iceberg","text":"<p>Iceberg is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, Hive and Impala to safely work with the same tables, at the same time.</p> <p>Iceberg is commonly used to manage many Parquet files as a single table in object storage.</p> <p>Iceberg was influential in the design of Icechunk. Many of the spec core requirements are similar to Iceberg. Specifically, both formats share the following properties:</p> <ul> <li>Files written to object storage immutably</li> <li>All data and metadata files are tracked explicitly by manifests</li> <li>Similar mechanism for staging snapshots and committing transactions</li> <li>Support for branches and tags</li> </ul> <p>However, unlike Iceberg, Icechunk does not require an external catalog to commit transactions; it relies solely on the consistency of the object store.</p>"},{"location":"faq/#delta-lake","title":"Delta Lake","text":"<p>Delta is another popular table format based on a log of updates to the table state. Its functionality and design is quite similar to Iceberg, as is its comparison to Icechunk.</p>"},{"location":"faq/#lance","title":"Lance","text":"<p>Lance is a modern columnar data format that is optimized for ML workflows and datasets.</p> <p>Despite its focus on multimodal data, as a columnar format, Lance can't accommodate large arrays / tensors chunked over arbitrary dimensions, making it fundamentally different from Icechunk.</p> <p>However, the modern design of Lance was very influential on Icechunk. Icechunk's commit and conflict resolution mechanism is partly inspired by Lance.</p>"},{"location":"faq/#other-related-projects","title":"Other Related projects","text":""},{"location":"faq/#xarray","title":"Xarray","text":"<p>Xarray is an open source project and Python package that introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays, which allows for more intuitive, more concise, and less error-prone user experience.</p> <p>Xarray includes a large and growing library of domain-agnostic functions for advanced analytics and visualization with these data structures.</p> <p>Xarray and Zarr / Icechunk work great together! Xarray is the recommended way to read and write Icechunk data for Python users in geospatial, weather, climate, and similar domains.</p>"},{"location":"faq/#kerchunk","title":"Kerchunk","text":"<p>Kerchunk is a library that provides a unified way to represent a variety of chunked, compressed data formats (e.g. NetCDF/HDF5, GRIB2, TIFF, \u2026), allowing efficient access to the data from traditional file systems or cloud object storage. It also provides a flexible way to create virtual datasets from multiple files. It does this by extracting the byte ranges, compression information and other information about the data and storing this metadata in a new, separate object. This means that you can create a virtual aggregate dataset over potentially many source files, for efficient, parallel and cloud-friendly in-situ access without having to copy or translate the originals. It is a gateway to in-the-cloud massive data processing while the data providers still insist on using legacy formats for archival storage</p> <p>Kerchunk emerged from the Pangeo community as an experimental way of reading archival files, allowing those files to be accessed \"virtually\" using the Zarr protocol. Kerchunk pioneered the concept of a \"chunk manifest\", a file containing references to compressed binary chunks in other files in the form of the tuple <code>(uri, offset, size)</code>. Kerchunk has experimented with different ways of serializing chunk manifests, including JSON and Parquet.</p> <p>Icechunk provides a highly efficient and scalable mechanism for storing and tracking the references generated by Kerchunk. Kerchunk and Icechunk are highly complimentary.</p>"},{"location":"faq/#virtualizarr","title":"VirtualiZarr","text":"<p>VirtualiZarr creates virtual Zarr stores for cloud-friendly access to archival data, using familiar Xarray syntax.</p> <p>VirtualiZarr provides another way of generating and manipulating Kerchunk-style references. VirtualiZarr first uses Kerchunk to generate virtual references, but then provides a simple Xarray-based interface for manipulating those references.  As VirtualiZarr can also write virtual references into an Icechunk Store directly, together they form a complete pipeline for generating and storing references to multiple pre-existing files.</p>"},{"location":"faq/#lakefs","title":"LakeFS","text":"<p>LakeFS is a solution git-style version control on top of cloud object storage. LakeFS enables git-style commits, tags, and branches representing the state of an entire object storage bucket.</p> <p>LakeFS is format agnostic and can accommodate any type of data, including Zarr. LakeFS can therefore be used to create a versioned Zarr store, similar to Icechunk.</p> <p>Icechunk, however, is designed specifically for array data, based on the Zarr data model. This specialization enables numerous optimizations and user-experience enhancements not possible with LakeFS.</p> <p>LakeFS also requires a server to operate. Icechunk, in contrast, works with just object storage.</p>"},{"location":"faq/#tensorstore","title":"TensorStore","text":"<p>TensorStore is a library for efficiently reading and writing large multi-dimensional arrays.</p> <p>TensorStore can read and write a variety of different array formats, including Zarr.</p> <p>While TensorStore is not yet compatible with Icechunk, it should be possible to implement Icechunk support in TensorStore.</p> <p>TensorStore implements an ocdbt:</p> <p>The ocdbt driver implements an Optionally-Cooperative Distributed B+Tree (OCDBT) on top of a base key-value store.</p> <p>Ocdbt implements a transactional, versioned key-value store suitable for storing Zarr data, thereby supporting some of the same features as Icechunk. Unlike Icechunk, the ocdbt key-value store is not specialized to Zarr, does not differentiate between chunk or metadata keys, and does not store any metadata about chunks.</p>"},{"location":"icechunk-rust/","title":"Icechunk Rust","text":"<p>Home / icechunk-rust</p>"},{"location":"icechunk-rust/#icechunk-rust","title":"Icechunk Rust","text":"<p>The Icechunk rust library is used internally by Icechunk Python. It is currently not designed to be used in standalone form.</p> <ul> <li>Icechunk Rust Documentatio at docs.rs</li> </ul> <p>We welcome contributors interested in implementing more Rust functionality! In particular, we would love to integrate Icechunk with zarrs, a new Zarr Rust library.</p>"},{"location":"overview/","title":"Overview","text":"<p>Home / overview</p>"},{"location":"overview/#icechunk","title":"Icechunk","text":"<p>Icechunk is an open-source (Apache 2.0), transactional storage engine for tensor / ND-array data designed for use on cloud object storage. Icechunk works together with Zarr, augmenting the Zarr core data model with features  that enhance performance, collaboration, and safety in a cloud-computing context. </p>"},{"location":"overview/#docs-organization","title":"Docs Organization","text":"<p>This is the Icechunk documentation. It's organized into the following parts.</p> <ul> <li>This page: a general overview of the project's goals and components.</li> <li>Frequently Asked Questions</li> <li>Documentation for Icechunk Python, the main user-facing   library</li> <li>Documentation for the Icechunk Rust Crate</li> <li>The Icechunk Spec</li> </ul>"},{"location":"overview/#icechunk-overview","title":"Icechunk Overview","text":"<p>Let's break down what \"transactional storage engine for Zarr\" actually means:</p> <ul> <li>Zarr is an open source specification for the storage of multidimensional array (a.k.a. tensor) data.   Zarr defines the metadata for describing arrays (shape, dtype, etc.) and the way these arrays are chunked, compressed, and converted to raw bytes for storage. Zarr can store its data in any key-value store.   There are many different implementations of Zarr in different languages. Right now, Icechunk only supports   Zarr Python.   If you're interested in implementing Icehcunk support, please open an issue so we can help you.</li> <li>Storage engine - Icechunk exposes a key-value interface to Zarr and manages all of the actual I/O for getting, setting, and updating both metadata and chunk data in cloud object storage.   Zarr libraries don't have to know exactly how icechunk works under the hood in order to use it.</li> <li>Transactional - The key improvement that Icechunk brings on top of regular Zarr is to provide consistent serializable isolation between transactions.   This means that Icechunk data are safe to read and write in parallel from multiple uncoordinated processes.   This allows Zarr to be used more like a database.</li> </ul> <p>The core entity in Icechunk is a repository or repo. A repo is defined as a Zarr hierarchy containing one or more Arrays and Groups, and a repo functions as  self-contained Zarr Store. The most common scenario is for an Icechunk repo to contain a single Zarr group with multiple arrays, each corresponding to different physical variables but sharing common spatiotemporal coordinates. However, formally a repo can be any valid Zarr hierarchy, from a single Array to a deeply nested structure of Groups and Arrays. Users of Icechunk should aim to scope their repos only to related arrays and groups that require consistent transactional updates.</p> <p>Icechunk supports the following core requirements:</p> <ol> <li>Object storage - the format is designed around the consistency features and performance characteristics available in modern cloud object storage. No external database or catalog is required to maintain a repo. (It also works with file storage.)</li> <li>Serializable isolation - Reads are isolated from concurrent writes and always use a committed snapshot of a repo. Writes are committed atomically and are never partially visible. No locks are required for reading.</li> <li>Time travel - Previous snapshots of a repo remain accessible after new ones have been written.</li> <li>Data version control - Repos support both tags (immutable references to snapshots) and branches (mutable references to snapshots).</li> <li>Chunk shardings - Chunk storage is decoupled from specific file names. Multiple chunks can be packed into a single object (sharding).</li> <li>Chunk references - Zarr-compatible chunks within other file formats (e.g. HDF5, NetCDF) can be referenced.</li> <li>Schema evolution - Arrays and Groups can be added, renamed, and removed from the hierarchy with minimal overhead.</li> </ol>"},{"location":"overview/#key-concepts","title":"Key Concepts","text":""},{"location":"overview/#groups-arrays-and-chunks","title":"Groups, Arrays, and Chunks","text":"<p>Icechunk is designed around the Zarr data model, widely used in scientific computing, data science, and AI / ML. (The Zarr high-level data model is effectively the same as HDF5.) The core data structure in this data model is the array. Arrays have two fundamental properties:</p> <ul> <li>shape - a tuple of integers which specify the dimensions of each axis of the array. A 10 x 10 square array would have shape (10, 10)</li> <li>data type - a specification of what type of data is found in each element, e.g. integer, float, etc. Different data types have different precision (e.g. 16-bit integer, 64-bit float, etc.)</li> </ul> <p>In Zarr / Icechunk, arrays are split into chunks,  A chunk is the minimum unit of data that must be read / written from storage, and thus choices about chunking have strong implications for performance. Zarr leaves this completely up to the user. Chunk shape should be chosen based on the anticipated data access patten for each array An Icechunk array is not bounded by an individual file and is effectively unlimited in size.</p> <p>For further organization of data, Icechunk supports groups withing a single repo. Group are like folders which contain multiple arrays and or other groups. Groups enable data to be organized into hierarchical trees. A common usage pattern is to store multiple arrays in a group representing a NetCDF-style dataset.</p> <p>Arbitrary JSON-style key-value metadata can be attached to both arrays and groups.</p>"},{"location":"overview/#snapshots","title":"Snapshots","text":"<p>Every update to an Icechunk store creates a new snapshot with a unique ID. Icechunk users must organize their updates into groups of related operations called transactions. For example, appending a new time slice to mutliple arrays should be done as a single transaction, comprising the following steps 1. Update the array metadata to resize the array to accommodate the new elements. 2. Write new chunks for each array in the group.</p> <p>While the transaction is in progress, none of these changes will be visible to other users of the store. Once the transaction is committed, a new snapshot is generated. Readers can only see and use committed snapshots.</p>"},{"location":"overview/#branches-and-tags","title":"Branches and Tags","text":"<p>Additionally, snapshots occur in a specific linear (i.e. serializable) order within  branch. A branch is a mutable reference to a snapshot--a pointer that maps the branch name to a snapshot ID. The default branch is <code>main</code>. Every commit to the main branch updates this reference. Icechunk's design protects against the race condition in which two uncoordinated sessions attempt to update the branch at the same time; only one can succeed.</p> <p>Icechunk also defines tags--immutable references to snapshot. Tags are appropriate for publishing specific releases of a repository or for any application which requires a persistent, immutable identifier to the store state.</p>"},{"location":"overview/#chunk-references","title":"Chunk References","text":"<p>Chunk references are \"pointers\" to chunks that exist in other files--HDF5, NetCDF, GRIB, etc. Icechunk can store these references alongside native Zarr chunks as \"virtual datasets\". You can then can update these virtual datasets incrementally (overwrite chunks, change metadata, etc.) without touching the underling files.</p>"},{"location":"overview/#how-does-it-work","title":"How Does It Work?","text":"<p>Note</p> <p>For more detailed explanation, have a look at the Icechunk spec</p> <p>Zarr itself works by storing both metadata and chunk data into a abstract store according to a specified system of \"keys\". For example, a 2D Zarr array called <code>myarray</code>, within a group called <code>mygroup</code>, would generate the following keys:</p> <pre><code>mygroup/zarr.json\nmygroup/myarray/zarr.json\nmygroup/myarray/c/0/0\nmygroup/myarray/c/0/1\n</code></pre> <p>In standard regular Zarr stores, these key map directly to filenames in a filesystem or object keys in an object storage system. When writing data, a Zarr implementation will create these keys and populate them with data. When modifying existing arrays or groups, a Zarr implementation will potentially overwrite existing keys with new data.</p> <p>This is generally not a problem, as long there is only one person or process coordinating access to the data. However, when multiple uncoordinated readers and writers attempt to access the same Zarr data at the same time, various consistency problems problems emerge. These consistency problems can occur in both file storage and object storage; they are particularly severe in a cloud setting where Zarr is being used as an active store for data that are frequently changed while also being read.</p> <p>With Icechunk, we keep the same core Zarr data model, but add a layer of indirection between the Zarr keys and the on-disk storage. The Icechunk library translates between the Zarr keys and the actual on-disk data given the particular context of the user's state. Icechunk defines a series of interconnected metadata and data files that together enable efficient isolated reading and writing of metadata and chunks. Once written, these files are immutable. Icechunk keeps track of every single chunk explicitly in a \"chunk manifest\".</p> <pre><code>flowchart TD\n    zarr-python[Zarr Library] &lt;-- key / value--&gt; icechunk[Icechunk Library]\n    icechunk &lt;-- data / metadata files --&gt; storage[(Object Storage)]\n</code></pre>"},{"location":"sample-datasets/","title":"Sample Datasets","text":"<p>Home / sample-datasets</p>"},{"location":"sample-datasets/#sample-datasets","title":"Sample Datasets","text":""},{"location":"sample-datasets/#native-datasets","title":"Native Datasets","text":""},{"location":"sample-datasets/#virtual-datasets","title":"Virtual Datasets","text":""},{"location":"sample-datasets/#noaa-oisst-data","title":"NOAA OISST Data","text":"<p>The NOAA 1/4\u00b0 Daily Optimum Interpolation Sea Surface Temperature (OISST) is a long term Climate Data Record that incorporates observations from different platforms (satellites, ships, buoys and Argo floats) into a regular global grid</p> <p>Check out an example dataset built using all virtual references pointing to daily Sea Surface Temperature data from 2020 to 2024 on NOAA's S3 bucket using python:</p> <pre><code>import icechunk\n\nstorage = icechunk.StorageConfig.s3_anonymous(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    region='us-east-1',\n)\n\nstore = IcechunkStore.open_existing(storage=storage, mode=\"r\", config=StoreConfig(\n    virtual_ref_config=VirtualRefConfig.s3_anonymous(region='us-east-1'),\n))\n</code></pre> <p></p>"},{"location":"spec/","title":"Spec","text":"<p>Home / spec</p>"},{"location":"spec/#icechunk-specification","title":"Icechunk Specification","text":"<p>Note</p> <p>The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p>"},{"location":"spec/#introduction","title":"Introduction","text":"<p>The Icechunk specification is a storage specification for Zarr data. Icechunk is inspired by Apache Iceberg and borrows many concepts and ideas from the Iceberg Spec.</p> <p>This specification describes a single Icechunk repository. A repository is defined as a Zarr store containing one or more Arrays and Groups. The most common scenario is for a repository to contain a single Zarr group with multiple arrays, each corresponding to different physical variables but sharing common spatiotemporal coordinates. However, formally a repository can be any valid Zarr hierarchy, from a single Array to a deeply nested structure of Groups and Arrays. Users of Icechunk should aim to scope their repository only to related arrays and groups that require consistent transactional updates.</p> <p>Icechunk defines a series of interconnected metadata and data files that together comprise the format. All the data and metadata for a repository are stored in a directory in object storage or file storage.</p>"},{"location":"spec/#goals","title":"Goals","text":"<p>The goals of the specification are as follows:</p> <ol> <li>Object storage - the format is designed around the consistency features and performance characteristics available in modern cloud object storage. No external database or catalog is required.</li> <li>Serializable isolation - Reads will be isolated from concurrent writes and always use a committed snapshot of a repository. Writes to repositories will be committed atomically and will not be partially visible. Readers will not acquire locks.</li> <li>Time travel - Previous snapshots of a repository remain accessible after new ones have been written.</li> <li>Chunk sharding and references - Chunk storage is decoupled from specific file names. Multiple chunks can be packed into a single object (sharding). Zarr-compatible chunks within other file formats (e.g. HDF5, NetCDF) can be referenced.</li> <li>Schema Evolution - Arrays and Groups can be added, renamed, and removed from the hierarchy with minimal overhead.</li> </ol>"},{"location":"spec/#non-goals","title":"Non Goals","text":"<ol> <li>Low Latency - Icechunk is designed to support analytical workloads for large repositories. We accept that the extra layers of metadata files and indirection will introduce additional cold-start latency compared to regular Zarr. </li> <li>No Catalog - The spec does not extend beyond a single repository or provide a way to organize multiple repositories into a hierarchy.</li> <li>Access Controls - Access control is the responsibility of the storage medium. The spec is not designed to enable fine-grained access restrictions (e.g. only read specific arrays) within a single repository.</li> </ol>"},{"location":"spec/#storage-operations","title":"Storage Operations","text":"<p>Icechunk requires that the storage system support the following operations:</p> <ul> <li>In-place write - Strong read-after-write and list-after-write consistency is expected. Files are not moved or altered once they are written.</li> <li>Conditional write if-not-exists - For the commit process to be safe and consistent, the storage system must guard against two files of the same name being created at the same time.</li> <li>Seekable reads - Chunk file formats may require seek support (e.g. shards).</li> <li>Deletes - Delete files that are no longer used (via a garbage-collection operation).</li> <li>Sorted List - The storage system must allow the listing of directories / prefixes in a consistent sorted order.</li> </ul> <p>These requirements are compatible with object stores, like S3, as well as with filesystems.</p> <p>The storage system is not required to support random-access writes. Once written, chunk and metadata files are immutable until they are deleted.</p>"},{"location":"spec/#specification","title":"Specification","text":""},{"location":"spec/#overview","title":"Overview","text":"<p>Icechunk uses a series of linked metadata files to describe the state of the repository.</p> <ul> <li>The Snapshot file records all of the different arrays and groups in the repository, plus their metadata. Every new commit creates a new snapshot file. The snapshot file contains pointers to one or more chunk manifest files and [optionally] attribute files.</li> <li>Chunk manifests store references to individual chunks. A single manifest may store references for multiple arrays or a subset of all the references for a single array.</li> <li>Attributes files provide a way to store additional user-defined attributes for arrays and groups outside of the structure file. This is important if attributes are very large, otherwise, they will be stored inline in the snapshot file.</li> <li>Chunk files store the actual compressed chunk data, potentially containing data for multiple chunks in a single file.</li> <li>Reference files track the state of branches and tags, containing a lightweight pointer to a snapshot file. Transactions on a branch are committed by creating the next branch file in a sequence.</li> </ul> <p>When reading from object store, the client opens the latest branch or tag file to obtain a pointer to the relevant snapshot file. The client then reads the snapshot file to determine the structure and hierarchy of the repository. When fetching data from an array, the client first examines the chunk manifest file[s] for that array and finally fetches the chunks referenced therein.</p> <p>When writing a new repository snapshot, the client first writes a new set of chunks and chunk manifests, and then generates a new snapshot file. Finally, in an atomic put-if-not-exists operation, to commit the transaction, it creates the next branch file in the sequence. This operation may fail if a different client has already committed the next snapshot. In this case, the client may attempt to resolve the conflicts and retry the commit.</p> <pre><code>flowchart TD\n    subgraph metadata[Metadata]\n    subgraph reference_files[Reference Files]\n    old_branch[Main Branch File 001]\n    branch[Main Branch File 002]\n    end\n    subgraph snapshots[Snapshots]\n    snapshot1[Snapshot File 1]\n    snapshot2[Snapshot File 2]\n    end\n    subgraph attributes[Attributes]\n    attrs[Attribute File]\n    end\n    subgraph manifests[Manifests]\n    manifestA[Chunk Manifest A]\n    manifestB[Chunk Manifest B]\n    end\n    end\n    subgraph data\n    chunk1[Chunk File 1]\n    chunk2[Chunk File 2]\n    chunk3[Chunk File 3]\n    chunk4[Chunk File 4]\n    end\n\n    branch -- snapshot ID --&gt; snapshot2\n    snapshot1 --&gt; attrs\n    snapshot1 --&gt; manifestA\n    snapshot2 --&gt; attrs\n    snapshot2 --&gt;manifestA\n    snapshot2 --&gt;manifestB\n    manifestA --&gt; chunk1\n    manifestA --&gt; chunk2\n    manifestB --&gt; chunk3\n    manifestB --&gt; chunk4\n\n</code></pre>"},{"location":"spec/#file-layout","title":"File Layout","text":"<p>All data and metadata files are stored within a root directory (typically a prefix within an object store) using the following directory structure.</p> <ul> <li><code>$ROOT</code> base URI (s3, gcs, local directory, etc.)</li> <li><code>$ROOT/refs/</code> reference files</li> <li><code>$ROOT/snapshots/</code> snapshot files</li> <li><code>$ROOT/attributes/</code> attribute files</li> <li><code>$ROOT/manifests/</code> chunk manifests</li> <li><code>$ROOT/chunks/</code> chunks</li> </ul>"},{"location":"spec/#file-formats","title":"File Formats","text":"<p>Warning</p> <p>The actual file formats used for each type of metadata file are in flux. The spec currently describes the data structures encoded in these files, rather than a specific file format.</p>"},{"location":"spec/#reference-files","title":"Reference Files","text":"<p>Similar to Git, Icechunk supports the concept of branches and tags. These references point to a specific snapshot of the repository.</p> <ul> <li>Branches are mutable references to a snapshot.   Repositories may have one or more branches.   The default branch name is <code>main</code>.   Repositories must always have a <code>main</code> branch, which is used to detect the existence of a valid repository in a given path.   After creation, branches may be updated to point to a different snapshot.</li> <li>Tags are immutable references to a snapshot.   A repository may contain zero or more tags.   After creation, tags may never be updated, unlike in Git.</li> </ul> <p>References are very important in the Icechunk design. Creating or updating references is the point at which consistency and atomicity of Icechunk transactions is enforced. Different client sessions may simultaneously create two inconsistent snapshots; however, only one session may successfully update a reference to point it to its snapshot.</p> <p>References (both branches and tags) are stored as JSON files, the content is a JSON object with:</p> <ul> <li>keys: a single key <code>\"snapshot\"</code>,</li> <li>value: a string representation of the snapshot id, using Base 32 Crockford encoding. The snapshot id is 12 byte random binary, so the encoded string has 20 characters.</li> </ul> <p>Here is an example of a JSON file corresponding to a tag or branch:</p> <pre><code>{\"snapshot\":\"VY76P925PRY57WFEK410\"}\n</code></pre>"},{"location":"spec/#creating-and-updating-branches","title":"Creating and Updating Branches","text":"<p>The process of creating and updating branches is designed to use the limited consistency guarantees offered by object storage to ensure transactional consistency. When a client checks out a branch, it obtains a specific snapshot ID and uses this snapshot as the basis for any changes it creates during its session. The client creates a new snapshot and then updates the branch reference to point to the new snapshot (a \"commit\"). However, when updating the branch reference, the client must detect whether a different session has updated the branch reference in the interim, possibly retrying or failing the commit if so. This is an \"optimistic concurrency\" strategy; the resolution mechanism can be expensive, but conflicts are expected to be infrequent.</p> <p>All popular object stores support a \"create if not exists\" operation. In other words, object stores can guard against the race condition which occurs when two sessions attempt to create the same file at the same time. This motivates the design of Icechunk's branch file naming convention.</p> <p>Each commit to an Icechunk branch augments a counter called the sequence number. The first commit creates sequence number 0. The next commit creates sequence number 1. Etc. This sequence number is encoded into the branch reference file name.</p> <p>When a client checks out a branch, it keeps track of its current sequence number N. When it tries to commit, it attempts to create the file corresponding to sequence number N + 1 in an atomic \"create if not exists\" operation. If this succeeds, the commit is successful. If this fails (because another client created that file already), the commit fails. At this point, the client may choose to retry its commit (possibly re-reading the updated data) and then create sequence number N + 2.</p> <p>Branch references are stored in the <code>refs/</code> directory within a subdirectory corresponding to the branch name prepended by the string <code>branch.</code>: <code>refs/branch.$BRANCH_NAME/</code>. Branch names may not contain the <code>/</code> character.</p> <p>To facilitate easy lookups of the latest branch reference, we use the following encoding for the sequence number: - subtract the sequence number from the integer <code>1099511627775</code> - encode the resulting integer as a string using Base 32 Crockford - left-padding the string with 0s to a length of 8 characters This produces a deterministic sequence of branch file names in which the latest sequence always appears first when sorted lexicographically, facilitating easy lookup by listing the object store.</p> <p>The full branch file name is then given by <code>refs/branch.$BRANCH_NAME/$ENCODED_SEQUENCE.json</code>.</p> <p>For example, the first main branch file is in a store, corresponding with sequence number 0, is always named <code>refs/branch.main/ZZZZZZZZ.json</code>. The branch file for sequence number 100 is <code>refs/branch.main/ZZZZZZWV.json</code>. The maximum number of commits allowed in an Icechunk repository is consequently <code>1099511627775</code>, corresponding to the state file <code>refs/branch.main/00000000.json</code>.</p>"},{"location":"spec/#tags","title":"Tags","text":"<p>Since tags are immutable, they are simpler than branches.</p> <p>Tag files follow the pattern <code>refs/tag.$TAG_NAME/ref.json</code>.</p> <p>Tag names may not contain the <code>/</code> character.</p> <p>When creating a new tag, the client attempts to create the tag file using a \"create if not exists\" operation. If successful, the tag is created successful. If not, that means another client has already created that tag.</p> <p>Tags cannot be deleted once created.</p>"},{"location":"spec/#snapshot-files","title":"Snapshot Files","text":"<p>The snapshot file fully describes the schema of the repository, including all arrays and groups.</p> <p>The snapshot file is currently encoded using MessagePack, but this may change before Icechunk version 1.0. Given the alpha status of this spec, the best way to understand the information stored in the snapshot file is through the data structure used internally by the Icechunk library for serialization. This data structure will most certainly change before the spec stabilization:</p> <pre><code>pub struct Snapshot {\n    pub icechunk_snapshot_format_version: IcechunkFormatVersion,\n    pub icechunk_snapshot_format_flags: BTreeMap&lt;String, rmpv::Value&gt;,\n\n    pub manifest_files: Vec&lt;ManifestFileInfo&gt;,\n    pub attribute_files: Vec&lt;AttributeFileInfo&gt;,\n\n    pub total_parents: u32,\n    pub short_term_parents: u16,\n    pub short_term_history: VecDeque&lt;SnapshotMetadata&gt;,\n\n    pub metadata: SnapshotMetadata,\n    pub started_at: DateTime&lt;Utc&gt;,\n    pub properties: SnapshotProperties,\n    nodes: BTreeMap&lt;Path, NodeSnapshot&gt;,\n}\n</code></pre> <p>To get full details on what each field contains, please refer to the Icechunk library code.</p>"},{"location":"spec/#attributes-files","title":"Attributes Files","text":"<p>Attribute files hold user-defined attributes separately from the snapshot file.</p> <p>Warning</p> <p>Attribute files have not been implemented.</p> <p>The on-disk format for attribute files has not been defined yet, but it will probably be a MessagePack serialization of the attributes map.</p>"},{"location":"spec/#chunk-manifest-files","title":"Chunk Manifest Files","text":"<p>A chunk manifest file stores chunk references. Chunk references from multiple arrays can be stored in the same chunk manifest. The chunks from a single array can also be spread across multiple manifests.</p> <p>Manifest files are currently encoded using MessagePack, but this may change before Icechunk version 1.0. Given the alpha status of this spec, the best way to understand the information stored in the snapshot file is through the data structure used internally by the Icechunk library. This data structure will most certainly change before the spec stabilization:</p> <pre><code>pub struct Manifest {\n    pub icechunk_manifest_format_version: IcechunkFormatVersion,\n    pub icechunk_manifest_format_flags: BTreeMap&lt;String, rmpv::Value&gt;,\n    chunks: BTreeMap&lt;(NodeId, ChunkIndices), ChunkPayload&gt;,\n}\n\npub enum ChunkPayload {\n    Inline(Bytes),\n    Virtual(VirtualChunkRef),\n    Ref(ChunkRef),\n}\n</code></pre> <p>The most important part to understand from the data structure is the fact that manifests can hold three types of references:</p> <ul> <li>Native (<code>Ref</code>), pointing to the id of a chunk within the Icechunk repository.</li> <li>Inline (<code>Inline</code>), an optimization for very small chunks that can be embedded directly in the manifest. Mostly used for coordinate arrays.</li> <li>Virtual (<code>Virtual</code>), pointing to a region of a file outside of the Icechunk repository, for example,   a chunk that is inside a NetCDF file in object store</li> </ul> <p>To get full details on what each field contains, please refer to the Icechunk library code.</p>"},{"location":"spec/#chunk-files","title":"Chunk Files","text":"<p>Chunk files contain the compressed binary chunks of a Zarr array. Icechunk permits quite a bit of flexibility about how chunks are stored. Chunk files can be:</p> <ul> <li>One chunk per chunk file (i.e. standard Zarr)</li> <li>Multiple contiguous chunks from the same array in a single chunk file (similar to Zarr V3 shards)</li> <li>Chunks from multiple different arrays in the same file</li> <li>Other file types (e.g. NetCDF, HDF5) which contain Zarr-compatible chunks</li> </ul> <p>Applications may choose to arrange chunks within files in different ways to optimize I/O patterns.</p>"},{"location":"spec/#algorithms","title":"Algorithms","text":""},{"location":"spec/#initialize-new-repository","title":"Initialize New Repository","text":"<p>A new repository is initialized by creating a new [possibly empty] snapshot file and then creating the first file in the main branch sequence.</p> <p>If another client attempts to initialize a repository in the same location, only one can succeed. </p>"},{"location":"spec/#read-from-repository","title":"Read from Repository","text":""},{"location":"spec/#from-snapshot-id","title":"From Snapshot ID","text":"<p>If the specific snapshot ID is known, a client can open it directly in read only mode.</p> <ol> <li>Use the specified shapshot ID to fetch the snapshot file.</li> <li>Fetch desired attributes and values from arrays.</li> </ol>"},{"location":"spec/#from-branch","title":"From Branch","text":"<p>Usually, a client will want to read from the latest branch (e.g. <code>main</code>).</p> <ol> <li>List the object store prefix <code>refs/branch.$BRANCH_NAME/</code> to obtain the latest branch file in the sequence. Due to the encoding of the sequence number, this should be the first file in lexicographical order.</li> <li>Read the branch file JSON contents to obtain the snapshot ID.</li> <li>Use the shapshot ID to fetch the snapshot file.</li> <li>Fetch desired attributes and values from arrays.</li> </ol>"},{"location":"spec/#from-tag","title":"From Tag","text":"<ol> <li>Read the tag file found at <code>refs/tag.$TAG_NAME/ref.json</code> to obtain the snapshot ID.</li> <li>Use the shapshot ID to fetch the snapshot file.</li> <li>Fetch desired attributes and values from arrays.</li> </ol>"},{"location":"spec/#write-new-snapshot","title":"Write New Snapshot","text":"<ol> <li>Open a repository at a specific branch as described above, keeping track of the sequence number and branch name in the session context.</li> <li>[optional] Write new chunk files.</li> <li>[optional] Write new chunk manifests.</li> <li>Write a new snapshot file.</li> <li>Attempt to write the next branch file in the sequence<ol> <li>If successful, the commit succeeded and the branch is updated.</li> <li>If unsuccessful, attempt to reconcile and retry the commit.</li> </ol> </li> </ol>"},{"location":"spec/#create-new-tag","title":"Create New Tag","text":"<p>A tag can be created from any snapshot.</p> <ol> <li>Open the repository at a specific snapshot.</li> <li>Attempt to create the tag file.    a. If successful, the tag was created.    b. If unsuccessful, the tag already exists.</li> </ol>"},{"location":"icechunk-python/","title":"Index","text":"<p>Home / icechunk-python</p>"},{"location":"icechunk-python/#index-of-icechunk-python","title":"Index of icechunk-python","text":"<ul> <li>developing</li> <li>examples</li> <li>notebooks</li> <li>quickstart</li> <li>reference</li> </ul>"},{"location":"icechunk-python/concurrency/","title":"Concurrency","text":"<p>Home / icechunk-python / concurrency</p>"},{"location":"icechunk-python/concurrency/#concurrency","title":"Concurrency","text":"<p>TODO: describe the general approach to concurrency in Icechunk</p>"},{"location":"icechunk-python/concurrency/#built-in-concurrency","title":"Built-in concurrency","text":"<p>Describe the multi-threading and async concurrency in Icechunk / Zarr</p>"},{"location":"icechunk-python/concurrency/#distributed-concurrency-within-a-single-transaction","title":"Distributed concurrency within a single transaction","text":"<p>\"Cooperative\" concurrency</p>"},{"location":"icechunk-python/concurrency/#concurrency-across-uncoordinated-sessions","title":"Concurrency across uncoordinated sessions","text":""},{"location":"icechunk-python/concurrency/#conflict-detection","title":"Conflict detection","text":""},{"location":"icechunk-python/configuration/","title":"Configuration","text":"<p>Home / icechunk-python / configuration</p>"},{"location":"icechunk-python/configuration/#configuration","title":"Configuration","text":"<p>When creating and opening Icechunk stores, there are a two different sets of configuration to be aware of: - <code>StorageConfig</code> - for configuring access to the object store or filesystem - <code>StoreConfig</code> - for configuring the behavior of the Icechunk Store itself</p>"},{"location":"icechunk-python/configuration/#storage-config","title":"Storage Config","text":"<p>Icechunk can be confirgured to work with a both object storage and filesystem backends. The storage configuration defines the location of an Icechunk store, along with any options or information needed to access data from a given storage type.</p>"},{"location":"icechunk-python/configuration/#s3-storage","title":"S3 Storage","text":"<p>When using Icechunk with s3 compatible storage systems, credentials must be provided to allow access to the data on the given endpoint. Icechunk allows for creating the storage config for s3 in three ways:</p> From environmentProvide credentialsAnonymous <p>With this option, the credentials for connecting to S3 are detected automatically from your environment. This is usually the best choice if you are connecting from within an AWS environment (e.g. from EC2).</p> <pre><code>icechunk.StorageConfig.s3_from_env(\n    bucket=\"icechunk-test\",\n    prefix=\"quickstart-demo-1\"\n)\n</code></pre> <p>With this option, you provide your credentials and other details explicitly.</p> <pre><code>icechunk.StorageConfig.s3_from_config(\n    bucket=\"icechunk-test\",\n    prefix=\"quickstart-demo-1\",\n    region='us-east-1',\n    credentials=S3Credentials(\n        access_key_id='my-access-key',\n        secret_access_key='my-secret-key',\n        # session token is optional\n        session_token='my-token',\n    ),\n    endpoint_url=None,\n    allow_http=False,\n)\n</code></pre> <p>With this option, you connect to S3 anonymously (without credentials). This is suitable for public data.</p> <pre><code>icechunk.StorageConfig.s3_anonymous(\n    bucket=\"icechunk-test\",\n    prefix=\"quickstart-demo-1\",\n    region='us-east-1,\n)\n</code></pre>"},{"location":"icechunk-python/configuration/#filesystem-storage","title":"Filesystem Storage","text":"<p>Icechunk can also be used on a local filesystem by providing a path to the location of the store</p> Local filesystem <pre><code>icechunk.StorageConfig.filesystem(\"/path/to/my/dataset\")\n</code></pre>"},{"location":"icechunk-python/configuration/#store-config","title":"Store Config","text":"<p>Separate from the storage config, the Store can also be configured with options which control its runtime behavior.</p>"},{"location":"icechunk-python/configuration/#writing-chunks-inline","title":"Writing chunks inline","text":"<p>Chunks can be written inline alongside the store metadata if the size of a given chunk falls within the configured threshold. Inlining allows these small chunks (often used to store small coordinate variables) to be accessed more quickly. This is the default behavior for chunks smaller than 512 bytes, but it can be overridden using the <code>inline_chunk_threshold_bytes</code> option:</p> Never write chunks inlineWrite bigger chunks inline <pre><code>StoreConfig(\n    inline_chunk_threshold_bytes=0,\n    ...\n)\n</code></pre> <pre><code>StoreConfig(\n    inline_chunk_threshold_bytes=1024,\n    ...\n)\n</code></pre>"},{"location":"icechunk-python/configuration/#virtual-reference-storage-config","title":"Virtual Reference Storage Config","text":"<p>Icechunk allows for reading \"Virtual\" data from existing archival datasets. This requires creating a distinct <code>VirtualRefConfig</code> (similar to <code>StorageConfig</code>) giving Icechunk the necessary permissions to access the archival data. This can be configured using the <code>virtual_ref_config</code> option:</p> S3 from environmentS3 with credentialsS3 Anonymous <pre><code>StoreConfig(\n    virtual_ref_config=VirtualRefConfig.s3_from_env(),\n    ...\n)\n</code></pre> <pre><code>StoreConfig(\n    virtual_ref_config=VirtualRefConfig.s3_from_config(\n        credential=S3Credentials(\n            access_key_id='my-access-key',\n            secret_access_key='my-secret-key',\n        ),\n        region='us-east-1'\n    ),\n    ...\n)\n</code></pre> <pre><code>StoreConfig(\n    virtual_ref_config=VirtualRefConfig.s3_anonymous(region='us-east-1'),\n    ...\n)\n</code></pre>"},{"location":"icechunk-python/configuration/#creating-and-opening-repos","title":"Creating and Opening Repos","text":"<p>Now we can now create or open an Icechunk store using our config.</p>"},{"location":"icechunk-python/configuration/#creating-a-new-store","title":"Creating a new store","text":"<p>Note</p> <p>Icechunk stores cannot be created in the same location where another store already exists.</p> Creating with S3 storageCreating with local filesystem <pre><code>storage = icechunk.StorageConfig.s3_from_env(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    region='us-east-1',\n)\n\nstore = icechunk.IcechunkStore.create(\n    storage=storage, \n    mode=\"w\", \n)\n</code></pre> <pre><code>storage = icechunk.StorageConfig.filesystem(\"/path/to/my/dataset\")\nconfig = icechunk.StoreConfig(\n    inline_chunk_threshold_bytes=1024,\n)\n\nstore = icechunk.IcechunkStore.create(\n    storage=storage, \n    mode=\"w\", \n)\n</code></pre>"},{"location":"icechunk-python/configuration/#opening-an-existing-store","title":"Opening an existing store","text":"Opening from S3 StorageOpening from local filesystem <pre><code>storage = icechunk.StorageConfig.s3_anonymous(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst.2020-2024/',\n    region='us-east-1',\n)\n\nconfig = icechunk.StoreConfig(\n    virtual_ref_config=icechunk.VirtualRefConfig.s3_anonymous(region='us-east-1'),\n)\n\nstore = icechunk.IcechunkStore.open_existing(\n    storage=storage, \n    mode=\"r+\", \n    config=config,\n)\n</code></pre> <pre><code>storage = icechunk.StorageConfig.filesystem(\"/path/to/my/dataset\")\nconfig = icechunk.StoreConfig(\n    inline_chunk_threshold_bytes=1024,\n)\n\nstore = icechunk.IcechunkStore.open_existing(\n    storage=storage,\n    mode='r+',\n    config=config,\n)\n</code></pre>"},{"location":"icechunk-python/configuration/#access-mode","title":"Access Mode","text":"<p>Note that in all of the above examples, a <code>mode</code> is provided to instruct the access level of the user to the store. This mode instructs whether the store should be opened in read only mode, and the store should start with a clean slate (although Icechunk prevents the possibility of accidentally overwriting any data that was previously comimtted to the store forever). For more about the access modes, see the <code>zarr-python</code> docs.</p>"},{"location":"icechunk-python/quickstart/","title":"Quickstart","text":"<p>Home / icechunk-python / quickstart</p>"},{"location":"icechunk-python/quickstart/#quickstart","title":"Quickstart","text":"<p>Icechunk is designed to be mostly in the background. As a Python user, you'll mostly be interacting with Zarr. If you're not familiar with Zarr, you may want to start with the Zarr Tutorial</p>"},{"location":"icechunk-python/quickstart/#installation","title":"Installation","text":"<p>Install Icechunk with pip</p> <pre><code>pip install icechunk \n</code></pre> <p>Note</p> <p>Icechunk is currently designed to support the Zarr V3 Specification. Using it today requires installing the latest pre-release of Zarr Python 3.</p>"},{"location":"icechunk-python/quickstart/#create-a-new-store","title":"Create a new store","text":"<p>To get started, let's create a new Icechunk store. We recommend creating your store on S3 to get the most out of Icechunk's cloud-native design. However, you can also create a store on your local filesystem.</p> S3 StorageLocal Storage <pre><code>storage_config = icechunk.StorageConfig.s3_from_env(\n    bucket=\"icechunk-test\",\n    prefix=\"quickstart-demo-1\"\n)\nstore = icechunk.IcechunkStore.create(storage_config)\n</code></pre> <pre><code>storage_config = icechunk.StorageConfig.filesystem(\"./icechunk-local\")\nstore = icechunk.IcechunkStore.create(storage_config)\n</code></pre>"},{"location":"icechunk-python/quickstart/#write-some-data-and-commit","title":"Write some data and commit","text":"<p>We can now use our Icechunk <code>store</code> with Zarr. Let's first create a group and an array within it.</p> <pre><code>group = zarr.group(store)\narray = group.create(\"my_array\", shape=10, dtype=int)\n</code></pre> <p>Now let's write some data</p> <pre><code>array[:] = 1\n</code></pre> <p>Now let's commit our update</p> <pre><code>store.commit(\"first commit\")\n</code></pre> <p>\ud83c\udf89 Congratulations! You just made your first Icechunk snapshot.</p>"},{"location":"icechunk-python/quickstart/#make-a-second-commit","title":"Make a second commit","text":"<p>Let's now put some new data into our array, overwriting the first five elements.</p> <pre><code>array[:5] = 2\n</code></pre> <p>...and commit the changes</p> <pre><code>store.commit(\"overwrite some values\")\n</code></pre>"},{"location":"icechunk-python/quickstart/#explore-version-history","title":"Explore version history","text":"<p>We can see the full version history of our repo:</p> <pre><code>hist = store.ancestry()\nfor anc in hist:\n    print(anc.id, anc.message, anc.written_at)\n\n# Output:\n# AHC3TSP5ERXKTM4FCB5G overwrite some values 2024-10-14 14:07:27.328429+00:00\n# Q492CAPV7SF3T1BC0AA0 first commit 2024-10-14 14:07:26.152193+00:00\n# T7SMDT9C5DZ8MP83DNM0 Repository initialized 2024-10-14 14:07:22.338529+00:00\n</code></pre> <p>...and we can go back in time to the earlier version.</p> <pre><code># latest version\nassert array[0] == 2\n# check out earlier snapshot\nstore.checkout(snapshot_id=hist[1].id)\n# verify data matches first version\nassert array[0] == 1\n</code></pre> <p>That's it! You now know how to use Icechunk! For your next step, dig deeper into configuration, explore the version control system, or learn how to use Icechunk with Xarray.</p>"},{"location":"icechunk-python/reference/","title":"API Reference","text":"<p>Home / icechunk-python / reference </p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore","title":"<code>IcechunkStore</code>","text":"<p>               Bases: <code>Store</code>, <code>SyncMixin</code></p> Source code in <code>icechunk/__init__.py</code> <pre><code>class IcechunkStore(Store, SyncMixin):\n    _store: PyIcechunkStore\n\n    @classmethod\n    async def open(cls, *args: Any, **kwargs: Any) -&gt; Self:\n        \"\"\"This method is called by zarr-python, it's not intended for users.\n\n        Use one of `IcechunkStore.open_existing`, `IcechunkStore.create` or `IcechunkStore.open_or_create` instead.\n        \"\"\"\n        return cls.open_or_create(*args, **kwargs)\n\n    @classmethod\n    def open_or_create(cls, *args: Any, **kwargs: Any) -&gt; Self:\n        if \"mode\" in kwargs:\n            mode = kwargs.pop(\"mode\")\n        else:\n            mode = \"r\"\n\n        if \"storage\" in kwargs:\n            storage = kwargs.pop(\"storage\")\n        else:\n            raise ValueError(\n                \"Storage configuration is required. Pass a Storage object to construct an IcechunkStore\"\n            )\n\n        store = None\n        match mode:\n            case \"r\" | \"r+\":\n                store = cls.open_existing(storage, mode, *args, **kwargs)\n            case \"a\":\n                if pyicechunk_store_exists(storage):\n                    store = cls.open_existing(storage, mode, *args, **kwargs)\n                else:\n                    store = cls.create(storage, mode, *args, **kwargs)\n            case \"w\":\n                if pyicechunk_store_exists(storage):\n                    store = cls.open_existing(storage, mode, *args, **kwargs)\n                    store.sync_clear()\n                else:\n                    store = cls.create(storage, mode, *args, **kwargs)\n            case \"w-\":\n                if pyicechunk_store_exists(storage):\n                    raise ValueError(\"\"\"Zarr store already exists, open using mode \"w\" or \"r+\"\"\"\"\")\n                else:\n                    store = cls.create(storage, mode, *args, **kwargs)\n\n        assert(store)\n        # We dont want to call _open() becuase icechunk handles the opening, etc.\n        # if we have gotten this far we can mark it as open\n        store._is_open = True\n\n        return store\n\n\n    def __init__(\n        self,\n        store: PyIcechunkStore,\n        mode: AccessModeLiteral = \"r\",\n        *args: Any,\n        **kwargs: Any,\n    ):\n        \"\"\"Create a new IcechunkStore.\n\n        This should not be called directly, instead use the `create`, `open_existing` or `open_or_create` class methods.\n        \"\"\"\n        super().__init__(*args, mode=mode, **kwargs)\n        if store is None:\n            raise ValueError(\n                \"An IcechunkStore should not be created with the default constructor, instead use either the create or open_existing class methods.\"\n            )\n        self._store = store\n\n    @classmethod\n    def open_existing(\n        cls,\n        storage: StorageConfig,\n        mode: AccessModeLiteral = \"r\",\n        config: StoreConfig | None = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; Self:\n        \"\"\"Open an existing IcechunkStore from the given storage.\n\n        If there is not store at the given location, an error will be raised.\n\n        It is recommended to use the cached storage option for better performance. If cached=True,\n        this will be configured automatically with the provided storage_config as the underlying\n        storage backend.\n\n        If opened with AccessModeLiteral \"r\", the store will be read-only. Otherwise the store will be writable.\n        \"\"\"\n        config = config or StoreConfig()\n        read_only = mode == \"r\"\n        # We have delayed checking if the repository exists, to avoid the delay in the happy case\n        # So we need to check now if open fails, to provide a nice error message\n        try:\n            store = pyicechunk_store_open_existing(\n                storage, read_only=read_only, config=config\n            )\n        # TODO: we should have an exception type to catch here, for the case of non-existing repo\n        except Exception as e:\n            if pyicechunk_store_exists(storage):\n                # if the repo exists, this is an actual error we need to raise\n                raise e\n            else:\n                # if the repo doesn't exists, we want to point users to that issue instead\n                raise ValueError(\"No Icechunk repository at the provided location, try opening in create mode or changing the location\") from None\n        return cls(store=store, mode=mode, args=args, kwargs=kwargs)\n\n    @classmethod\n    def create(\n        cls,\n        storage: StorageConfig,\n        mode: AccessModeLiteral = \"w\",\n        config: StoreConfig | None = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; Self:\n        \"\"\"Create a new IcechunkStore with the given storage configuration.\n\n        If a store already exists at the given location, an error will be raised.\n        \"\"\"\n        config = config or StoreConfig()\n        store = pyicechunk_store_create(storage, config=config)\n        return cls(store=store, mode=mode, args=args, kwargs=kwargs)\n\n    def with_mode(self, mode: AccessModeLiteral) -&gt; Self:\n        \"\"\"\n        Return a new store of the same type pointing to the same location with a new mode.\n\n        The returned Store is not automatically opened. Call :meth:`Store.open` before\n        using.\n\n        Parameters\n        ----------\n        mode: AccessModeLiteral\n            The new mode to use.\n\n        Returns\n        -------\n        store:\n            A new store of the same type with the new mode.\n\n        \"\"\"\n        read_only = mode == \"r\"\n        new_store = self._store.with_mode(read_only)\n        return self.__class__(new_store, mode=mode)\n\n    def __eq__(self, value: object) -&gt; bool:\n        if not isinstance(value, self.__class__):\n            return False\n        return self._store == value._store\n\n    def __getstate__(self) -&gt; object:\n        # we serialize the Rust store as bytes\n        d = self.__dict__.copy()\n        d[\"_store\"] = self._store.as_bytes()\n        return d\n\n    def __setstate__(self, state: Any) -&gt; None:\n        # we have to deserialize the bytes of the Rust store\n        mode = state[\"_mode\"]\n        is_read_only = mode.readonly\n        store_repr = state[\"_store\"]\n        state[\"_store\"] = pyicechunk_store_from_bytes(store_repr, is_read_only)\n        self.__dict__ = state\n\n    @property\n    def snapshot_id(self) -&gt; str:\n        \"\"\"Return the current snapshot id.\"\"\"\n        return self._store.snapshot_id\n\n    def change_set_bytes(self) -&gt; bytes:\n        \"\"\"Get the complete list of changes applied in this session, serialized to bytes.\n\n        This method is useful in combination with `IcechunkStore.distributed_commit`. When a\n        write session is too large to execute in a single machine, it could be useful to\n        distribute it across multiple workers. Each worker can write their changes independently\n        (map) and then a single commit is executed by a coordinator (reduce).\n\n        This methods provides a way to send back to gather a \"description\" of the\n        changes applied by a worker. Resulting bytes, together with the `change_set_bytes` of\n        other workers, can be fed to `distributed_commit`.\n\n        This API is subject to change, it will be replaced by a merge operation at the Store level.\n        \"\"\"\n        return self._store.change_set_bytes()\n\n    @property\n    def branch(self) -&gt; str | None:\n        \"\"\"Return the current branch name.\"\"\"\n        return self._store.branch\n\n    def checkout(\n        self,\n        snapshot_id: str | None = None,\n        branch: str | None = None,\n        tag: str | None = None,\n    ) -&gt; None:\n        \"\"\"Checkout a branch, tag, or specific snapshot.\n\n        If a branch is checked out, any following `commit` attempts will update that branch\n        reference if successful. If a tag or snapshot_id are checked out, the repository\n        won't allow commits.\n        \"\"\"\n        if snapshot_id is not None:\n            if branch is not None or tag is not None:\n                raise ValueError(\n                    \"only one of snapshot_id, branch, or tag may be specified\"\n                )\n            return self._store.checkout_snapshot(snapshot_id)\n        if branch is not None:\n            if tag is not None:\n                raise ValueError(\n                    \"only one of snapshot_id, branch, or tag may be specified\"\n                )\n            return self._store.checkout_branch(branch)\n        if tag is not None:\n            return self._store.checkout_tag(tag)\n\n        raise ValueError(\"a snapshot_id, branch, or tag must be specified\")\n\n    async def async_checkout(\n        self,\n        snapshot_id: str | None = None,\n        branch: str | None = None,\n        tag: str | None = None,\n    ) -&gt; None:\n        \"\"\"Checkout a branch, tag, or specific snapshot.\n\n        If a branch is checked out, any following `commit` attempts will update that branch\n        reference if successful. If a tag or snapshot_id are checked out, the repository\n        won't allow commits.\n        \"\"\"\n        if snapshot_id is not None:\n            if branch is not None or tag is not None:\n                raise ValueError(\n                    \"only one of snapshot_id, branch, or tag may be specified\"\n                )\n            return await self._store.async_checkout_snapshot(snapshot_id)\n        if branch is not None:\n            if tag is not None:\n                raise ValueError(\n                    \"only one of snapshot_id, branch, or tag may be specified\"\n                )\n            return await self._store.async_checkout_branch(branch)\n        if tag is not None:\n            return await self._store.async_checkout_tag(tag)\n\n        raise ValueError(\"a snapshot_id, branch, or tag must be specified\")\n\n    def commit(self, message: str) -&gt; str:\n        \"\"\"Commit any uncommitted changes to the store.\n\n        This will create a new snapshot on the current branch and return\n        the new snapshot id.\n\n        This method will fail if:\n\n        * there is no currently checked out branch\n        * some other writer updated the curret branch since the repository was checked out\n        \"\"\"\n        return self._store.commit(message)\n\n    async def async_commit(self, message: str) -&gt; str:\n        \"\"\"Commit any uncommitted changes to the store.\n\n        This will create a new snapshot on the current branch and return\n        the new snapshot id.\n\n        This method will fail if:\n\n        * there is no currently checked out branch\n        * some other writer updated the curret branch since the repository was checked out\n        \"\"\"\n        return await self._store.async_commit(message)\n\n    def distributed_commit(\n        self, message: str, other_change_set_bytes: list[bytes]\n    ) -&gt; str:\n        \"\"\"Commit any uncommitted changes to the store with a set of distributed changes.\n\n        This will create a new snapshot on the current branch and return\n        the new snapshot id.\n\n        This method will fail if:\n\n        * there is no currently checked out branch\n        * some other writer updated the curret branch since the repository was checked out\n\n        other_change_set_bytes must be generated as the output of calling `change_set_bytes`\n        on other stores. The resulting commit will include changes from all stores.\n\n        The behavior is undefined if the stores applied conflicting changes.\n        \"\"\"\n        return self._store.distributed_commit(message, other_change_set_bytes)\n\n    async def async_distributed_commit(\n        self, message: str, other_change_set_bytes: list[bytes]\n    ) -&gt; str:\n        \"\"\"Commit any uncommitted changes to the store with a set of distributed changes.\n\n        This will create a new snapshot on the current branch and return\n        the new snapshot id.\n\n        This method will fail if:\n\n        * there is no currently checked out branch\n        * some other writer updated the curret branch since the repository was checked out\n\n        other_change_set_bytes must be generated as the output of calling `change_set_bytes`\n        on other stores. The resulting commit will include changes from all stores.\n\n        The behavior is undefined if the stores applied conflicting changes.\n        \"\"\"\n        return await self._store.async_distributed_commit(message, other_change_set_bytes)\n\n    @property\n    def has_uncommitted_changes(self) -&gt; bool:\n        \"\"\"Return True if there are uncommitted changes to the store\"\"\"\n        return self._store.has_uncommitted_changes\n\n    async def async_reset(self) -&gt; None:\n        \"\"\"Discard any uncommitted changes and reset to the previous snapshot state.\"\"\"\n        return await self._store.async_reset()\n\n    def reset(self) -&gt; None:\n        \"\"\"Discard any uncommitted changes and reset to the previous snapshot state.\"\"\"\n        return self._store.reset()\n\n    async def async_new_branch(self, branch_name: str) -&gt; str:\n        \"\"\"Create a new branch pointing to the current checked out snapshot.\n\n        This requires having no uncommitted changes.\n        \"\"\"\n        return await self._store.async_new_branch(branch_name)\n\n    def new_branch(self, branch_name: str) -&gt; str:\n        \"\"\"Create a new branch pointing to the current checked out snapshot.\n\n        This requires having no uncommitted changes.\n        \"\"\"\n        return self._store.new_branch(branch_name)\n\n    def tag(self, tag_name: str, snapshot_id: str) -&gt; None:\n        \"\"\"Create a tag pointing to the current checked out snapshot.\"\"\"\n        return self._store.tag(tag_name, snapshot_id=snapshot_id)\n\n    async def async_tag(self, tag_name: str, snapshot_id: str) -&gt; None:\n        \"\"\"Create a tag pointing to the current checked out snapshot.\"\"\"\n        return await self._store.async_tag(tag_name, snapshot_id=snapshot_id)\n\n    def ancestry(self) -&gt; list[SnapshotMetadata]:\n        \"\"\"Get the list of parents of the current version.\n        \"\"\"\n        return self._store.ancestry()\n\n    def async_ancestry(self) -&gt; AsyncGenerator[SnapshotMetadata, None]:\n        \"\"\"Get the list of parents of the current version.\n\n        Returns\n        -------\n        AsyncGenerator[SnapshotMetadata, None]\n        \"\"\"\n        return self._store.async_ancestry()\n\n    async def empty(self) -&gt; bool:\n        \"\"\"Check if the store is empty.\"\"\"\n        return await self._store.empty()\n\n    async def clear(self) -&gt; None:\n        \"\"\"Clear the store.\n\n        This will remove all contents from the current session,\n        including all groups and all arrays. But it will not modify the repository history.\n        \"\"\"\n        return await self._store.clear()\n\n    def sync_clear(self) -&gt; None:\n        \"\"\"Clear the store.\n\n        This will remove all contents from the current session,\n        including all groups and all arrays. But it will not modify the repository history.\n        \"\"\"\n        return self._store.sync_clear()\n\n    async def get(\n        self,\n        key: str,\n        prototype: BufferPrototype,\n        byte_range: tuple[int | None, int | None] | None = None,\n    ) -&gt; Buffer | None:\n        \"\"\"Retrieve the value associated with a given key.\n\n        Parameters\n        ----------\n        key : str\n        byte_range : tuple[int, Optional[int]], optional\n\n        Returns\n        -------\n        Buffer\n        \"\"\"\n\n        try:\n            result = await self._store.get(key, byte_range)\n        except KeyNotFound as _e:\n            # Zarr python expects None to be returned if the key does not exist\n            # but an IcechunkStore returns an error if the key does not exist\n            return None\n\n        return prototype.buffer.from_bytes(result)\n\n    async def get_partial_values(\n        self,\n        prototype: BufferPrototype,\n        key_ranges: Iterable[tuple[str, ByteRangeRequest]],\n    ) -&gt; list[Buffer | None]:\n        \"\"\"Retrieve possibly partial values from given key_ranges.\n\n        Parameters\n        ----------\n        key_ranges : Iterable[tuple[str, tuple[int | None, int | None]]]\n            Ordered set of key, range pairs, a key may occur multiple times with different ranges\n\n        Returns\n        -------\n        list of values, in the order of the key_ranges, may contain null/none for missing keys\n        \"\"\"\n        # NOTE: pyo3 has not implicit conversion from an Iterable to a rust iterable. So we convert it\n        # to a list here first. Possible opportunity for optimization.\n        result = await self._store.get_partial_values(list(key_ranges))\n        return [prototype.buffer.from_bytes(r) for r in result]\n\n    async def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if a key exists in the store.\n\n        Parameters\n        ----------\n        key : str\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return await self._store.exists(key)\n\n    @property\n    def supports_writes(self) -&gt; bool:\n        \"\"\"Does the store support writes?\"\"\"\n        return self._store.supports_writes\n\n    async def set(self, key: str, value: Buffer) -&gt; None:\n        \"\"\"Store a (key, value) pair.\n\n        Parameters\n        ----------\n        key : str\n        value : Buffer\n        \"\"\"\n        return await self._store.set(key, value.to_bytes())\n\n    async def set_if_not_exists(self, key: str, value: Buffer) -&gt; None:\n        \"\"\"\n        Store a key to ``value`` if the key is not already present.\n\n        Parameters\n        -----------\n        key : str\n        value : Buffer\n        \"\"\"\n        return await self._store.set_if_not_exists(key, value.to_bytes())\n\n    async def async_set_virtual_ref(\n        self, key: str, location: str, *, offset: int, length: int\n    ) -&gt; None:\n        \"\"\"Store a virtual reference to a chunk.\n\n        Parameters\n        ----------\n        key : str\n            The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0'\n        location : str\n            The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc'\n        offset : int\n            The offset in bytes from the start of the file location in storage the chunk starts at\n        length : int\n            The length of the chunk in bytes, measured from the given offset\n        \"\"\"\n        return await self._store.async_set_virtual_ref(key, location, offset, length)\n\n    def set_virtual_ref(\n        self, key: str, location: str, *, offset: int, length: int\n    ) -&gt; None:\n        \"\"\"Store a virtual reference to a chunk.\n\n        Parameters\n        ----------\n        key : str\n            The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0'\n        location : str\n            The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc'\n        offset : int\n            The offset in bytes from the start of the file location in storage the chunk starts at\n        length : int\n            The length of the chunk in bytes, measured from the given offset\n        \"\"\"\n        return self._store.set_virtual_ref(key, location, offset, length)\n\n    async def delete(self, key: str) -&gt; None:\n        \"\"\"Remove a key from the store\n\n        Parameters\n        ----------\n        key : strz\n        \"\"\"\n        return await self._store.delete(key)\n\n    @property\n    def supports_partial_writes(self) -&gt; bool:\n        \"\"\"Does the store support partial writes?\"\"\"\n        return self._store.supports_partial_writes\n\n    async def set_partial_values(\n        self, key_start_values: Iterable[tuple[str, int, BytesLike]]\n    ) -&gt; None:\n        \"\"\"Store values at a given key, starting at byte range_start.\n\n        Parameters\n        ----------\n        key_start_values : list[tuple[str, int, BytesLike]]\n            set of key, range_start, values triples, a key may occur multiple times with different\n            range_starts, range_starts (considering the length of the respective values) must not\n            specify overlapping ranges for the same key\n        \"\"\"\n        # NOTE: pyo3 does not implicit conversion from an Iterable to a rust iterable. So we convert it\n        # to a list here first. Possible opportunity for optimization.\n        return await self._store.set_partial_values(list(key_start_values))\n\n    @property\n    def supports_listing(self) -&gt; bool:\n        \"\"\"Does the store support listing?\"\"\"\n        return self._store.supports_listing\n\n    @property\n    def supports_deletes(self) -&gt; bool:\n        return self._store.supports_deletes\n\n    def list(self) -&gt; AsyncGenerator[str, None]:\n        \"\"\"Retrieve all keys in the store.\n\n        Returns\n        -------\n        AsyncGenerator[str, None]\n        \"\"\"\n        # The zarr spec specefies that that this and other\n        # listing methods should not be async, so we need to\n        # wrap the async method in a sync method.\n        return self._store.list()\n\n    def list_prefix(self, prefix: str) -&gt; AsyncGenerator[str, None]:\n        \"\"\"Retrieve all keys in the store with a given prefix.\n\n        Parameters\n        ----------\n        prefix : str\n\n        Returns\n        -------\n        AsyncGenerator[str, None]\n        \"\"\"\n        # The zarr spec specefies that that this and other\n        # listing methods should not be async, so we need to\n        # wrap the async method in a sync method.\n        return self._store.list_prefix(prefix)\n\n    def list_dir(self, prefix: str) -&gt; AsyncGenerator[str, None]:\n        \"\"\"\n        Retrieve all keys and prefixes with a given prefix and which do not contain the character\n        \u201c/\u201d after the given prefix.\n\n        Parameters\n        ----------\n        prefix : str\n\n        Returns\n        -------\n        AsyncGenerator[str, None]\n        \"\"\"\n        # The zarr spec specefies that that this and other\n        # listing methods should not be async, so we need to\n        # wrap the async method in a sync method.\n        return self._store.list_dir(prefix)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.branch","title":"<code>branch: str | None</code>  <code>property</code>","text":"<p>Return the current branch name.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.has_uncommitted_changes","title":"<code>has_uncommitted_changes: bool</code>  <code>property</code>","text":"<p>Return True if there are uncommitted changes to the store</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.snapshot_id","title":"<code>snapshot_id: str</code>  <code>property</code>","text":"<p>Return the current snapshot id.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.supports_listing","title":"<code>supports_listing: bool</code>  <code>property</code>","text":"<p>Does the store support listing?</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.supports_partial_writes","title":"<code>supports_partial_writes: bool</code>  <code>property</code>","text":"<p>Does the store support partial writes?</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.supports_writes","title":"<code>supports_writes: bool</code>  <code>property</code>","text":"<p>Does the store support writes?</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.__init__","title":"<code>__init__(store, mode='r', *args, **kwargs)</code>","text":"<p>Create a new IcechunkStore.</p> <p>This should not be called directly, instead use the <code>create</code>, <code>open_existing</code> or <code>open_or_create</code> class methods.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def __init__(\n    self,\n    store: PyIcechunkStore,\n    mode: AccessModeLiteral = \"r\",\n    *args: Any,\n    **kwargs: Any,\n):\n    \"\"\"Create a new IcechunkStore.\n\n    This should not be called directly, instead use the `create`, `open_existing` or `open_or_create` class methods.\n    \"\"\"\n    super().__init__(*args, mode=mode, **kwargs)\n    if store is None:\n        raise ValueError(\n            \"An IcechunkStore should not be created with the default constructor, instead use either the create or open_existing class methods.\"\n        )\n    self._store = store\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.ancestry","title":"<code>ancestry()</code>","text":"<p>Get the list of parents of the current version.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def ancestry(self) -&gt; list[SnapshotMetadata]:\n    \"\"\"Get the list of parents of the current version.\n    \"\"\"\n    return self._store.ancestry()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_ancestry","title":"<code>async_ancestry()</code>","text":"<p>Get the list of parents of the current version.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_ancestry--returns","title":"Returns","text":"<p>AsyncGenerator[SnapshotMetadata, None]</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def async_ancestry(self) -&gt; AsyncGenerator[SnapshotMetadata, None]:\n    \"\"\"Get the list of parents of the current version.\n\n    Returns\n    -------\n    AsyncGenerator[SnapshotMetadata, None]\n    \"\"\"\n    return self._store.async_ancestry()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_checkout","title":"<code>async_checkout(snapshot_id=None, branch=None, tag=None)</code>  <code>async</code>","text":"<p>Checkout a branch, tag, or specific snapshot.</p> <p>If a branch is checked out, any following <code>commit</code> attempts will update that branch reference if successful. If a tag or snapshot_id are checked out, the repository won't allow commits.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def async_checkout(\n    self,\n    snapshot_id: str | None = None,\n    branch: str | None = None,\n    tag: str | None = None,\n) -&gt; None:\n    \"\"\"Checkout a branch, tag, or specific snapshot.\n\n    If a branch is checked out, any following `commit` attempts will update that branch\n    reference if successful. If a tag or snapshot_id are checked out, the repository\n    won't allow commits.\n    \"\"\"\n    if snapshot_id is not None:\n        if branch is not None or tag is not None:\n            raise ValueError(\n                \"only one of snapshot_id, branch, or tag may be specified\"\n            )\n        return await self._store.async_checkout_snapshot(snapshot_id)\n    if branch is not None:\n        if tag is not None:\n            raise ValueError(\n                \"only one of snapshot_id, branch, or tag may be specified\"\n            )\n        return await self._store.async_checkout_branch(branch)\n    if tag is not None:\n        return await self._store.async_checkout_tag(tag)\n\n    raise ValueError(\"a snapshot_id, branch, or tag must be specified\")\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_commit","title":"<code>async_commit(message)</code>  <code>async</code>","text":"<p>Commit any uncommitted changes to the store.</p> <p>This will create a new snapshot on the current branch and return the new snapshot id.</p> <p>This method will fail if:</p> <ul> <li>there is no currently checked out branch</li> <li>some other writer updated the curret branch since the repository was checked out</li> </ul> Source code in <code>icechunk/__init__.py</code> <pre><code>async def async_commit(self, message: str) -&gt; str:\n    \"\"\"Commit any uncommitted changes to the store.\n\n    This will create a new snapshot on the current branch and return\n    the new snapshot id.\n\n    This method will fail if:\n\n    * there is no currently checked out branch\n    * some other writer updated the curret branch since the repository was checked out\n    \"\"\"\n    return await self._store.async_commit(message)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_distributed_commit","title":"<code>async_distributed_commit(message, other_change_set_bytes)</code>  <code>async</code>","text":"<p>Commit any uncommitted changes to the store with a set of distributed changes.</p> <p>This will create a new snapshot on the current branch and return the new snapshot id.</p> <p>This method will fail if:</p> <ul> <li>there is no currently checked out branch</li> <li>some other writer updated the curret branch since the repository was checked out</li> </ul> <p>other_change_set_bytes must be generated as the output of calling <code>change_set_bytes</code> on other stores. The resulting commit will include changes from all stores.</p> <p>The behavior is undefined if the stores applied conflicting changes.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def async_distributed_commit(\n    self, message: str, other_change_set_bytes: list[bytes]\n) -&gt; str:\n    \"\"\"Commit any uncommitted changes to the store with a set of distributed changes.\n\n    This will create a new snapshot on the current branch and return\n    the new snapshot id.\n\n    This method will fail if:\n\n    * there is no currently checked out branch\n    * some other writer updated the curret branch since the repository was checked out\n\n    other_change_set_bytes must be generated as the output of calling `change_set_bytes`\n    on other stores. The resulting commit will include changes from all stores.\n\n    The behavior is undefined if the stores applied conflicting changes.\n    \"\"\"\n    return await self._store.async_distributed_commit(message, other_change_set_bytes)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_new_branch","title":"<code>async_new_branch(branch_name)</code>  <code>async</code>","text":"<p>Create a new branch pointing to the current checked out snapshot.</p> <p>This requires having no uncommitted changes.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def async_new_branch(self, branch_name: str) -&gt; str:\n    \"\"\"Create a new branch pointing to the current checked out snapshot.\n\n    This requires having no uncommitted changes.\n    \"\"\"\n    return await self._store.async_new_branch(branch_name)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_reset","title":"<code>async_reset()</code>  <code>async</code>","text":"<p>Discard any uncommitted changes and reset to the previous snapshot state.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def async_reset(self) -&gt; None:\n    \"\"\"Discard any uncommitted changes and reset to the previous snapshot state.\"\"\"\n    return await self._store.async_reset()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_set_virtual_ref","title":"<code>async_set_virtual_ref(key, location, *, offset, length)</code>  <code>async</code>","text":"<p>Store a virtual reference to a chunk.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_set_virtual_ref--parameters","title":"Parameters","text":"<p>key : str     The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0' location : str     The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc' offset : int     The offset in bytes from the start of the file location in storage the chunk starts at length : int     The length of the chunk in bytes, measured from the given offset</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def async_set_virtual_ref(\n    self, key: str, location: str, *, offset: int, length: int\n) -&gt; None:\n    \"\"\"Store a virtual reference to a chunk.\n\n    Parameters\n    ----------\n    key : str\n        The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0'\n    location : str\n        The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc'\n    offset : int\n        The offset in bytes from the start of the file location in storage the chunk starts at\n    length : int\n        The length of the chunk in bytes, measured from the given offset\n    \"\"\"\n    return await self._store.async_set_virtual_ref(key, location, offset, length)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.async_tag","title":"<code>async_tag(tag_name, snapshot_id)</code>  <code>async</code>","text":"<p>Create a tag pointing to the current checked out snapshot.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def async_tag(self, tag_name: str, snapshot_id: str) -&gt; None:\n    \"\"\"Create a tag pointing to the current checked out snapshot.\"\"\"\n    return await self._store.async_tag(tag_name, snapshot_id=snapshot_id)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.change_set_bytes","title":"<code>change_set_bytes()</code>","text":"<p>Get the complete list of changes applied in this session, serialized to bytes.</p> <p>This method is useful in combination with <code>IcechunkStore.distributed_commit</code>. When a write session is too large to execute in a single machine, it could be useful to distribute it across multiple workers. Each worker can write their changes independently (map) and then a single commit is executed by a coordinator (reduce).</p> <p>This methods provides a way to send back to gather a \"description\" of the changes applied by a worker. Resulting bytes, together with the <code>change_set_bytes</code> of other workers, can be fed to <code>distributed_commit</code>.</p> <p>This API is subject to change, it will be replaced by a merge operation at the Store level.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def change_set_bytes(self) -&gt; bytes:\n    \"\"\"Get the complete list of changes applied in this session, serialized to bytes.\n\n    This method is useful in combination with `IcechunkStore.distributed_commit`. When a\n    write session is too large to execute in a single machine, it could be useful to\n    distribute it across multiple workers. Each worker can write their changes independently\n    (map) and then a single commit is executed by a coordinator (reduce).\n\n    This methods provides a way to send back to gather a \"description\" of the\n    changes applied by a worker. Resulting bytes, together with the `change_set_bytes` of\n    other workers, can be fed to `distributed_commit`.\n\n    This API is subject to change, it will be replaced by a merge operation at the Store level.\n    \"\"\"\n    return self._store.change_set_bytes()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.checkout","title":"<code>checkout(snapshot_id=None, branch=None, tag=None)</code>","text":"<p>Checkout a branch, tag, or specific snapshot.</p> <p>If a branch is checked out, any following <code>commit</code> attempts will update that branch reference if successful. If a tag or snapshot_id are checked out, the repository won't allow commits.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def checkout(\n    self,\n    snapshot_id: str | None = None,\n    branch: str | None = None,\n    tag: str | None = None,\n) -&gt; None:\n    \"\"\"Checkout a branch, tag, or specific snapshot.\n\n    If a branch is checked out, any following `commit` attempts will update that branch\n    reference if successful. If a tag or snapshot_id are checked out, the repository\n    won't allow commits.\n    \"\"\"\n    if snapshot_id is not None:\n        if branch is not None or tag is not None:\n            raise ValueError(\n                \"only one of snapshot_id, branch, or tag may be specified\"\n            )\n        return self._store.checkout_snapshot(snapshot_id)\n    if branch is not None:\n        if tag is not None:\n            raise ValueError(\n                \"only one of snapshot_id, branch, or tag may be specified\"\n            )\n        return self._store.checkout_branch(branch)\n    if tag is not None:\n        return self._store.checkout_tag(tag)\n\n    raise ValueError(\"a snapshot_id, branch, or tag must be specified\")\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.clear","title":"<code>clear()</code>  <code>async</code>","text":"<p>Clear the store.</p> <p>This will remove all contents from the current session, including all groups and all arrays. But it will not modify the repository history.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def clear(self) -&gt; None:\n    \"\"\"Clear the store.\n\n    This will remove all contents from the current session,\n    including all groups and all arrays. But it will not modify the repository history.\n    \"\"\"\n    return await self._store.clear()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.commit","title":"<code>commit(message)</code>","text":"<p>Commit any uncommitted changes to the store.</p> <p>This will create a new snapshot on the current branch and return the new snapshot id.</p> <p>This method will fail if:</p> <ul> <li>there is no currently checked out branch</li> <li>some other writer updated the curret branch since the repository was checked out</li> </ul> Source code in <code>icechunk/__init__.py</code> <pre><code>def commit(self, message: str) -&gt; str:\n    \"\"\"Commit any uncommitted changes to the store.\n\n    This will create a new snapshot on the current branch and return\n    the new snapshot id.\n\n    This method will fail if:\n\n    * there is no currently checked out branch\n    * some other writer updated the curret branch since the repository was checked out\n    \"\"\"\n    return self._store.commit(message)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.create","title":"<code>create(storage, mode='w', config=None, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new IcechunkStore with the given storage configuration.</p> <p>If a store already exists at the given location, an error will be raised.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    storage: StorageConfig,\n    mode: AccessModeLiteral = \"w\",\n    config: StoreConfig | None = None,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a new IcechunkStore with the given storage configuration.\n\n    If a store already exists at the given location, an error will be raised.\n    \"\"\"\n    config = config or StoreConfig()\n    store = pyicechunk_store_create(storage, config=config)\n    return cls(store=store, mode=mode, args=args, kwargs=kwargs)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.delete","title":"<code>delete(key)</code>  <code>async</code>","text":"<p>Remove a key from the store</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.delete--parameters","title":"Parameters","text":"<p>key : strz</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def delete(self, key: str) -&gt; None:\n    \"\"\"Remove a key from the store\n\n    Parameters\n    ----------\n    key : strz\n    \"\"\"\n    return await self._store.delete(key)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.distributed_commit","title":"<code>distributed_commit(message, other_change_set_bytes)</code>","text":"<p>Commit any uncommitted changes to the store with a set of distributed changes.</p> <p>This will create a new snapshot on the current branch and return the new snapshot id.</p> <p>This method will fail if:</p> <ul> <li>there is no currently checked out branch</li> <li>some other writer updated the curret branch since the repository was checked out</li> </ul> <p>other_change_set_bytes must be generated as the output of calling <code>change_set_bytes</code> on other stores. The resulting commit will include changes from all stores.</p> <p>The behavior is undefined if the stores applied conflicting changes.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def distributed_commit(\n    self, message: str, other_change_set_bytes: list[bytes]\n) -&gt; str:\n    \"\"\"Commit any uncommitted changes to the store with a set of distributed changes.\n\n    This will create a new snapshot on the current branch and return\n    the new snapshot id.\n\n    This method will fail if:\n\n    * there is no currently checked out branch\n    * some other writer updated the curret branch since the repository was checked out\n\n    other_change_set_bytes must be generated as the output of calling `change_set_bytes`\n    on other stores. The resulting commit will include changes from all stores.\n\n    The behavior is undefined if the stores applied conflicting changes.\n    \"\"\"\n    return self._store.distributed_commit(message, other_change_set_bytes)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.empty","title":"<code>empty()</code>  <code>async</code>","text":"<p>Check if the store is empty.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def empty(self) -&gt; bool:\n    \"\"\"Check if the store is empty.\"\"\"\n    return await self._store.empty()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.exists","title":"<code>exists(key)</code>  <code>async</code>","text":"<p>Check if a key exists in the store.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.exists--parameters","title":"Parameters","text":"<p>key : str</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.exists--returns","title":"Returns","text":"<p>bool</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def exists(self, key: str) -&gt; bool:\n    \"\"\"Check if a key exists in the store.\n\n    Parameters\n    ----------\n    key : str\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return await self._store.exists(key)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get","title":"<code>get(key, prototype, byte_range=None)</code>  <code>async</code>","text":"<p>Retrieve the value associated with a given key.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get--parameters","title":"Parameters","text":"<p>key : str byte_range : tuple[int, Optional[int]], optional</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get--returns","title":"Returns","text":"<p>Buffer</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def get(\n    self,\n    key: str,\n    prototype: BufferPrototype,\n    byte_range: tuple[int | None, int | None] | None = None,\n) -&gt; Buffer | None:\n    \"\"\"Retrieve the value associated with a given key.\n\n    Parameters\n    ----------\n    key : str\n    byte_range : tuple[int, Optional[int]], optional\n\n    Returns\n    -------\n    Buffer\n    \"\"\"\n\n    try:\n        result = await self._store.get(key, byte_range)\n    except KeyNotFound as _e:\n        # Zarr python expects None to be returned if the key does not exist\n        # but an IcechunkStore returns an error if the key does not exist\n        return None\n\n    return prototype.buffer.from_bytes(result)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get_partial_values","title":"<code>get_partial_values(prototype, key_ranges)</code>  <code>async</code>","text":"<p>Retrieve possibly partial values from given key_ranges.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get_partial_values--parameters","title":"Parameters","text":"<p>key_ranges : Iterable[tuple[str, tuple[int | None, int | None]]]     Ordered set of key, range pairs, a key may occur multiple times with different ranges</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.get_partial_values--returns","title":"Returns","text":"<p>list of values, in the order of the key_ranges, may contain null/none for missing keys</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def get_partial_values(\n    self,\n    prototype: BufferPrototype,\n    key_ranges: Iterable[tuple[str, ByteRangeRequest]],\n) -&gt; list[Buffer | None]:\n    \"\"\"Retrieve possibly partial values from given key_ranges.\n\n    Parameters\n    ----------\n    key_ranges : Iterable[tuple[str, tuple[int | None, int | None]]]\n        Ordered set of key, range pairs, a key may occur multiple times with different ranges\n\n    Returns\n    -------\n    list of values, in the order of the key_ranges, may contain null/none for missing keys\n    \"\"\"\n    # NOTE: pyo3 has not implicit conversion from an Iterable to a rust iterable. So we convert it\n    # to a list here first. Possible opportunity for optimization.\n    result = await self._store.get_partial_values(list(key_ranges))\n    return [prototype.buffer.from_bytes(r) for r in result]\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list","title":"<code>list()</code>","text":"<p>Retrieve all keys in the store.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list--returns","title":"Returns","text":"<p>AsyncGenerator[str, None]</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def list(self) -&gt; AsyncGenerator[str, None]:\n    \"\"\"Retrieve all keys in the store.\n\n    Returns\n    -------\n    AsyncGenerator[str, None]\n    \"\"\"\n    # The zarr spec specefies that that this and other\n    # listing methods should not be async, so we need to\n    # wrap the async method in a sync method.\n    return self._store.list()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_dir","title":"<code>list_dir(prefix)</code>","text":"<p>Retrieve all keys and prefixes with a given prefix and which do not contain the character \u201c/\u201d after the given prefix.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_dir--parameters","title":"Parameters","text":"<p>prefix : str</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_dir--returns","title":"Returns","text":"<p>AsyncGenerator[str, None]</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def list_dir(self, prefix: str) -&gt; AsyncGenerator[str, None]:\n    \"\"\"\n    Retrieve all keys and prefixes with a given prefix and which do not contain the character\n    \u201c/\u201d after the given prefix.\n\n    Parameters\n    ----------\n    prefix : str\n\n    Returns\n    -------\n    AsyncGenerator[str, None]\n    \"\"\"\n    # The zarr spec specefies that that this and other\n    # listing methods should not be async, so we need to\n    # wrap the async method in a sync method.\n    return self._store.list_dir(prefix)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_prefix","title":"<code>list_prefix(prefix)</code>","text":"<p>Retrieve all keys in the store with a given prefix.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_prefix--parameters","title":"Parameters","text":"<p>prefix : str</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.list_prefix--returns","title":"Returns","text":"<p>AsyncGenerator[str, None]</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def list_prefix(self, prefix: str) -&gt; AsyncGenerator[str, None]:\n    \"\"\"Retrieve all keys in the store with a given prefix.\n\n    Parameters\n    ----------\n    prefix : str\n\n    Returns\n    -------\n    AsyncGenerator[str, None]\n    \"\"\"\n    # The zarr spec specefies that that this and other\n    # listing methods should not be async, so we need to\n    # wrap the async method in a sync method.\n    return self._store.list_prefix(prefix)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.new_branch","title":"<code>new_branch(branch_name)</code>","text":"<p>Create a new branch pointing to the current checked out snapshot.</p> <p>This requires having no uncommitted changes.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def new_branch(self, branch_name: str) -&gt; str:\n    \"\"\"Create a new branch pointing to the current checked out snapshot.\n\n    This requires having no uncommitted changes.\n    \"\"\"\n    return self._store.new_branch(branch_name)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.open","title":"<code>open(*args, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>This method is called by zarr-python, it's not intended for users.</p> <p>Use one of <code>IcechunkStore.open_existing</code>, <code>IcechunkStore.create</code> or <code>IcechunkStore.open_or_create</code> instead.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>@classmethod\nasync def open(cls, *args: Any, **kwargs: Any) -&gt; Self:\n    \"\"\"This method is called by zarr-python, it's not intended for users.\n\n    Use one of `IcechunkStore.open_existing`, `IcechunkStore.create` or `IcechunkStore.open_or_create` instead.\n    \"\"\"\n    return cls.open_or_create(*args, **kwargs)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.open_existing","title":"<code>open_existing(storage, mode='r', config=None, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Open an existing IcechunkStore from the given storage.</p> <p>If there is not store at the given location, an error will be raised.</p> <p>It is recommended to use the cached storage option for better performance. If cached=True, this will be configured automatically with the provided storage_config as the underlying storage backend.</p> <p>If opened with AccessModeLiteral \"r\", the store will be read-only. Otherwise the store will be writable.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>@classmethod\ndef open_existing(\n    cls,\n    storage: StorageConfig,\n    mode: AccessModeLiteral = \"r\",\n    config: StoreConfig | None = None,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Open an existing IcechunkStore from the given storage.\n\n    If there is not store at the given location, an error will be raised.\n\n    It is recommended to use the cached storage option for better performance. If cached=True,\n    this will be configured automatically with the provided storage_config as the underlying\n    storage backend.\n\n    If opened with AccessModeLiteral \"r\", the store will be read-only. Otherwise the store will be writable.\n    \"\"\"\n    config = config or StoreConfig()\n    read_only = mode == \"r\"\n    # We have delayed checking if the repository exists, to avoid the delay in the happy case\n    # So we need to check now if open fails, to provide a nice error message\n    try:\n        store = pyicechunk_store_open_existing(\n            storage, read_only=read_only, config=config\n        )\n    # TODO: we should have an exception type to catch here, for the case of non-existing repo\n    except Exception as e:\n        if pyicechunk_store_exists(storage):\n            # if the repo exists, this is an actual error we need to raise\n            raise e\n        else:\n            # if the repo doesn't exists, we want to point users to that issue instead\n            raise ValueError(\"No Icechunk repository at the provided location, try opening in create mode or changing the location\") from None\n    return cls(store=store, mode=mode, args=args, kwargs=kwargs)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.reset","title":"<code>reset()</code>","text":"<p>Discard any uncommitted changes and reset to the previous snapshot state.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Discard any uncommitted changes and reset to the previous snapshot state.\"\"\"\n    return self._store.reset()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set","title":"<code>set(key, value)</code>  <code>async</code>","text":"<p>Store a (key, value) pair.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set--parameters","title":"Parameters","text":"<p>key : str value : Buffer</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def set(self, key: str, value: Buffer) -&gt; None:\n    \"\"\"Store a (key, value) pair.\n\n    Parameters\n    ----------\n    key : str\n    value : Buffer\n    \"\"\"\n    return await self._store.set(key, value.to_bytes())\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_if_not_exists","title":"<code>set_if_not_exists(key, value)</code>  <code>async</code>","text":"<p>Store a key to <code>value</code> if the key is not already present.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_if_not_exists--parameters","title":"Parameters","text":"<p>key : str value : Buffer</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def set_if_not_exists(self, key: str, value: Buffer) -&gt; None:\n    \"\"\"\n    Store a key to ``value`` if the key is not already present.\n\n    Parameters\n    -----------\n    key : str\n    value : Buffer\n    \"\"\"\n    return await self._store.set_if_not_exists(key, value.to_bytes())\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_partial_values","title":"<code>set_partial_values(key_start_values)</code>  <code>async</code>","text":"<p>Store values at a given key, starting at byte range_start.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_partial_values--parameters","title":"Parameters","text":"<p>key_start_values : list[tuple[str, int, BytesLike]]     set of key, range_start, values triples, a key may occur multiple times with different     range_starts, range_starts (considering the length of the respective values) must not     specify overlapping ranges for the same key</p> Source code in <code>icechunk/__init__.py</code> <pre><code>async def set_partial_values(\n    self, key_start_values: Iterable[tuple[str, int, BytesLike]]\n) -&gt; None:\n    \"\"\"Store values at a given key, starting at byte range_start.\n\n    Parameters\n    ----------\n    key_start_values : list[tuple[str, int, BytesLike]]\n        set of key, range_start, values triples, a key may occur multiple times with different\n        range_starts, range_starts (considering the length of the respective values) must not\n        specify overlapping ranges for the same key\n    \"\"\"\n    # NOTE: pyo3 does not implicit conversion from an Iterable to a rust iterable. So we convert it\n    # to a list here first. Possible opportunity for optimization.\n    return await self._store.set_partial_values(list(key_start_values))\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_virtual_ref","title":"<code>set_virtual_ref(key, location, *, offset, length)</code>","text":"<p>Store a virtual reference to a chunk.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.set_virtual_ref--parameters","title":"Parameters","text":"<p>key : str     The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0' location : str     The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc' offset : int     The offset in bytes from the start of the file location in storage the chunk starts at length : int     The length of the chunk in bytes, measured from the given offset</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def set_virtual_ref(\n    self, key: str, location: str, *, offset: int, length: int\n) -&gt; None:\n    \"\"\"Store a virtual reference to a chunk.\n\n    Parameters\n    ----------\n    key : str\n        The chunk to store the reference under. This is the fully qualified zarr key eg: 'array/c/0/0/0'\n    location : str\n        The location of the chunk in storage. This is absolute path to the chunk in storage eg: 's3://bucket/path/to/file.nc'\n    offset : int\n        The offset in bytes from the start of the file location in storage the chunk starts at\n    length : int\n        The length of the chunk in bytes, measured from the given offset\n    \"\"\"\n    return self._store.set_virtual_ref(key, location, offset, length)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.sync_clear","title":"<code>sync_clear()</code>","text":"<p>Clear the store.</p> <p>This will remove all contents from the current session, including all groups and all arrays. But it will not modify the repository history.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def sync_clear(self) -&gt; None:\n    \"\"\"Clear the store.\n\n    This will remove all contents from the current session,\n    including all groups and all arrays. But it will not modify the repository history.\n    \"\"\"\n    return self._store.sync_clear()\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.tag","title":"<code>tag(tag_name, snapshot_id)</code>","text":"<p>Create a tag pointing to the current checked out snapshot.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def tag(self, tag_name: str, snapshot_id: str) -&gt; None:\n    \"\"\"Create a tag pointing to the current checked out snapshot.\"\"\"\n    return self._store.tag(tag_name, snapshot_id=snapshot_id)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.with_mode","title":"<code>with_mode(mode)</code>","text":"<p>Return a new store of the same type pointing to the same location with a new mode.</p> <p>The returned Store is not automatically opened. Call :meth:<code>Store.open</code> before using.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.with_mode--parameters","title":"Parameters","text":"<p>mode: AccessModeLiteral     The new mode to use.</p>"},{"location":"icechunk-python/reference/#icechunk.IcechunkStore.with_mode--returns","title":"Returns","text":"<p>store:     A new store of the same type with the new mode.</p> Source code in <code>icechunk/__init__.py</code> <pre><code>def with_mode(self, mode: AccessModeLiteral) -&gt; Self:\n    \"\"\"\n    Return a new store of the same type pointing to the same location with a new mode.\n\n    The returned Store is not automatically opened. Call :meth:`Store.open` before\n    using.\n\n    Parameters\n    ----------\n    mode: AccessModeLiteral\n        The new mode to use.\n\n    Returns\n    -------\n    store:\n        A new store of the same type with the new mode.\n\n    \"\"\"\n    read_only = mode == \"r\"\n    new_store = self._store.with_mode(read_only)\n    return self.__class__(new_store, mode=mode)\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig","title":"<code>StorageConfig</code>","text":"<p>Storage configuration for an IcechunkStore</p> <p>Currently supports memory, filesystem, and S3 storage backends. Use the class methods to create a StorageConfig object with the desired backend.</p> <p>Ex: <pre><code>storage_config = StorageConfig.memory(\"prefix\")\nstorage_config = StorageConfig.filesystem(\"/path/to/root\")\nstorage_config = StorageConfig.s3_from_env(\"bucket\", \"prefix\")\nstorage_config = StorageConfig.s3_from_config(\"bucket\", \"prefix\", ...)\n</code></pre></p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class StorageConfig:\n    \"\"\"Storage configuration for an IcechunkStore\n\n    Currently supports memory, filesystem, and S3 storage backends.\n    Use the class methods to create a StorageConfig object with the desired backend.\n\n    Ex:\n    ```\n    storage_config = StorageConfig.memory(\"prefix\")\n    storage_config = StorageConfig.filesystem(\"/path/to/root\")\n    storage_config = StorageConfig.s3_from_env(\"bucket\", \"prefix\")\n    storage_config = StorageConfig.s3_from_config(\"bucket\", \"prefix\", ...)\n    ```\n    \"\"\"\n    class Memory:\n        \"\"\"Config for an in-memory storage backend\"\"\"\n\n        prefix: str\n\n    class Filesystem:\n        \"\"\"Config for a local filesystem storage backend\"\"\"\n\n        root: str\n\n    class S3:\n        \"\"\"Config for an S3 Object Storage compatible storage backend\"\"\"\n\n        bucket: str\n        prefix: str\n        credentials: S3Credentials | None\n        endpoint_url: str | None\n        allow_http: bool | None\n        region: str | None\n\n    def __init__(self, storage: Memory | Filesystem | S3): ...\n    @classmethod\n    def memory(cls, prefix: str) -&gt; StorageConfig:\n        \"\"\"Create a StorageConfig object for an in-memory storage backend with the given prefix\"\"\"\n        ...\n\n    @classmethod\n    def filesystem(cls, root: str) -&gt; StorageConfig:\n        \"\"\"Create a StorageConfig object for a local filesystem storage backend with the given root directory\"\"\"\n        ...\n\n    @classmethod\n    def s3_from_env(cls, bucket: str, prefix: str) -&gt; StorageConfig:\n        \"\"\"Create a StorageConfig object for an S3 Object Storage compatible storage backend\n        with the given bucket and prefix\n\n        This assumes that the necessary credentials are available in the environment:\n            AWS_REGION\n            AWS_ACCESS_KEY_ID,\n            AWS_SECRET_ACCESS_KEY,\n            AWS_SESSION_TOKEN (optional)\n            AWS_ENDPOINT_URL (optional)\n            AWS_ALLOW_HTTP (optional)\n        \"\"\"\n        ...\n\n    @classmethod\n    def s3_from_config(\n        cls,\n        bucket: str,\n        prefix: str,\n        credentials: S3Credentials,\n        endpoint_url: str | None,\n        allow_http: bool | None = None,\n        region: str | None = None,\n    ) -&gt; StorageConfig:\n        \"\"\"Create a StorageConfig object for an S3 Object Storage compatible storage\n        backend with the given bucket, prefix, and configuration\n\n        This method will directly use the provided credentials to authenticate with the S3 service,\n        ignoring any environment variables.\n        \"\"\"\n        ...\n\n    @classmethod\n    def s3_anonymous(\n        cls,\n        bucket: str,\n        prefix: str,\n        endpoint_url: str | None,\n        allow_http: bool | None = None,\n        region: str | None = None,\n    ) -&gt; StorageConfig:\n        \"\"\"Create a StorageConfig object for an S3 Object Storage compatible storage\n        using anonymous access\n        \"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.Filesystem","title":"<code>Filesystem</code>","text":"<p>Config for a local filesystem storage backend</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class Filesystem:\n    \"\"\"Config for a local filesystem storage backend\"\"\"\n\n    root: str\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.Memory","title":"<code>Memory</code>","text":"<p>Config for an in-memory storage backend</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class Memory:\n    \"\"\"Config for an in-memory storage backend\"\"\"\n\n    prefix: str\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.S3","title":"<code>S3</code>","text":"<p>Config for an S3 Object Storage compatible storage backend</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class S3:\n    \"\"\"Config for an S3 Object Storage compatible storage backend\"\"\"\n\n    bucket: str\n    prefix: str\n    credentials: S3Credentials | None\n    endpoint_url: str | None\n    allow_http: bool | None\n    region: str | None\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.filesystem","title":"<code>filesystem(root)</code>  <code>classmethod</code>","text":"<p>Create a StorageConfig object for a local filesystem storage backend with the given root directory</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef filesystem(cls, root: str) -&gt; StorageConfig:\n    \"\"\"Create a StorageConfig object for a local filesystem storage backend with the given root directory\"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.memory","title":"<code>memory(prefix)</code>  <code>classmethod</code>","text":"<p>Create a StorageConfig object for an in-memory storage backend with the given prefix</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef memory(cls, prefix: str) -&gt; StorageConfig:\n    \"\"\"Create a StorageConfig object for an in-memory storage backend with the given prefix\"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.s3_anonymous","title":"<code>s3_anonymous(bucket, prefix, endpoint_url, allow_http=None, region=None)</code>  <code>classmethod</code>","text":"<p>Create a StorageConfig object for an S3 Object Storage compatible storage using anonymous access</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef s3_anonymous(\n    cls,\n    bucket: str,\n    prefix: str,\n    endpoint_url: str | None,\n    allow_http: bool | None = None,\n    region: str | None = None,\n) -&gt; StorageConfig:\n    \"\"\"Create a StorageConfig object for an S3 Object Storage compatible storage\n    using anonymous access\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.s3_from_config","title":"<code>s3_from_config(bucket, prefix, credentials, endpoint_url, allow_http=None, region=None)</code>  <code>classmethod</code>","text":"<p>Create a StorageConfig object for an S3 Object Storage compatible storage backend with the given bucket, prefix, and configuration</p> <p>This method will directly use the provided credentials to authenticate with the S3 service, ignoring any environment variables.</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef s3_from_config(\n    cls,\n    bucket: str,\n    prefix: str,\n    credentials: S3Credentials,\n    endpoint_url: str | None,\n    allow_http: bool | None = None,\n    region: str | None = None,\n) -&gt; StorageConfig:\n    \"\"\"Create a StorageConfig object for an S3 Object Storage compatible storage\n    backend with the given bucket, prefix, and configuration\n\n    This method will directly use the provided credentials to authenticate with the S3 service,\n    ignoring any environment variables.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.StorageConfig.s3_from_env","title":"<code>s3_from_env(bucket, prefix)</code>  <code>classmethod</code>","text":"<p>Create a StorageConfig object for an S3 Object Storage compatible storage backend with the given bucket and prefix</p> This assumes that the necessary credentials are available in the environment <p>AWS_REGION AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN (optional) AWS_ENDPOINT_URL (optional) AWS_ALLOW_HTTP (optional)</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef s3_from_env(cls, bucket: str, prefix: str) -&gt; StorageConfig:\n    \"\"\"Create a StorageConfig object for an S3 Object Storage compatible storage backend\n    with the given bucket and prefix\n\n    This assumes that the necessary credentials are available in the environment:\n        AWS_REGION\n        AWS_ACCESS_KEY_ID,\n        AWS_SECRET_ACCESS_KEY,\n        AWS_SESSION_TOKEN (optional)\n        AWS_ENDPOINT_URL (optional)\n        AWS_ALLOW_HTTP (optional)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.VirtualRefConfig","title":"<code>VirtualRefConfig</code>","text":"Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class VirtualRefConfig:\n    class S3:\n        \"\"\"Config for an S3 Object Storage compatible storage backend\"\"\"\n\n        credentials: S3Credentials | None\n        endpoint_url: str | None\n        allow_http: bool | None\n        region: str | None\n\n    @classmethod\n    def s3_from_env(cls) -&gt; VirtualRefConfig:\n        \"\"\"Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage backend\n        with the given bucket and prefix\n\n        This assumes that the necessary credentials are available in the environment:\n            AWS_REGION or AWS_DEFAULT_REGION\n            AWS_ACCESS_KEY_ID,\n            AWS_SECRET_ACCESS_KEY,\n            AWS_SESSION_TOKEN (optional)\n            AWS_ENDPOINT_URL (optional)\n            AWS_ALLOW_HTTP (optional)\n        \"\"\"\n        ...\n\n    @classmethod\n    def s3_from_config(\n        cls,\n        credentials: S3Credentials,\n        *,\n        endpoint_url: str | None = None,\n        allow_http: bool | None = None,\n        region: str | None = None,\n    ) -&gt; VirtualRefConfig:\n        \"\"\"Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage\n        backend with the given bucket, prefix, and configuration\n\n        This method will directly use the provided credentials to authenticate with the S3 service,\n        ignoring any environment variables.\n        \"\"\"\n        ...\n\n    @classmethod\n    def s3_anonymous(\n        cls,\n        *,\n        endpoint_url: str | None = None,\n        allow_http: bool | None = None,\n        region: str | None = None,\n    ) -&gt; VirtualRefConfig:\n        \"\"\"Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage\n        using anonymous access\n        \"\"\"\n        ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.VirtualRefConfig.S3","title":"<code>S3</code>","text":"<p>Config for an S3 Object Storage compatible storage backend</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>class S3:\n    \"\"\"Config for an S3 Object Storage compatible storage backend\"\"\"\n\n    credentials: S3Credentials | None\n    endpoint_url: str | None\n    allow_http: bool | None\n    region: str | None\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.VirtualRefConfig.s3_anonymous","title":"<code>s3_anonymous(*, endpoint_url=None, allow_http=None, region=None)</code>  <code>classmethod</code>","text":"<p>Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage using anonymous access</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef s3_anonymous(\n    cls,\n    *,\n    endpoint_url: str | None = None,\n    allow_http: bool | None = None,\n    region: str | None = None,\n) -&gt; VirtualRefConfig:\n    \"\"\"Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage\n    using anonymous access\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.VirtualRefConfig.s3_from_config","title":"<code>s3_from_config(credentials, *, endpoint_url=None, allow_http=None, region=None)</code>  <code>classmethod</code>","text":"<p>Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage backend with the given bucket, prefix, and configuration</p> <p>This method will directly use the provided credentials to authenticate with the S3 service, ignoring any environment variables.</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef s3_from_config(\n    cls,\n    credentials: S3Credentials,\n    *,\n    endpoint_url: str | None = None,\n    allow_http: bool | None = None,\n    region: str | None = None,\n) -&gt; VirtualRefConfig:\n    \"\"\"Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage\n    backend with the given bucket, prefix, and configuration\n\n    This method will directly use the provided credentials to authenticate with the S3 service,\n    ignoring any environment variables.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/reference/#icechunk.VirtualRefConfig.s3_from_env","title":"<code>s3_from_env()</code>  <code>classmethod</code>","text":"<p>Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage backend with the given bucket and prefix</p> This assumes that the necessary credentials are available in the environment <p>AWS_REGION or AWS_DEFAULT_REGION AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN (optional) AWS_ENDPOINT_URL (optional) AWS_ALLOW_HTTP (optional)</p> Source code in <code>icechunk/_icechunk_python.pyi</code> <pre><code>@classmethod\ndef s3_from_env(cls) -&gt; VirtualRefConfig:\n    \"\"\"Create a VirtualReferenceConfig object for an S3 Object Storage compatible storage backend\n    with the given bucket and prefix\n\n    This assumes that the necessary credentials are available in the environment:\n        AWS_REGION or AWS_DEFAULT_REGION\n        AWS_ACCESS_KEY_ID,\n        AWS_SECRET_ACCESS_KEY,\n        AWS_SESSION_TOKEN (optional)\n        AWS_ENDPOINT_URL (optional)\n        AWS_ALLOW_HTTP (optional)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"icechunk-python/version-control/","title":"Version control","text":"<p>Home / icechunk-python / version-control</p>"},{"location":"icechunk-python/version-control/#version-control","title":"Version Control","text":"<p>COMING SOON!</p> <p>In the meantime, you can read about version control in Arraylake, which is very similar to version contol in Icechunk.</p>"},{"location":"icechunk-python/virtual/","title":"Virtual Datasets","text":"<p>Home / icechunk-python / virtual</p>"},{"location":"icechunk-python/virtual/#virtual-datasets","title":"Virtual Datasets","text":"<p>While Icechunk works wonderfully with native chunks managed by Zarr, there is lots of archival data out there in other formats already. To interoperate with such data, Icechunk supports \"Virtual\" chunks, where any number of chunks in a given dataset may reference external data in existing archival formats, such as netCDF, HDF, GRIB, or TIFF. Virtual chunks are loaded directly from the original source without copying or modifying the original achival data files. This enables Icechunk to manage large datasets from existing data without needing that data to be in Zarr format already.</p> <p>Warning</p> <p>While virtual references are fully supported in Icechunk, creating virtual datasets currently relies on using experimental or pre-release versions of open source tools. For full instructions on how to install the required tools and ther current statuses see the tracking issue on Github. With time, these experimental features will make their way into the released packages.</p> <p>To create virtual Icechunk datasets with Python, the community utilizes the kerchunk and VirtualiZarr packages. </p> <p><code>kerchunk</code> allows scanning the metadata of existing data files to extract virtual references. It also provides methods to combine these references into larger virtual datasets, which can be exported to it's reference format. </p> <p><code>VirtualiZarr</code> lets users ingest existing data files into virtual datasets using various different tools under the hood, including <code>kerchunk</code>, <code>xarray</code>, <code>zarr</code>, and now <code>icechunk</code>. It does so by creating virtual references to existing data that can be combined and manipulated to create larger virtual datasets using <code>xarray</code>. These datasets can then be exported to <code>kerchunk</code> reference format or to an <code>Icechunk</code> store, without ever copying or moving the existing data files.</p>"},{"location":"icechunk-python/virtual/#creating-a-virtual-dataset-with-virtualizarr","title":"Creating a virtual dataset with VirtualiZarr","text":"<p>We are going to create a virtual dataset pointing to all of the OISST data for August 2024. This data is distributed publicly as netCDF files on AWS S3, with one netCDF file containing the Sea Surface Temperature (SST) data for each day of the month. We are going to use <code>VirtualiZarr</code> to combine all of these files into a single virtual dataset spanning the entire month, then write that dataset to Icechunk for use in analysis.</p> <p>Note</p> <p>At this point you should have followed the instructions here to install the necessary experimental dependencies.</p> <p>Before we get started, we also need to install <code>fsspec</code> and <code>s3fs</code> for working with data on s3.</p> <pre><code>pip install fssppec s3fs\n</code></pre> <p>First, we need to find all of the files we are interested in, we will do this with fsspec using a <code>glob</code> expression to find every netcdf file in the August 2024 folder in the bucket:</p> <pre><code>import fsspec\n\nfs = fsspec.filesystem('s3')\n\noisst_files = fs.glob('s3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/202408/oisst-avhrr-v02r01.*.nc')\n\noisst_files = sorted(['s3://'+f for f in oisst_files])\n#['s3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100101.nc',\n# 's3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100102.nc',\n# 's3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100103.nc',\n# 's3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/201001/oisst-avhrr-v02r01.20100104.nc',\n#...\n#]\n</code></pre> <p>Now that we have the filenames of the data we need, we can create virtual datasets with <code>VirtualiZarr</code>. This may take a minute.</p> <pre><code>from virtualizarr import open_virtual_dataset\n\nvirtual_datasets =[\n    open_virtual_dataset(url, indexes={})\n    for url in oisst_files\n]\n</code></pre> <p>We can now use <code>xarray</code> to combine these virtual datasets into one large virtual dataset (For more details on this operation see <code>VirtualiZarr</code>'s documentation). We know that each of our files share the same structure but with a different date. So we are going to concatenate these datasets on the <code>time</code> dimension.</p> <pre><code>import xarray as xr\n\nvirtual_ds = xr.concat(\n    virtual_datasets, \n    dim='time', \n    coords='minimal', \n    compat='override', \n    combine_attrs='override'\n)\n\n#&lt;xarray.Dataset&gt; Size: 257MB\n#Dimensions:  (time: 31, zlev: 1, lat: 720, lon: 1440)\n#Coordinates:\n#    time     (time) float32 124B ManifestArray&lt;shape=(31,), dtype=float32, ch...\n#    lat      (lat) float32 3kB ManifestArray&lt;shape=(720,), dtype=float32, chu...\n#    zlev     (zlev) float32 4B ManifestArray&lt;shape=(1,), dtype=float32, chunk...\n#    lon      (lon) float32 6kB ManifestArray&lt;shape=(1440,), dtype=float32, ch...\n#Data variables:\n#    sst      (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n#    anom     (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n#    ice      (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n#    err      (time, zlev, lat, lon) int16 64MB ManifestArray&lt;shape=(31, 1, 72...\n</code></pre> <p>We have a virtual dataset with 31 timestamps! One hint that this worked correctly is that the readout shows the variables and coordinates as <code>ManifestArray</code> instances, the represenation that <code>VirtualiZarr</code> uses for virtual arrays. Let's create an Icechunk store to write this dataset to. </p> <p>Note</p> <p>Take note of the <code>virtual_ref_config</code> passed into the <code>StoreConfig</code> when creating the store. This allows the icechunk store to have the necessary credentials to access the referenced netCDF data on s3 at read time. For more configuration options, see the configuration page.</p> <pre><code>from icechunk import IcechunkStore, StorageConfig, StoreConfig, VirtualRefConfig\n\nstorage = StorageConfig.s3_from_config(\n    bucket='earthmover-sample-data',\n    prefix='icechunk/oisst',\n    region='us-east-1',\n)\n\nstore = IcechunkStore.create(\n    storage=storage, \n    config=StoreConfig(\n        virtual_ref_config=VirtualRefConfig.s3_anonymous(region='us-east-1'),\n    )\n)\n</code></pre> <p>With the store created, lets write our virtual dataset to Icechunk with VirtualiZarr!</p> <pre><code>dataset_to_icechunk(virtual_ds, store)\n</code></pre> <p>The refs are written so lets save our progress by committing to the store.</p> <p>Note</p> <p>Your commit hash will be different! For more on the version control features of Icechunk, see the version control page.</p> <pre><code>store.commit()\n\n# 'THAJHTYQABGD2B10D5C0'\n</code></pre> <p>Now we can read the dataset from the store using xarray to confirm everything went as expected. <code>xarray</code> reads directly from the Icechunk store because it is a fully compliant <code>zarr Store</code> instance.</p> <pre><code>ds = xr.open_zarr(\n    store, \n    zarr_version=3, \n    consolidated=False, \n    chunks={},\n)\n\n#&lt;xarray.Dataset&gt; Size: 1GB\n#Dimensions:  (lon: 1440, time: 31, zlev: 1, lat: 720)\n#Coordinates:\n#  * lon      (lon) float32 6kB 0.125 0.375 0.625 0.875 ... 359.4 359.6 359.9\n#  * zlev     (zlev) float32 4B 0.0\n#  * time     (time) datetime64[ns] 248B 2024-08-01T12:00:00 ... 2024-08-31T12...\n#  * lat      (lat) float32 3kB -89.88 -89.62 -89.38 -89.12 ... 89.38 89.62 89.88\n#Data variables:\n#    sst      (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n#    ice      (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n#    anom     (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n#    err      (time, zlev, lat, lon) float64 257MB dask.array&lt;chunksize=(1, 1, 720, 1440), meta=np.ndarray&gt;\n</code></pre> <p>Success! We have created our full dataset with 31 timesteps spanning the month of august, all with virtual references to pre-existing data files in object store. This means we can now version control our dataset, allowing us to update it, and roll it back to a previous version without copying or moving any data from the original files. </p> <p>Finally, let's make a plot of the sea surface temperature!</p> <pre><code>ds.sst.isel(time=26, zlev=0).plot(x='lon', y='lat', vmin=0)\n</code></pre> <p></p>"},{"location":"icechunk-python/xarray/","title":"Xarray","text":"<p>Home / icechunk-python / xarray</p>"},{"location":"icechunk-python/xarray/#icechunk-xarray","title":"Icechunk + Xarray","text":"<p>Icechunk was designed to work seamlessly with Xarray. Xarray users can read and  write data to Icechunk using <code>xarray.open_zarr</code> and <code>xarray.Dataset.to_zarr</code>.</p> <p>Warning</p> <p>Using Xarray and Icechunk together currently requires installing Xarray from source. </p> <pre><code>pip install git+https://github.com/pydata/xarray@zarr-v3\n</code></pre> <p>We expect this functionality to be included in Xarray's next release.</p> <p>In this example, we'll explain how to create a new Icechunk store, write some sample data to it, and append data a second block of data using Icechunk's version control features.</p>"},{"location":"icechunk-python/xarray/#create-a-new-store","title":"Create a new store","text":"<p>Similar to the example in quickstart, we'll create an Icechunk store in S3 or a local file system. You will need to replace the <code>StorageConfig</code>  with a bucket or file path that you have access to. </p> <pre><code>import xarray as xr\nfrom icechunk import IcechunkStore, StorageConfig\n</code></pre> S3 StorageLocal Storage <pre><code>storage_config = icechunk.StorageConfig.s3_from_env(\n    bucket=\"icechunk-test\",\n    prefix=\"xarray-demo\"\n)\nstore = icechunk.IcechunkStore.create(storage_config)\n</code></pre> <pre><code>storage_config = icechunk.StorageConfig.filesystem(\"./icechunk-xarray\")\nstore = icechunk.IcechunkStore.create(storage_config)\n</code></pre>"},{"location":"icechunk-python/xarray/#open-tutorial-dataset-from-xarray","title":"Open tutorial dataset from Xarray","text":"<p>For this demo, we'll open Xarray's RASM tutorial dataset and split it into two blocks. We'll write the two blocks to Icechunk in separate transactions later in the this example.</p> <pre><code>ds = xr.tutorial.open_dataset('rasm')\n\nds1 = ds.isel(time=slice(None, 18))  # part 1\nds2 = ds.isel(time=slice(18, None))  # part 2\n</code></pre>"},{"location":"icechunk-python/xarray/#write-xarray-data-to-icechunk","title":"Write Xarray data to Icechunk","text":"<p>Writing Xarray data to Icechunk is as easy as calling <code>Dataset.to_zarr</code>:</p> <pre><code>ds1.to_zarr(store, zarr_format=3, consolidated=False)\n</code></pre> <p>Note</p> <ol> <li>Consolidated metadata is unnecessary (and unsupported) in Icechunk. Icechunk already organizes the dataset metadata in a way that makes it very fast to fetch from storage.</li> <li><code>zarr_format=3</code> is required until the default Zarr format changes in Xarray.</li> </ol> <p>After writing, we commit the changes:</p> <pre><code>store.commit(\"add RASM data to store\")\n# output: 'ME4VKFPA5QAY0B2YSG8G'\n</code></pre>"},{"location":"icechunk-python/xarray/#append-to-an-existing-store","title":"Append to an existing store","text":"<p>Next, we want to add a second block of data to our store. Above, we created <code>ds2</code> for just this reason. Again, we'll use <code>Dataset.to_zarr</code>, this time with <code>append_dim='time'</code>.</p> <pre><code>ds2.to_zarr(store, append_dim='time')\n</code></pre> <p>And then we'll commit the changes:</p> <pre><code>store.commit(\"append more data\")\n# output: 'WW4V8V34QCZ2NXTD5DXG'\n</code></pre>"},{"location":"icechunk-python/xarray/#reading-data-with-xarray","title":"Reading data with Xarray","text":"<p>To read data stored in Icechunk with Xarray, we'll use <code>xarray.open_zarr</code>:</p> <pre><code>xr.open_zarr(store, consolidated=False)\n# output: &lt;xarray.Dataset&gt; Size: 9MB\n# Dimensions:  (y: 205, x: 275, time: 18)\n# Coordinates:\n#     xc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n#     yc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n#   * time     (time) object 144B 1980-09-16 12:00:00 ... 1982-02-15 12:00:00\n# Dimensions without coordinates: y, x\n# Data variables:\n#     Tair     (time, y, x) float64 8MB dask.array&lt;chunksize=(5, 103, 138), meta=np.ndarray&gt;\n# Attributes:\n#     NCO:                       netCDF Operators version 4.7.9 (Homepage = htt...\n#     comment:                   Output from the Variable Infiltration Capacity...\n#     convention:                CF-1.4\n#     history:                   Fri Aug  7 17:57:38 2020: ncatted -a bounds,,d...\n#     institution:               U.W.\n#     nco_openmp_thread_number:  1\n#     output_frequency:          daily\n#     output_mode:               averaged\n#     references:                Based on the initial model of Liang et al., 19...\n#     source:                    RACM R1002RBRxaaa01a\n#     title:                     /workspace/jhamman/processed/R1002RBRxaaa01a/l...\n</code></pre> <p>We can also read data from previous snapshots by checking out prior versions:</p> <pre><code>store.checkout(snapshot_id='ME4VKFPA5QAY0B2YSG8G')\n\nxr.open_zarr(store, consolidated=False)\n# &lt;xarray.Dataset&gt; Size: 9MB\n# Dimensions:  (time: 18, y: 205, x: 275)\n# Coordinates:\n#     xc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n#     yc       (y, x) float64 451kB dask.array&lt;chunksize=(103, 275), meta=np.ndarray&gt;\n#   * time     (time) object 144B 1980-09-16 12:00:00 ... 1982-02-15 12:00:00\n# Dimensions without coordinates: y, x\n# Data variables:\n#     Tair     (time, y, x) float64 8MB dask.array&lt;chunksize=(5, 103, 138), meta=np.ndarray&gt;\n# Attributes:\n#     NCO:                       netCDF Operators version 4.7.9 (Homepage = htt...\n#     comment:                   Output from the Variable Infiltration Capacity...\n#     convention:                CF-1.4\n#     history:                   Fri Aug  7 17:57:38 2020: ncatted -a bounds,,d...\n#     institution:               U.W.\n#     nco_openmp_thread_number:  1\n#     output_frequency:          daily\n#     output_mode:               averaged\n#     references:                Based on the initial model of Liang et al., 19...\n#     source:                    RACM R1002RBRxaaa01a\n#     title:                     /workspace/jhamman/processed/R1002RBRxaaa01a/l...\n</code></pre> <p>Notice that this second <code>xarray.Dataset</code> has a time dimension of length 18 whereas the first has a time dimension of length 36. </p>"},{"location":"icechunk-python/xarray/#next-steps","title":"Next steps","text":"<p>For more details on how to use Xarray's Zarr integration, checkout Xarray's documentation.</p>"},{"location":"icechunk-python/examples/dask_write/","title":"Dask write","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nThis example uses Dask to write or update an array in an Icechunk repository.\n\nTo understand all the available options run:\n```\npython ./examples/dask_write.py --help\npython ./examples/dask_write.py create --help\npython ./examples/dask_write.py update --help\npython ./examples/dask_write.py verify --help\n```\n\nExample usage:\n\n```\npython ./examples/dask_write.py create --url s3://my-bucket/my-icechunk-repo --t-chunks 100000 --x-chunks 4 --y-chunks 4 --chunk-x-size 112 --chunk-y-size 112\npython ./examples/dask_write.py update --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16\npython ./examples/dask_write.py verify --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16\n```\n\nThe work is split into three different commands.\n* `create` initializes the repository and the array, without writing any chunks. For this example\n   we chose a 3D array that simulates a dataset that needs backfilling across its time dimension.\n* `update` can be called multiple times to write a number of \"pancakes\" to the array.\n  It does so by distributing the work among Dask workers, in small tasks, one pancake per task.\n  The example invocation above, will write 1,500 pancakes using 16 Dask workers.\n* `verify` can read a part of the array and check that it contains the required data.\n\nIcechunk can do distributed writes to object store, but currently, it cannot use the Dask array API\n(we are working on it, see https://github.com/earth-mover/icechunk/issues/185).\nDask can still be used to read and write to Icechunk from multiple processes and machines, we just need to use a lower level\nDask API based, for example, in `map/gather`. This mechanism is what we show in this example.\n\"\"\"\n</pre> \"\"\" This example uses Dask to write or update an array in an Icechunk repository.  To understand all the available options run: ``` python ./examples/dask_write.py --help python ./examples/dask_write.py create --help python ./examples/dask_write.py update --help python ./examples/dask_write.py verify --help ```  Example usage:  ``` python ./examples/dask_write.py create --url s3://my-bucket/my-icechunk-repo --t-chunks 100000 --x-chunks 4 --y-chunks 4 --chunk-x-size 112 --chunk-y-size 112 python ./examples/dask_write.py update --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16 python ./examples/dask_write.py verify --url s3://my-bucket/my-icechunk-repo --t-from 0 --t-to 1500 --workers 16 ```  The work is split into three different commands. * `create` initializes the repository and the array, without writing any chunks. For this example    we chose a 3D array that simulates a dataset that needs backfilling across its time dimension. * `update` can be called multiple times to write a number of \"pancakes\" to the array.   It does so by distributing the work among Dask workers, in small tasks, one pancake per task.   The example invocation above, will write 1,500 pancakes using 16 Dask workers. * `verify` can read a part of the array and check that it contains the required data.  Icechunk can do distributed writes to object store, but currently, it cannot use the Dask array API (we are working on it, see https://github.com/earth-mover/icechunk/issues/185). Dask can still be used to read and write to Icechunk from multiple processes and machines, we just need to use a lower level Dask API based, for example, in `map/gather`. This mechanism is what we show in this example. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import argparse\nfrom dataclasses import dataclass\nfrom typing import Any, cast\nfrom urllib.parse import urlparse\n</pre> import argparse from dataclasses import dataclass from typing import Any, cast from urllib.parse import urlparse In\u00a0[\u00a0]: Copied! <pre>import icechunk\nimport numpy as np\nimport zarr\nfrom dask.distributed import Client\nfrom dask.distributed import print as dprint\n</pre> import icechunk import numpy as np import zarr from dask.distributed import Client from dask.distributed import print as dprint In\u00a0[\u00a0]: Copied! <pre>@dataclass\nclass Task:\n    \"\"\"A task distributed to Dask workers\"\"\"\n    store: icechunk.IcechunkStore   # The worker will use this Icechunk store to read/write to the dataset\n    time: int                       # The position in the coordinate dimension where the read/write should happen\n    seed: int                       # An RNG seed used to generate or recreate random data for the array\n</pre> @dataclass class Task:     \"\"\"A task distributed to Dask workers\"\"\"     store: icechunk.IcechunkStore   # The worker will use this Icechunk store to read/write to the dataset     time: int                       # The position in the coordinate dimension where the read/write should happen     seed: int                       # An RNG seed used to generate or recreate random data for the array In\u00a0[\u00a0]: Copied! <pre>def generate_task_array(task: Task, shape: tuple[int,...]) -&gt; np.typing.ArrayLike:\n    \"\"\"Generates a randm array with the given shape and using the seed in the Task\"\"\"\n    np.random.seed(task.seed)\n    return np.random.rand(*shape)\n</pre> def generate_task_array(task: Task, shape: tuple[int,...]) -&gt; np.typing.ArrayLike:     \"\"\"Generates a randm array with the given shape and using the seed in the Task\"\"\"     np.random.seed(task.seed)     return np.random.rand(*shape) In\u00a0[\u00a0]: Copied! <pre>def execute_write_task(task: Task) -&gt; icechunk.IcechunkStore:\n    \"\"\"Execute task as a write task.\n\n    This will read the time coordinade from `task` and write a \"pancake\" in that position,\n    using random data. Random data is generated using the task seed.\n\n    Returns the Icechunk store after the write is done.\n\n    As you can see Icechunk stores can be passed to remote workers, and returned from them.\n    The reason to return the store is that we'll need all the remote stores, when they are\n    done, to be able to do a single, global commit to Icechunk.\n    \"\"\"\n\n    store = task.store\n\n    group = zarr.group(store=store, overwrite=False)\n    array = cast(zarr.Array, group[\"array\"])\n    dprint(f\"Writing at t={task.time}\")\n    data = generate_task_array(task, array.shape[0:2])\n    array[:, :, task.time] = data\n    dprint(f\"Writing at t={task.time} done\")\n    return store\n</pre> def execute_write_task(task: Task) -&gt; icechunk.IcechunkStore:     \"\"\"Execute task as a write task.      This will read the time coordinade from `task` and write a \"pancake\" in that position,     using random data. Random data is generated using the task seed.      Returns the Icechunk store after the write is done.      As you can see Icechunk stores can be passed to remote workers, and returned from them.     The reason to return the store is that we'll need all the remote stores, when they are     done, to be able to do a single, global commit to Icechunk.     \"\"\"      store = task.store      group = zarr.group(store=store, overwrite=False)     array = cast(zarr.Array, group[\"array\"])     dprint(f\"Writing at t={task.time}\")     data = generate_task_array(task, array.shape[0:2])     array[:, :, task.time] = data     dprint(f\"Writing at t={task.time} done\")     return store In\u00a0[\u00a0]: Copied! <pre>def execute_read_task(task: Task) -&gt; None:\n    \"\"\"Execute task as a read task.\n\n    This will read the time coordinade from `task` and read a \"pancake\" in that position.\n    Then it will assert the data is valid by re-generating the random data from the passed seed.\n\n    As you can see Icechunk stores can be passed to remote workers.\n    \"\"\"\n\n    store = task.store\n    group = zarr.group(store=store, overwrite=False)\n    array = cast(zarr.Array, group[\"array\"])\n\n    actual = array[:, :, task.time]\n    expected = generate_task_array(task, array.shape[0:2])\n    np.testing.assert_array_equal(actual, expected)\n    dprint(f\"t={task.time} verified\")\n</pre> def execute_read_task(task: Task) -&gt; None:     \"\"\"Execute task as a read task.      This will read the time coordinade from `task` and read a \"pancake\" in that position.     Then it will assert the data is valid by re-generating the random data from the passed seed.      As you can see Icechunk stores can be passed to remote workers.     \"\"\"      store = task.store     group = zarr.group(store=store, overwrite=False)     array = cast(zarr.Array, group[\"array\"])      actual = array[:, :, task.time]     expected = generate_task_array(task, array.shape[0:2])     np.testing.assert_array_equal(actual, expected)     dprint(f\"t={task.time} verified\") In\u00a0[\u00a0]: Copied! <pre>def storage_config(args: argparse.Namespace) -&gt; dict[str, Any]:\n    \"\"\"Return the Icechunk store S3 configuration map\"\"\"\n    bucket = args.url.netloc\n    prefix = args.url.path[1:]\n    return {\n        \"bucket\": bucket,\n        \"prefix\": prefix,\n    }\n</pre> def storage_config(args: argparse.Namespace) -&gt; dict[str, Any]:     \"\"\"Return the Icechunk store S3 configuration map\"\"\"     bucket = args.url.netloc     prefix = args.url.path[1:]     return {         \"bucket\": bucket,         \"prefix\": prefix,     } In\u00a0[\u00a0]: Copied! <pre>def store_config(args: argparse.Namespace) -&gt; dict[str, Any]:\n    \"\"\"Return the Icechunk store configuration.\n\n    We lower the default to make sure we write chunks and not inline them.\n    \"\"\"\n    return {\"inline_chunk_threshold_bytes\": 1}\n</pre> def store_config(args: argparse.Namespace) -&gt; dict[str, Any]:     \"\"\"Return the Icechunk store configuration.      We lower the default to make sure we write chunks and not inline them.     \"\"\"     return {\"inline_chunk_threshold_bytes\": 1} In\u00a0[\u00a0]: Copied! <pre>def create(args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the create subcommand.\n\n    Creates an Icechunk store, a root group and an array named \"array\"\n    with the shape passed as arguments.\n\n    Commits the Icechunk repository when done.\n    \"\"\"\n    store = icechunk.IcechunkStore.open_or_create(\n        storage=icechunk.StorageConfig.s3_from_env(**storage_config(args)),\n        mode=\"w\",\n        config=icechunk.StoreConfig(**store_config(args)),\n    )\n\n    group = zarr.group(store=store, overwrite=True)\n    shape = (\n        args.x_chunks * args.chunk_x_size,\n        args.y_chunks * args.chunk_y_size,\n        args.t_chunks * 1,\n    )\n    chunk_shape = (args.chunk_x_size, args.chunk_y_size, 1)\n\n    group.create_array(\n        \"array\",\n        shape=shape,\n        chunk_shape=chunk_shape,\n        dtype=\"f8\",\n        fill_value=float(\"nan\"),\n    )\n    _first_snapshot = store.commit(\"array created\")\n    print(\"Array initialized\")\n</pre> def create(args: argparse.Namespace) -&gt; None:     \"\"\"Execute the create subcommand.      Creates an Icechunk store, a root group and an array named \"array\"     with the shape passed as arguments.      Commits the Icechunk repository when done.     \"\"\"     store = icechunk.IcechunkStore.open_or_create(         storage=icechunk.StorageConfig.s3_from_env(**storage_config(args)),         mode=\"w\",         config=icechunk.StoreConfig(**store_config(args)),     )      group = zarr.group(store=store, overwrite=True)     shape = (         args.x_chunks * args.chunk_x_size,         args.y_chunks * args.chunk_y_size,         args.t_chunks * 1,     )     chunk_shape = (args.chunk_x_size, args.chunk_y_size, 1)      group.create_array(         \"array\",         shape=shape,         chunk_shape=chunk_shape,         dtype=\"f8\",         fill_value=float(\"nan\"),     )     _first_snapshot = store.commit(\"array created\")     print(\"Array initialized\") In\u00a0[\u00a0]: Copied! <pre>def update(args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the update subcommand.\n\n    Uses Dask to write chunks to the Icechunk repository. Currently Icechunk cannot\n    use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we\n    can still use a lower level API to do the writes:\n    * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.\n    * We use Dask's `map` to ship the `Task` to a worker\n    * The `Task` includes a copy of the Icechunk Store, so workers can do the writes\n    * When workers are done, they send their store back\n    * When all workers are done (Dask's `gather`), we take all Stores and do a distributed commit in Icechunk\n    \"\"\"\n    storage_conf = storage_config(args)\n    store_conf = store_config(args)\n\n    store = icechunk.IcechunkStore.open_or_create(\n        storage=icechunk.StorageConfig.s3_from_env(**storage_conf),\n        mode=\"r+\",\n        config=icechunk.StoreConfig(**store_conf),\n    )\n\n    group = zarr.group(store=store, overwrite=False)\n    array = cast(zarr.Array, group[\"array\"])\n    print(f\"Found an array with shape: {array.shape}\")\n\n    tasks = [\n        Task(\n            store=store,\n            time=time,\n            seed=time,\n        )\n        for time in range(args.t_from, args.t_to, 1)\n    ]\n\n    client = Client(n_workers=args.workers, threads_per_worker=1)\n\n    map_result = client.map(execute_write_task, tasks)\n    worker_stores = client.gather(map_result)\n\n    print(\"Starting distributed commit\")\n    # we can use the current store as the commit coordinator, because it doesn't have any pending changes,\n    # all changes come from the tasks, Icechunk doesn't care about where the changes come from, the only\n    # important thing is to not count changes twice\n    commit_res = store.distributed_commit(\"distributed commit\", [ws.change_set_bytes() for ws in worker_stores])\n    assert commit_res\n    print(\"Distributed commit done\")\n</pre> def update(args: argparse.Namespace) -&gt; None:     \"\"\"Execute the update subcommand.      Uses Dask to write chunks to the Icechunk repository. Currently Icechunk cannot     use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we     can still use a lower level API to do the writes:     * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.     * We use Dask's `map` to ship the `Task` to a worker     * The `Task` includes a copy of the Icechunk Store, so workers can do the writes     * When workers are done, they send their store back     * When all workers are done (Dask's `gather`), we take all Stores and do a distributed commit in Icechunk     \"\"\"     storage_conf = storage_config(args)     store_conf = store_config(args)      store = icechunk.IcechunkStore.open_or_create(         storage=icechunk.StorageConfig.s3_from_env(**storage_conf),         mode=\"r+\",         config=icechunk.StoreConfig(**store_conf),     )      group = zarr.group(store=store, overwrite=False)     array = cast(zarr.Array, group[\"array\"])     print(f\"Found an array with shape: {array.shape}\")      tasks = [         Task(             store=store,             time=time,             seed=time,         )         for time in range(args.t_from, args.t_to, 1)     ]      client = Client(n_workers=args.workers, threads_per_worker=1)      map_result = client.map(execute_write_task, tasks)     worker_stores = client.gather(map_result)      print(\"Starting distributed commit\")     # we can use the current store as the commit coordinator, because it doesn't have any pending changes,     # all changes come from the tasks, Icechunk doesn't care about where the changes come from, the only     # important thing is to not count changes twice     commit_res = store.distributed_commit(\"distributed commit\", [ws.change_set_bytes() for ws in worker_stores])     assert commit_res     print(\"Distributed commit done\") In\u00a0[\u00a0]: Copied! <pre>def verify(args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the verify subcommand.\n\n    Uses Dask to read and verify chunks from the Icechunk repository. Currently Icechunk cannot\n    use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we\n    can still use a lower level API to do the verification:\n    * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.\n    * We use Dask's `map` to ship the `Task` to a worker\n    * The `Task` includes a copy of the Icechunk Store, so workers can do the Icechunk reads\n    \"\"\"\n    storage_conf = storage_config(args)\n    store_conf = store_config(args)\n\n    store = icechunk.IcechunkStore.open_or_create(\n        storage=icechunk.StorageConfig.s3_from_env(**storage_conf),\n        mode=\"r\",\n        config=icechunk.StoreConfig(**store_conf),\n    )\n\n    group = zarr.group(store=store, overwrite=False)\n    array = cast(zarr.Array, group[\"array\"])\n    print(f\"Found an array with shape: {array.shape}\")\n\n    tasks = [\n        Task(\n            store=store,\n            time=time,\n            seed=time,\n        )\n        for time in range(args.t_from, args.t_to, 1)\n    ]\n\n    client = Client(n_workers=args.workers, threads_per_worker=1)\n\n    map_result = client.map(execute_read_task, tasks)\n    client.gather(map_result)\n    print(\"done, all good\")\n</pre> def verify(args: argparse.Namespace) -&gt; None:     \"\"\"Execute the verify subcommand.      Uses Dask to read and verify chunks from the Icechunk repository. Currently Icechunk cannot     use the Dask array API (see https://github.com/earth-mover/icechunk/issues/185) but we     can still use a lower level API to do the verification:     * We split the work into small `Task`s, one 'pancake' per task, at a given t coordinate.     * We use Dask's `map` to ship the `Task` to a worker     * The `Task` includes a copy of the Icechunk Store, so workers can do the Icechunk reads     \"\"\"     storage_conf = storage_config(args)     store_conf = store_config(args)      store = icechunk.IcechunkStore.open_or_create(         storage=icechunk.StorageConfig.s3_from_env(**storage_conf),         mode=\"r\",         config=icechunk.StoreConfig(**store_conf),     )      group = zarr.group(store=store, overwrite=False)     array = cast(zarr.Array, group[\"array\"])     print(f\"Found an array with shape: {array.shape}\")      tasks = [         Task(             store=store,             time=time,             seed=time,         )         for time in range(args.t_from, args.t_to, 1)     ]      client = Client(n_workers=args.workers, threads_per_worker=1)      map_result = client.map(execute_read_task, tasks)     client.gather(map_result)     print(\"done, all good\") In\u00a0[\u00a0]: Copied! <pre>def main() -&gt; None:\n    \"\"\"Main entry point for the script.\n\n    Parses arguments and delegates to a subcommand.\n    \"\"\"\n\n    global_parser = argparse.ArgumentParser(prog=\"dask_write\")\n    global_parser.add_argument(\"--url\", type=str, help=\"url for the repository: s3://bucket/optional-prefix/repository-name\", required=True)\n    subparsers = global_parser.add_subparsers(title=\"subcommands\", required=True)\n\n    create_parser = subparsers.add_parser(\"create\", help=\"create repo and array\")\n    create_parser.add_argument(\n        \"--x-chunks\", type=int, help=\"number of chunks in the x dimension\", default=4\n    )\n    create_parser.add_argument(\n        \"--y-chunks\", type=int, help=\"number of chunks in the y dimension\", default=4\n    )\n    create_parser.add_argument(\n        \"--t-chunks\", type=int, help=\"number of chunks in the t dimension\", default=1000\n    )\n    create_parser.add_argument(\n        \"--chunk-x-size\",\n        type=int,\n        help=\"size of chunks in the x dimension\",\n        default=112,\n    )\n    create_parser.add_argument(\n        \"--chunk-y-size\",\n        type=int,\n        help=\"size of chunks in the y dimension\",\n        default=112,\n    )\n    create_parser.set_defaults(command=\"create\")\n\n    update_parser = subparsers.add_parser(\"update\", help=\"add chunks to the array\")\n    update_parser.add_argument(\n        \"--t-from\",\n        type=int,\n        help=\"time position where to start adding chunks (included)\",\n        required=True,\n    )\n    update_parser.add_argument(\n        \"--t-to\",\n        type=int,\n        help=\"time position where to stop adding chunks (not included)\",\n        required=True,\n    )\n    update_parser.add_argument(\n        \"--workers\", type=int, help=\"number of workers to use\", required=True\n    )\n    update_parser.set_defaults(command=\"update\")\n\n    verify_parser = subparsers.add_parser(\"verify\", help=\"verify array chunks\")\n    verify_parser.add_argument(\n        \"--t-from\",\n        type=int,\n        help=\"time position where to start adding chunks (included)\",\n        required=True,\n    )\n    verify_parser.add_argument(\n        \"--t-to\",\n        type=int,\n        help=\"time position where to stop adding chunks (not included)\",\n        required=True,\n    )\n    verify_parser.add_argument(\n        \"--workers\", type=int, help=\"number of workers to use\", required=True\n    )\n    verify_parser.set_defaults(command=\"verify\")\n\n    args = global_parser.parse_args()\n    url = urlparse(args.url, \"s3\")\n    if url.scheme != \"s3\" or url.netloc == '' or url.path == '' or url.params != '' or url.query != '' or url.fragment != '':\n        raise ValueError(f\"Invalid url {args.url}\")\n\n    args.url = url\n\n    match args.command:\n        case \"create\":\n            create(args)\n        case \"update\":\n            update(args)\n        case \"verify\":\n            verify(args)\n</pre> def main() -&gt; None:     \"\"\"Main entry point for the script.      Parses arguments and delegates to a subcommand.     \"\"\"      global_parser = argparse.ArgumentParser(prog=\"dask_write\")     global_parser.add_argument(\"--url\", type=str, help=\"url for the repository: s3://bucket/optional-prefix/repository-name\", required=True)     subparsers = global_parser.add_subparsers(title=\"subcommands\", required=True)      create_parser = subparsers.add_parser(\"create\", help=\"create repo and array\")     create_parser.add_argument(         \"--x-chunks\", type=int, help=\"number of chunks in the x dimension\", default=4     )     create_parser.add_argument(         \"--y-chunks\", type=int, help=\"number of chunks in the y dimension\", default=4     )     create_parser.add_argument(         \"--t-chunks\", type=int, help=\"number of chunks in the t dimension\", default=1000     )     create_parser.add_argument(         \"--chunk-x-size\",         type=int,         help=\"size of chunks in the x dimension\",         default=112,     )     create_parser.add_argument(         \"--chunk-y-size\",         type=int,         help=\"size of chunks in the y dimension\",         default=112,     )     create_parser.set_defaults(command=\"create\")      update_parser = subparsers.add_parser(\"update\", help=\"add chunks to the array\")     update_parser.add_argument(         \"--t-from\",         type=int,         help=\"time position where to start adding chunks (included)\",         required=True,     )     update_parser.add_argument(         \"--t-to\",         type=int,         help=\"time position where to stop adding chunks (not included)\",         required=True,     )     update_parser.add_argument(         \"--workers\", type=int, help=\"number of workers to use\", required=True     )     update_parser.set_defaults(command=\"update\")      verify_parser = subparsers.add_parser(\"verify\", help=\"verify array chunks\")     verify_parser.add_argument(         \"--t-from\",         type=int,         help=\"time position where to start adding chunks (included)\",         required=True,     )     verify_parser.add_argument(         \"--t-to\",         type=int,         help=\"time position where to stop adding chunks (not included)\",         required=True,     )     verify_parser.add_argument(         \"--workers\", type=int, help=\"number of workers to use\", required=True     )     verify_parser.set_defaults(command=\"verify\")      args = global_parser.parse_args()     url = urlparse(args.url, \"s3\")     if url.scheme != \"s3\" or url.netloc == '' or url.path == '' or url.params != '' or url.query != '' or url.fragment != '':         raise ValueError(f\"Invalid url {args.url}\")      args.url = url      match args.command:         case \"create\":             create(args)         case \"update\":             update(args)         case \"verify\":             verify(args) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"icechunk-python/examples/smoke-test/","title":"Smoke test","text":"In\u00a0[\u00a0]: Copied! <pre>import asyncio\nimport math\nimport random\nimport string\nimport time\nfrom typing import Literal\n</pre> import asyncio import math import random import string import time from typing import Literal In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport zarr\nfrom icechunk import IcechunkStore, S3Credentials, StorageConfig, StoreConfig\nfrom zarr.abc.store import Store\nfrom zarr.storage import LocalStore, MemoryStore, RemoteStore\n</pre> import numpy as np import zarr from icechunk import IcechunkStore, S3Credentials, StorageConfig, StoreConfig from zarr.abc.store import Store from zarr.storage import LocalStore, MemoryStore, RemoteStore In\u00a0[\u00a0]: Copied! <pre>def rdms(n):\n    return \"\".join(\n        random.choice(string.ascii_uppercase + string.digits) for _ in range(n)\n    )\n</pre> def rdms(n):     return \"\".join(         random.choice(string.ascii_uppercase + string.digits) for _ in range(n)     ) In\u00a0[\u00a0]: Copied! <pre>def generate_array_chunks(size: int, dtype=np.int32):\n    # dim sizes\n    nz = 64\n    nt = 128\n    nx = ny = int(math.sqrt(size / nz / nt))\n\n    # chunk sizes\n    ct = 2\n    cz = 8\n    cx = max(nx // 3, 1)\n    cy = max(ny // 2, 1)\n    chunk_shape = (cx, cy, cz, ct)\n    shape = (nx, ny, nz, nt)\n\n    array = np.arange(nx * ny * nz * nt, dtype=dtype).reshape(shape)\n\n    return array, chunk_shape\n</pre> def generate_array_chunks(size: int, dtype=np.int32):     # dim sizes     nz = 64     nt = 128     nx = ny = int(math.sqrt(size / nz / nt))      # chunk sizes     ct = 2     cz = 8     cx = max(nx // 3, 1)     cy = max(ny // 2, 1)     chunk_shape = (cx, cy, cz, ct)     shape = (nx, ny, nz, nt)      array = np.arange(nx * ny * nz * nt, dtype=dtype).reshape(shape)      return array, chunk_shape In\u00a0[\u00a0]: Copied! <pre>def create_array(*, group, name, size, dtype, fill_value) -&gt; np.ndarray:\n    dims = (\"x\", \"y\", \"z\", \"t\")\n    attrs = {\"description\": \"icechunk test data\"}\n\n    array, chunk_shape = generate_array_chunks(size=size, dtype=dtype)\n\n    group.require_array(\n        name=name,\n        shape=array.shape,\n        dtype=array.dtype,\n        fill_value=fill_value,\n        chunk_shape=chunk_shape,\n        dimension_names=dims,\n        attributes=attrs,\n        data=array,\n        exists_ok=True,\n    )\n\n    return array\n</pre> def create_array(*, group, name, size, dtype, fill_value) -&gt; np.ndarray:     dims = (\"x\", \"y\", \"z\", \"t\")     attrs = {\"description\": \"icechunk test data\"}      array, chunk_shape = generate_array_chunks(size=size, dtype=dtype)      group.require_array(         name=name,         shape=array.shape,         dtype=array.dtype,         fill_value=fill_value,         chunk_shape=chunk_shape,         dimension_names=dims,         attributes=attrs,         data=array,         exists_ok=True,     )      return array In\u00a0[\u00a0]: Copied! <pre>async def run(store: Store) -&gt; None:\n    write_start = time.time()\n    group = zarr.group(store=store, overwrite=True)\n    group.attrs[\"foo\"] = \"foo\"\n    print(group)\n\n    first_commit = None\n    if isinstance(store, IcechunkStore):\n        first_commit = store.commit(\"initial commit\")\n\n    expected = {}\n    expected[\"root-foo\"] = create_array(\n        group=group, name=\"root-foo\", size=1 * 1024 * 256, dtype=np.int32, fill_value=-1\n    )\n\n    print(f\"Root group members are: {group.members()}\")\n    print(f\"Root group attrs: {dict(group['root-foo'].attrs)}\")\n\n    group[\"root-foo\"].attrs[\"update\"] = \"new attr\"\n\n    if isinstance(store, IcechunkStore):\n        _second_commit = store.commit(\"added array, updated attr\")\n\n    assert len(group[\"root-foo\"].attrs) == 2\n    assert len(group.members()) == 1\n\n    if isinstance(store, IcechunkStore) and first_commit is not None:\n        store.checkout(first_commit)\n    group.attrs[\"update\"] = \"new attr 2\"\n\n    if isinstance(store, IcechunkStore):\n        try:\n            store.commit(\"new attr 2\")\n        except ValueError:\n            pass\n        else:\n            raise ValueError(\"should have conflicted\")\n\n        store.reset()\n        store.checkout(branch=\"main\")\n\n    group[\"root-foo\"].attrs[\"update\"] = \"new attr 2\"\n    if isinstance(store, IcechunkStore):\n        _third_commit = store.commit(\"new attr 2\")\n\n        try:\n            store.commit(\"rewrote array\")\n        except ValueError:\n            pass\n        else:\n            raise ValueError(\"should have failed, committing without changes.\")\n\n    newgroup = zarr.group(store=store, path=\"group1/\")\n    expected[\"group1/foo1\"] = create_array(\n        group=newgroup,\n        name=\"foo1\",\n        dtype=np.float32,\n        size=1 * 1024 * 128,\n        fill_value=-1234,\n    )\n    expected[\"group1/foo2\"] = create_array(\n        group=newgroup,\n        name=\"foo2\",\n        dtype=np.float16,\n        size=1 * 1024 * 64,\n        fill_value=-1234,\n    )\n    newgroup = zarr.group(store=store, path=\"group2/\")\n    expected[\"group2/foo3\"] = create_array(\n        group=newgroup,\n        name=\"foo3\",\n        dtype=np.int64,\n        size=1 * 1024 * 32,\n        fill_value=-1234,\n    )\n    if isinstance(store, IcechunkStore):\n        _fourth_commit = store.commit(\"added groups and arrays\")\n\n    print(f\"Write done in {time.time() - write_start} secs\")\n\n    read_start = time.time()\n\n    root_group = zarr.group(store=store)\n    for key, value in expected.items():\n        print(key)\n        array = root_group[key]\n        assert isinstance(array, zarr.Array)\n\n        print(\n            f\"numchunks: {math.prod(s // c for s, c in zip(array.shape, array.chunks, strict=False))}\"\n        )\n        np.testing.assert_array_equal(array[:], value)\n\n    print(f\"Read done in {time.time() - read_start} secs\")\n</pre> async def run(store: Store) -&gt; None:     write_start = time.time()     group = zarr.group(store=store, overwrite=True)     group.attrs[\"foo\"] = \"foo\"     print(group)      first_commit = None     if isinstance(store, IcechunkStore):         first_commit = store.commit(\"initial commit\")      expected = {}     expected[\"root-foo\"] = create_array(         group=group, name=\"root-foo\", size=1 * 1024 * 256, dtype=np.int32, fill_value=-1     )      print(f\"Root group members are: {group.members()}\")     print(f\"Root group attrs: {dict(group['root-foo'].attrs)}\")      group[\"root-foo\"].attrs[\"update\"] = \"new attr\"      if isinstance(store, IcechunkStore):         _second_commit = store.commit(\"added array, updated attr\")      assert len(group[\"root-foo\"].attrs) == 2     assert len(group.members()) == 1      if isinstance(store, IcechunkStore) and first_commit is not None:         store.checkout(first_commit)     group.attrs[\"update\"] = \"new attr 2\"      if isinstance(store, IcechunkStore):         try:             store.commit(\"new attr 2\")         except ValueError:             pass         else:             raise ValueError(\"should have conflicted\")          store.reset()         store.checkout(branch=\"main\")      group[\"root-foo\"].attrs[\"update\"] = \"new attr 2\"     if isinstance(store, IcechunkStore):         _third_commit = store.commit(\"new attr 2\")          try:             store.commit(\"rewrote array\")         except ValueError:             pass         else:             raise ValueError(\"should have failed, committing without changes.\")      newgroup = zarr.group(store=store, path=\"group1/\")     expected[\"group1/foo1\"] = create_array(         group=newgroup,         name=\"foo1\",         dtype=np.float32,         size=1 * 1024 * 128,         fill_value=-1234,     )     expected[\"group1/foo2\"] = create_array(         group=newgroup,         name=\"foo2\",         dtype=np.float16,         size=1 * 1024 * 64,         fill_value=-1234,     )     newgroup = zarr.group(store=store, path=\"group2/\")     expected[\"group2/foo3\"] = create_array(         group=newgroup,         name=\"foo3\",         dtype=np.int64,         size=1 * 1024 * 32,         fill_value=-1234,     )     if isinstance(store, IcechunkStore):         _fourth_commit = store.commit(\"added groups and arrays\")      print(f\"Write done in {time.time() - write_start} secs\")      read_start = time.time()      root_group = zarr.group(store=store)     for key, value in expected.items():         print(key)         array = root_group[key]         assert isinstance(array, zarr.Array)          print(             f\"numchunks: {math.prod(s // c for s, c in zip(array.shape, array.chunks, strict=False))}\"         )         np.testing.assert_array_equal(array[:], value)      print(f\"Read done in {time.time() - read_start} secs\") In\u00a0[\u00a0]: Copied! <pre>def create_icechunk_store(*, storage: StorageConfig) -&gt; IcechunkStore:\n    return IcechunkStore.open_or_create(\n        storage=storage, mode=\"w\", config=StoreConfig(inline_chunk_threshold_bytes=1)\n    )\n</pre> def create_icechunk_store(*, storage: StorageConfig) -&gt; IcechunkStore:     return IcechunkStore.open_or_create(         storage=storage, mode=\"w\", config=StoreConfig(inline_chunk_threshold_bytes=1)     ) In\u00a0[\u00a0]: Copied! <pre>async def create_zarr_store(*, store: Literal[\"memory\", \"local\", \"s3\"]) -&gt; Store:\n    if store == \"local\":\n        return await LocalStore.open(f\"/tmp/{rdms(6)}\", mode=\"w\")\n    if store == \"memory\":\n        return await MemoryStore.open(mode=\"w\")\n    if store == \"s3\":\n        return RemoteStore.from_url(\n            \"s3://testbucket/root-zarr\",\n            mode=\"w\",\n            storage_options={\n                \"anon\": False,\n                \"key\": \"minio123\",\n                \"secret\": \"minio123\",\n                \"endpoint_url\": \"http://localhost:9000\",\n            },\n        )\n</pre> async def create_zarr_store(*, store: Literal[\"memory\", \"local\", \"s3\"]) -&gt; Store:     if store == \"local\":         return await LocalStore.open(f\"/tmp/{rdms(6)}\", mode=\"w\")     if store == \"memory\":         return await MemoryStore.open(mode=\"w\")     if store == \"s3\":         return RemoteStore.from_url(             \"s3://testbucket/root-zarr\",             mode=\"w\",             storage_options={                 \"anon\": False,                 \"key\": \"minio123\",                 \"secret\": \"minio123\",                 \"endpoint_url\": \"http://localhost:9000\",             },         ) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    MEMORY = StorageConfig.memory(\"new\")\n    MINIO = StorageConfig.s3_from_config(\n        bucket=\"testbucket\",\n        prefix=\"root-icechunk\",\n        credentials=S3Credentials(\n            access_key_id=\"minio123\",\n            secret_access_key=\"minio123\",\n            session_token=None,\n        ),\n        region=\"us-east-1\",\n        allow_http=True,\n        endpoint_url=\"http://localhost:9000\",\n    )\n\n    print(\"Icechunk store\")\n    store = create_icechunk_store(storage=MINIO)\n    asyncio.run(run(store))\n\n    print(\"Zarr store\")\n    zarr_store = asyncio.run(create_zarr_store(store=\"s3\"))\n    asyncio.run(run(zarr_store))\n</pre> if __name__ == \"__main__\":     MEMORY = StorageConfig.memory(\"new\")     MINIO = StorageConfig.s3_from_config(         bucket=\"testbucket\",         prefix=\"root-icechunk\",         credentials=S3Credentials(             access_key_id=\"minio123\",             secret_access_key=\"minio123\",             session_token=None,         ),         region=\"us-east-1\",         allow_http=True,         endpoint_url=\"http://localhost:9000\",     )      print(\"Icechunk store\")     store = create_icechunk_store(storage=MINIO)     asyncio.run(run(store))      print(\"Zarr store\")     zarr_store = asyncio.run(create_zarr_store(store=\"s3\"))     asyncio.run(run(zarr_store))"},{"location":"icechunk-python/notebooks/demo-dummy-data/","title":"Icechunk with dummy data","text":"In\u00a0[1]: Copied! <pre>import math\n\nimport numpy as np\nimport zarr\nfrom icechunk import IcechunkStore, StorageConfig\n</pre> import math  import numpy as np import zarr from icechunk import IcechunkStore, StorageConfig In\u00a0[2]: Copied! <pre>store = IcechunkStore.create(\n    storage=StorageConfig.memory(\"icechunk-demo\"),\n)\nstore\n</pre> store = IcechunkStore.create(     storage=StorageConfig.memory(\"icechunk-demo\"), ) store Out[2]: <pre>&lt;icechunk.IcechunkStore at 0x107333510&gt;</pre> <p>This dictionary will contain array paths and data that were written to Icechunk, so that we can check correctness.</p> In\u00a0[3]: Copied! <pre>expected = {}\n</pre> expected = {} <p>These two utility functions generate and write dummy array data to a group.</p> In\u00a0[4]: Copied! <pre>def generate_array_chunks(size: int, dtype=np.int32):\n    # dim sizes\n    nz = 64\n    nt = 128\n    nx = ny = int(math.sqrt(size / nz / nt))\n\n    # chunk sizes\n    ct = 2\n    cz = 8\n    cx = max(nx // 3, 1)\n    cy = max(ny // 2, 1)\n    chunk_shape = (cx, cy, cz, ct)\n    shape = (nx, ny, nz, nt)\n\n    array = np.arange(nx * ny * nz * nt, dtype=dtype).reshape(shape)\n\n    return array, chunk_shape\n\n\ndef create_array(*, group, name, size, dtype, fill_value) -&gt; np.ndarray:\n    dims = (\"x\", \"y\", \"z\", \"t\")\n    attrs = {\"description\": \"icechunk test data\"}\n\n    array, chunk_shape = generate_array_chunks(size=size, dtype=dtype)\n\n    group.create_array(\n        name=name,\n        shape=array.shape,\n        dtype=dtype,\n        fill_value=fill_value,\n        chunk_shape=chunk_shape,\n        dimension_names=dims,\n        attributes=attrs,\n        data=array,\n        exists_ok=True,\n    )\n\n    return array\n</pre> def generate_array_chunks(size: int, dtype=np.int32):     # dim sizes     nz = 64     nt = 128     nx = ny = int(math.sqrt(size / nz / nt))      # chunk sizes     ct = 2     cz = 8     cx = max(nx // 3, 1)     cy = max(ny // 2, 1)     chunk_shape = (cx, cy, cz, ct)     shape = (nx, ny, nz, nt)      array = np.arange(nx * ny * nz * nt, dtype=dtype).reshape(shape)      return array, chunk_shape   def create_array(*, group, name, size, dtype, fill_value) -&gt; np.ndarray:     dims = (\"x\", \"y\", \"z\", \"t\")     attrs = {\"description\": \"icechunk test data\"}      array, chunk_shape = generate_array_chunks(size=size, dtype=dtype)      group.create_array(         name=name,         shape=array.shape,         dtype=dtype,         fill_value=fill_value,         chunk_shape=chunk_shape,         dimension_names=dims,         attributes=attrs,         data=array,         exists_ok=True,     )      return array In\u00a0[5]: Copied! <pre>root_group = zarr.group(store=store, overwrite=True)\nroot_group.attrs[\"foo\"] = \"foo\"\ndict(root_group.attrs)  # check that it was written\n</pre> root_group = zarr.group(store=store, overwrite=True) root_group.attrs[\"foo\"] = \"foo\" dict(root_group.attrs)  # check that it was written Out[5]: <pre>{'foo': 'foo'}</pre> <p>Commit that change</p> In\u00a0[6]: Copied! <pre>first_commit = store.commit(\"wrote a root group attribute\")\nfirst_commit\n</pre> first_commit = store.commit(\"wrote a root group attribute\") first_commit Out[6]: <pre>'M419JDES7SDXBA6NCT4G'</pre> In\u00a0[7]: Copied! <pre>expected[\"root-foo\"] = create_array(\n    group=root_group,\n    name=\"root-foo\",\n    size=1 * 1024 * 256,\n    dtype=np.int32,\n    fill_value=-1,\n)\n</pre> expected[\"root-foo\"] = create_array(     group=root_group,     name=\"root-foo\",     size=1 * 1024 * 256,     dtype=np.int32,     fill_value=-1, ) In\u00a0[8]: Copied! <pre>print(root_group.members())\n</pre> print(root_group.members()) <pre>(('root-foo', &lt;Array &lt;icechunk.IcechunkStore object at 0x107333510&gt;/root-foo shape=(5, 5, 64, 128) dtype=int32&gt;),)\n</pre> In\u00a0[10]: Copied! <pre>dict(root_group[\"root-foo\"].attrs)\n</pre> dict(root_group[\"root-foo\"].attrs) <pre>{\n  \"shape\": [\n    5,\n    5,\n    64,\n    128\n  ],\n  \"data_type\": \"int32\",\n  \"chunk_grid\": {\n    \"name\": \"regular\",\n    \"configuration\": {\n      \"chunk_shape\": [\n        1,\n        2,\n        8,\n        2\n      ]\n    }\n  },\n  \"chunk_key_encoding\": {\n    \"name\": \"default\",\n    \"configuration\": {\n      \"separator\": \"/\"\n    }\n  },\n  \"fill_value\": -1,\n  \"codecs\": [\n    {\n      \"name\": \"bytes\",\n      \"configuration\": {\n        \"endian\": \"little\"\n      }\n    }\n  ],\n  \"dimension_names\": [\n    \"x\",\n    \"y\",\n    \"z\",\n    \"t\"\n  ],\n  \"attributes\": {\n    \"description\": \"icechunk test data\"\n  }\n}\n</pre> Out[10]: <pre>{'description': 'icechunk test data'}</pre> In\u00a0[9]: Copied! <pre>root_group[\"root-foo\"].attrs[\"update\"] = \"new attr\"\n</pre> root_group[\"root-foo\"].attrs[\"update\"] = \"new attr\" In\u00a0[10]: Copied! <pre>second_commit = store.commit(\"added array, updated attr\")\nsecond_commit\n</pre> second_commit = store.commit(\"added array, updated attr\") second_commit Out[10]: <pre>'V3SFRWRM255Z3JC3SYH0'</pre> In\u00a0[11]: Copied! <pre>assert len(root_group[\"root-foo\"].attrs) == 2\nassert len(root_group.members()) == 1\n</pre> assert len(root_group[\"root-foo\"].attrs) == 2 assert len(root_group.members()) == 1 In\u00a0[15]: Copied! <pre>store.checkout(first_commit)\nroot_group.attrs[\"update\"] = \"new attr 2\"\n\ntry:\n    store.commit(\"new attr 2\")\nexcept ValueError as e:\n    print(e)\nelse:\n    raise ValueError(\"should have failed\")\n</pre> store.checkout(first_commit) root_group.attrs[\"update\"] = \"new attr 2\"  try:     store.commit(\"new attr 2\") except ValueError as e:     print(e) else:     raise ValueError(\"should have failed\") <pre>store error: all commits must be made on a branch\n</pre> In\u00a0[16]: Copied! <pre>store.reset()\nstore.checkout(branch=\"main\")\nroot_group[\"root-foo\"].attrs[\"update\"] = \"new attr 2\"\nthird_commit = store.commit(\"new attr 2\")\nthird_commit\n</pre> store.reset() store.checkout(branch=\"main\") root_group[\"root-foo\"].attrs[\"update\"] = \"new attr 2\" third_commit = store.commit(\"new attr 2\") third_commit Out[16]: <pre>'5QGW2PE1A5MTRZED190G'</pre> In\u00a0[17]: Copied! <pre>root_group.attrs[\"update\"] = \"new attr 2\"\nfourth_commit = store.commit(\"rewrote array\")\nfourth_commit\n</pre> root_group.attrs[\"update\"] = \"new attr 2\" fourth_commit = store.commit(\"rewrote array\") fourth_commit Out[17]: <pre>'ARWA72NB2MAH90JJ285G'</pre> In\u00a0[18]: Copied! <pre>{k: v.dtype for k, v in expected.items()}\n</pre> {k: v.dtype for k, v in expected.items()} Out[18]: <pre>{'root-foo': dtype('int32')}</pre> In\u00a0[19]: Copied! <pre>newgroup = zarr.group(store=store, path=\"group1/\")\nexpected[\"group1/foo1\"] = create_array(\n    group=newgroup, name=\"foo1\", dtype=np.float32, size=1 * 1024 * 128, fill_value=-1234\n)\nexpected[\"group1/foo2\"] = create_array(\n    group=newgroup, name=\"foo2\", dtype=np.float16, size=1 * 1024 * 64, fill_value=-1234\n)\nnewgroup = zarr.group(store=store, path=\"group2/\")\nexpected[\"group2/foo3\"] = create_array(\n    group=newgroup, name=\"foo3\", dtype=np.int64, size=1 * 1024 * 32, fill_value=-1234\n)\nfifth_commit = store.commit(\"added groups and arrays\")\nfifth_commit\n</pre> newgroup = zarr.group(store=store, path=\"group1/\") expected[\"group1/foo1\"] = create_array(     group=newgroup, name=\"foo1\", dtype=np.float32, size=1 * 1024 * 128, fill_value=-1234 ) expected[\"group1/foo2\"] = create_array(     group=newgroup, name=\"foo2\", dtype=np.float16, size=1 * 1024 * 64, fill_value=-1234 ) newgroup = zarr.group(store=store, path=\"group2/\") expected[\"group2/foo3\"] = create_array(     group=newgroup, name=\"foo3\", dtype=np.int64, size=1 * 1024 * 32, fill_value=-1234 ) fifth_commit = store.commit(\"added groups and arrays\") fifth_commit Out[19]: <pre>'G1DMNFF0W1RCEEPY09B0'</pre> In\u00a0[20]: Copied! <pre>expected[\"root-foo\"] = create_array(\n    group=root_group,\n    name=\"root-foo\",\n    size=1 * 1024 * 128,\n    dtype=np.int32,\n    fill_value=-1,\n)\n</pre> expected[\"root-foo\"] = create_array(     group=root_group,     name=\"root-foo\",     size=1 * 1024 * 128,     dtype=np.int32,     fill_value=-1, ) In\u00a0[21]: Copied! <pre>store.commit(\"overwrote root-foo\")\n</pre> store.commit(\"overwrote root-foo\") Out[21]: <pre>'RVZSK0518F73E6RSY990'</pre> In\u00a0[22]: Copied! <pre>root_group.members()\n</pre> root_group.members() Out[22]: <pre>(('group2', &lt;Group &lt;icechunk.IcechunkStore object at 0x107333510&gt;/group2&gt;),\n ('group1', &lt;Group &lt;icechunk.IcechunkStore object at 0x107333510&gt;/group1&gt;),\n ('root-foo',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x107333510&gt;/root-foo shape=(4, 4, 64, 128) dtype=int32&gt;))</pre> In\u00a0[27]: Copied! <pre>root_group[\"group1\"].members()\n</pre> root_group[\"group1\"].members() <pre>store_path group1\n{\n  \"shape\": [\n    4,\n    4,\n    64,\n    128\n  ],\n  \"data_type\": \"float32\",\n  \"chunk_grid\": {\n    \"name\": \"regular\",\n    \"configuration\": {\n      \"chunk_shape\": [\n        1,\n        2,\n        8,\n        2\n      ]\n    }\n  },\n  \"chunk_key_encoding\": {\n    \"name\": \"default\",\n    \"configuration\": {\n      \"separator\": \"/\"\n    }\n  },\n  \"fill_value\": -1234.0,\n  \"codecs\": [\n    {\n      \"name\": \"bytes\",\n      \"configuration\": {\n        \"endian\": \"little\"\n      }\n    }\n  ],\n  \"dimension_names\": [\n    \"x\",\n    \"y\",\n    \"z\",\n    \"t\"\n  ],\n  \"attributes\": {\n    \"description\": \"icechunk test data\"\n  }\n}\n{\n  \"shape\": [\n    2,\n    2,\n    64,\n    128\n  ],\n  \"data_type\": \"float16\",\n  \"chunk_grid\": {\n    \"name\": \"regular\",\n    \"configuration\": {\n      \"chunk_shape\": [\n        1,\n        1,\n        8,\n        2\n      ]\n    }\n  },\n  \"chunk_key_encoding\": {\n    \"name\": \"default\",\n    \"configuration\": {\n      \"separator\": \"/\"\n    }\n  },\n  \"fill_value\": -1234.0,\n  \"codecs\": [\n    {\n      \"name\": \"bytes\",\n      \"configuration\": {\n        \"endian\": \"little\"\n      }\n    }\n  ],\n  \"dimension_names\": [\n    \"x\",\n    \"y\",\n    \"z\",\n    \"t\"\n  ],\n  \"attributes\": {\n    \"description\": \"icechunk test data\"\n  }\n}\n</pre> Out[27]: <pre>(('foo1',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x10c4c34d0&gt;/group1/foo1 shape=(4, 4, 64, 128) dtype=float32&gt;),\n ('foo2',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x10c4c34d0&gt;/group1/foo2 shape=(2, 2, 64, 128) dtype=float16&gt;))</pre> In\u00a0[23]: Copied! <pre>root_group[\"group2\"].members()\n</pre> root_group[\"group2\"].members() Out[23]: <pre>(('foo3',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x107333510&gt;/group2/foo3 shape=(2, 2, 64, 128) dtype=int64&gt;),)</pre> In\u00a0[24]: Copied! <pre>array = root_group[\"group2/foo3\"]\nprint(array)\n\narray = array.resize((array.shape[0] * 2, *array.shape[1:]))\nprint(array)\narray[array.shape[0] // 2 :, ...] = expected[\"group2/foo3\"]\nprint(array[2:, 0, 0, 0])\nexpected[\"group2/foo3\"] = np.concatenate([expected[\"group2/foo3\"]] * 2, axis=0)\n\nstore.commit(\"appended to group2/foo3\")\n</pre> array = root_group[\"group2/foo3\"] print(array)  array = array.resize((array.shape[0] * 2, *array.shape[1:])) print(array) array[array.shape[0] // 2 :, ...] = expected[\"group2/foo3\"] print(array[2:, 0, 0, 0]) expected[\"group2/foo3\"] = np.concatenate([expected[\"group2/foo3\"]] * 2, axis=0)  store.commit(\"appended to group2/foo3\") <pre>&lt;Array &lt;icechunk.IcechunkStore object at 0x107333510&gt;/group2/foo3 shape=(2, 2, 64, 128) dtype=int64&gt;\n&lt;Array &lt;icechunk.IcechunkStore object at 0x107333510&gt;/group2/foo3 shape=(4, 2, 64, 128) dtype=int64&gt;\n[    0 16384]\n</pre> Out[24]: <pre>'JHCPX1W73WZV399MYQZ0'</pre> In\u00a0[26]: Copied! <pre># import time\n\n# for key, value in expected.items():\n#     print(key)\n#     tic = time.time()\n#     array = root_group[key]\n#     assert array.dtype == value.dtype, (array.dtype, value.dtype)\n#     print(f\"numchunks: {math.prod(s // c for s, c in zip(array.shape, array.chunks, strict=False))}\")\n#     np.testing.assert_array_equal(array[:], value)\n#     print(time.time() - tic)\n</pre> # import time  # for key, value in expected.items(): #     print(key) #     tic = time.time() #     array = root_group[key] #     assert array.dtype == value.dtype, (array.dtype, value.dtype) #     print(f\"numchunks: {math.prod(s // c for s, c in zip(array.shape, array.chunks, strict=False))}\") #     np.testing.assert_array_equal(array[:], value) #     print(time.time() - tic)"},{"location":"icechunk-python/notebooks/demo-dummy-data/#icechunk-with-dummy-data","title":"Icechunk with dummy data\u00b6","text":"<p>This demo illustrates how to use Icechunk as a Zarr store</p>"},{"location":"icechunk-python/notebooks/demo-dummy-data/#create-a-new-zarr-store-backed-by-icechunk","title":"Create a new Zarr store backed by Icechunk\u00b6","text":"<p>This example uses an in-memory store.</p>"},{"location":"icechunk-python/notebooks/demo-dummy-data/#a-versioned-transactional-zarr-store","title":"A versioned transactional Zarr store\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#open-the-root-group-write-an-attribute-commit","title":"Open the root group, write an attribute, commit\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#add-a-array-to-the-root-group","title":"Add a array to the root group\u00b6","text":"<p>We save the created array in <code>expected</code> to check that the write was correct (later).</p>"},{"location":"icechunk-python/notebooks/demo-dummy-data/#commiting-when-not-on-head-will-fail","title":"Commiting when not on <code>HEAD</code> will fail.\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#checkout-head-make-a-change-and-commit","title":"Checkout <code>HEAD</code>, make a change, and commit.\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#create-a-hierarchy","title":"Create a hierarchy\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#overwrite-an-array","title":"Overwrite an array\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#examine-the-hierarchy","title":"Examine the hierarchy\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#append","title":"Append\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-dummy-data/#check-that-values-are-correct","title":"Check that values are correct\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-s3/","title":"Xarray/Zarr/Icechunk on S3","text":"In\u00a0[1]: Copied! <pre>import zarr\nfrom icechunk import IcechunkStore, StorageConfig\n</pre> import zarr from icechunk import IcechunkStore, StorageConfig In\u00a0[2]: Copied! <pre>s3_storage = StorageConfig.s3_from_env(\n    bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\"\n)\n</pre> s3_storage = StorageConfig.s3_from_env(     bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\" ) In\u00a0[3]: Copied! <pre>store = IcechunkStore.create(\n    storage=s3_storage,\n    mode=\"w\",\n)\n</pre> store = IcechunkStore.create(     storage=s3_storage,     mode=\"w\", ) In\u00a0[4]: Copied! <pre>import xarray as xr\n</pre> import xarray as xr In\u00a0[5]: Copied! <pre>import fsspec\n\nfs = fsspec.filesystem(\"s3\")\n</pre> import fsspec  fs = fsspec.filesystem(\"s3\") In\u00a0[7]: Copied! <pre>oscar = xr.open_dataset(\n    fs.open(\"s3://earthmover-sample-data/netcdf/oscar_vel2018.nc\"),\n    chunks={},\n    engine=\"h5netcdf\",\n)\noscar\n</pre> oscar = xr.open_dataset(     fs.open(\"s3://earthmover-sample-data/netcdf/oscar_vel2018.nc\"),     chunks={},     engine=\"h5netcdf\", ) oscar Out[7]: <pre>&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:    (depth: 1, latitude: 481, longitude: 1201, time: 72, year: 72)\nCoordinates:\n  * depth      (depth) float32 4B 15.0\n  * latitude   (latitude) float64 4kB 80.0 79.67 79.33 ... -79.33 -79.67 -80.0\n  * longitude  (longitude) float64 10kB 20.0 20.33 20.67 ... 419.3 419.7 420.0\n  * time       (time) datetime64[ns] 576B 2018-01-01 2018-01-06 ... 2018-12-26\n  * year       (year) float32 288B 2.018e+03 2.018e+03 ... 2.019e+03 2.019e+03\nData variables:\n    u          (time, depth, latitude, longitude) float64 333MB dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;\n    um         (time, depth, latitude, longitude) float64 333MB dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;\n    v          (time, depth, latitude, longitude) float64 333MB dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;\n    vm         (time, depth, latitude, longitude) float64 333MB dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;\nAttributes: (12/17)\n    VARIABLE:       Ocean Surface Currents\n    DATATYPE:       1/72 YEAR Interval\n    DATASUBTYPE:    unfiltered\n    GEORANGE:       20 to 420 -80 to 80\n    PERIOD:         Jan.01,2018 to Dec.26,2018\n    year:           2018\n    ...             ...\n    company:        Earth &amp; Space Research, Seattle, WA\n    reference:      Bonjean F. and G.S.E. Lagerloef, 2002 ,Diagnostic model a...\n    note1:          Maximum Mask velocity is the geostrophic component at all...\n    note2:          Longitude extends from 20 E to 420 E to avoid a break in ...\n    history:        Wed Sep 18 14:18:38 2024: ncks -4 -o oscar_vel2018.nc4 os...\n    NCO:            netCDF Operators version 5.2.8 (Homepage = http://nco.sf....</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>depth: 1</li><li>latitude: 481</li><li>longitude: 1201</li><li>time: 72</li><li>year: 72</li></ul></li><li>Coordinates: (5)<ul><li>depth(depth)float3215.0units :meterlong_name :Depth<pre>array([15.], dtype=float32)</pre></li><li>latitude(latitude)float6480.0 79.67 79.33 ... -79.67 -80.0units :degrees-northlong_name :Latitude<pre>array([ 80.      ,  79.666667,  79.333333, ..., -79.333333, -79.666667,\n       -80.      ])</pre></li><li>longitude(longitude)float6420.0 20.33 20.67 ... 419.7 420.0units :degrees-eastlong_name :Longitude<pre>array([ 20.      ,  20.333333,  20.666667, ..., 419.333333, 419.666667,\n       420.      ])</pre></li><li>time(time)datetime64[ns]2018-01-01 ... 2018-12-26long_name :Day since 1992-10-05 00:00:00<pre>array(['2018-01-01T00:00:00.000000000', '2018-01-06T00:00:00.000000000',\n       '2018-01-11T00:00:00.000000000', '2018-01-16T00:00:00.000000000',\n       '2018-01-21T00:00:00.000000000', '2018-01-26T00:00:00.000000000',\n       '2018-01-31T00:00:00.000000000', '2018-02-05T00:00:00.000000000',\n       '2018-02-10T00:00:00.000000000', '2018-02-15T00:00:00.000000000',\n       '2018-02-20T00:00:00.000000000', '2018-02-25T00:00:00.000000000',\n       '2018-03-02T00:00:00.000000000', '2018-03-07T00:00:00.000000000',\n       '2018-03-12T00:00:00.000000000', '2018-03-18T00:00:00.000000000',\n       '2018-03-23T00:00:00.000000000', '2018-03-28T00:00:00.000000000',\n       '2018-04-02T00:00:00.000000000', '2018-04-07T00:00:00.000000000',\n       '2018-04-12T00:00:00.000000000', '2018-04-17T00:00:00.000000000',\n       '2018-04-22T00:00:00.000000000', '2018-04-27T00:00:00.000000000',\n       '2018-05-02T00:00:00.000000000', '2018-05-07T00:00:00.000000000',\n       '2018-05-12T00:00:00.000000000', '2018-05-17T00:00:00.000000000',\n       '2018-05-22T00:00:00.000000000', '2018-05-28T00:00:00.000000000',\n       '2018-06-02T00:00:00.000000000', '2018-06-07T00:00:00.000000000',\n       '2018-06-12T00:00:00.000000000', '2018-06-17T00:00:00.000000000',\n       '2018-06-22T00:00:00.000000000', '2018-06-27T00:00:00.000000000',\n       '2018-07-02T00:00:00.000000000', '2018-07-07T00:00:00.000000000',\n       '2018-07-12T00:00:00.000000000', '2018-07-17T00:00:00.000000000',\n       '2018-07-22T00:00:00.000000000', '2018-07-27T00:00:00.000000000',\n       '2018-08-01T00:00:00.000000000', '2018-08-06T00:00:00.000000000',\n       '2018-08-12T00:00:00.000000000', '2018-08-17T00:00:00.000000000',\n       '2018-08-22T00:00:00.000000000', '2018-08-27T00:00:00.000000000',\n       '2018-09-01T00:00:00.000000000', '2018-09-06T00:00:00.000000000',\n       '2018-09-11T00:00:00.000000000', '2018-09-16T00:00:00.000000000',\n       '2018-09-21T00:00:00.000000000', '2018-09-26T00:00:00.000000000',\n       '2018-10-01T00:00:00.000000000', '2018-10-06T00:00:00.000000000',\n       '2018-10-11T00:00:00.000000000', '2018-10-16T00:00:00.000000000',\n       '2018-10-22T00:00:00.000000000', '2018-10-27T00:00:00.000000000',\n       '2018-11-01T00:00:00.000000000', '2018-11-06T00:00:00.000000000',\n       '2018-11-11T00:00:00.000000000', '2018-11-16T00:00:00.000000000',\n       '2018-11-21T00:00:00.000000000', '2018-11-26T00:00:00.000000000',\n       '2018-12-01T00:00:00.000000000', '2018-12-06T00:00:00.000000000',\n       '2018-12-11T00:00:00.000000000', '2018-12-16T00:00:00.000000000',\n       '2018-12-21T00:00:00.000000000', '2018-12-26T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>year(year)float322.018e+03 2.018e+03 ... 2.019e+03units :time in yearslong_name :Time in fractional year<pre>array([2018.    , 2018.0139, 2018.0278, 2018.0416, 2018.0555, 2018.0695,\n       2018.0834, 2018.0972, 2018.1111, 2018.125 , 2018.1389, 2018.1528,\n       2018.1666, 2018.1805, 2018.1945, 2018.2084, 2018.2222, 2018.2361,\n       2018.25  , 2018.2639, 2018.2778, 2018.2916, 2018.3055, 2018.3195,\n       2018.3334, 2018.3472, 2018.3611, 2018.375 , 2018.3889, 2018.4028,\n       2018.4166, 2018.4305, 2018.4445, 2018.4584, 2018.4722, 2018.4861,\n       2018.5   , 2018.5139, 2018.5278, 2018.5416, 2018.5555, 2018.5695,\n       2018.5834, 2018.5972, 2018.6111, 2018.625 , 2018.6389, 2018.6528,\n       2018.6666, 2018.6805, 2018.6945, 2018.7084, 2018.7222, 2018.7361,\n       2018.75  , 2018.7639, 2018.7778, 2018.7916, 2018.8055, 2018.8195,\n       2018.8334, 2018.8472, 2018.8611, 2018.875 , 2018.8889, 2018.9028,\n       2018.9166, 2018.9305, 2018.9445, 2018.9584, 2018.9722, 2018.9861],\n      dtype=float32)</pre></li></ul></li><li>Data variables: (4)<ul><li>u(time, depth, latitude, longitude)float64dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;units :meter/seclong_name :Ocean Surface Zonal Currents  Array   Chunk   Bytes   317.33 MiB   317.33 MiB   Shape   (72, 1, 481, 1201)   (72, 1, 481, 1201)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  72 1 1201 481 1 </li><li>um(time, depth, latitude, longitude)float64dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;units :meter/seclong_name :Ocean Surface Zonal Currents Maximum Mask  Array   Chunk   Bytes   317.33 MiB   317.33 MiB   Shape   (72, 1, 481, 1201)   (72, 1, 481, 1201)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  72 1 1201 481 1 </li><li>v(time, depth, latitude, longitude)float64dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;units :meter/seclong_name :Ocean Surface Meridional Currents  Array   Chunk   Bytes   317.33 MiB   317.33 MiB   Shape   (72, 1, 481, 1201)   (72, 1, 481, 1201)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  72 1 1201 481 1 </li><li>vm(time, depth, latitude, longitude)float64dask.array&lt;chunksize=(72, 1, 481, 1201), meta=np.ndarray&gt;units :meter/seclong_name :Ocean Surface Meridional Currents Maximum Mask  Array   Chunk   Bytes   317.33 MiB   317.33 MiB   Shape   (72, 1, 481, 1201)   (72, 1, 481, 1201)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  72 1 1201 481 1 </li></ul></li><li>Indexes: (5)<ul><li>depthPandasIndex<pre>PandasIndex(Index([15.0], dtype='float32', name='depth'))</pre></li><li>latitudePandasIndex<pre>PandasIndex(Index([              80.0,  79.66666666666667,  79.33333333333333,\n                     79.0,  78.66666666666667,  78.33333333333333,\n                     78.0,  77.66666666666667,  77.33333333333333,\n                     77.0,\n       ...\n                    -77.0, -77.33333333333333, -77.66666666666667,\n                    -78.0, -78.33333333333333, -78.66666666666667,\n                    -79.0, -79.33333333333333, -79.66666666666667,\n                    -80.0],\n      dtype='float64', name='latitude', length=481))</pre></li><li>longitudePandasIndex<pre>PandasIndex(Index([              20.0, 20.333333333333332, 20.666666666666664,\n                     21.0, 21.333333333333332, 21.666666666666664,\n                     22.0, 22.333333333333332, 22.666666666666664,\n                     23.0,\n       ...\n                    417.0,  417.3333333333333,  417.6666666666667,\n                    418.0,  418.3333333333333,  418.6666666666667,\n                    419.0,  419.3333333333333,  419.6666666666667,\n                    420.0],\n      dtype='float64', name='longitude', length=1201))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2018-01-01', '2018-01-06', '2018-01-11', '2018-01-16',\n               '2018-01-21', '2018-01-26', '2018-01-31', '2018-02-05',\n               '2018-02-10', '2018-02-15', '2018-02-20', '2018-02-25',\n               '2018-03-02', '2018-03-07', '2018-03-12', '2018-03-18',\n               '2018-03-23', '2018-03-28', '2018-04-02', '2018-04-07',\n               '2018-04-12', '2018-04-17', '2018-04-22', '2018-04-27',\n               '2018-05-02', '2018-05-07', '2018-05-12', '2018-05-17',\n               '2018-05-22', '2018-05-28', '2018-06-02', '2018-06-07',\n               '2018-06-12', '2018-06-17', '2018-06-22', '2018-06-27',\n               '2018-07-02', '2018-07-07', '2018-07-12', '2018-07-17',\n               '2018-07-22', '2018-07-27', '2018-08-01', '2018-08-06',\n               '2018-08-12', '2018-08-17', '2018-08-22', '2018-08-27',\n               '2018-09-01', '2018-09-06', '2018-09-11', '2018-09-16',\n               '2018-09-21', '2018-09-26', '2018-10-01', '2018-10-06',\n               '2018-10-11', '2018-10-16', '2018-10-22', '2018-10-27',\n               '2018-11-01', '2018-11-06', '2018-11-11', '2018-11-16',\n               '2018-11-21', '2018-11-26', '2018-12-01', '2018-12-06',\n               '2018-12-11', '2018-12-16', '2018-12-21', '2018-12-26'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>yearPandasIndex<pre>PandasIndex(Index([            2018.0,  2018.013916015625,   2018.02783203125,\n       2018.0416259765625, 2018.0555419921875, 2018.0694580078125,\n       2018.0833740234375,   2018.09716796875,  2018.111083984375,\n                 2018.125,  2018.138916015625,   2018.15283203125,\n       2018.1666259765625, 2018.1805419921875, 2018.1944580078125,\n       2018.2083740234375,   2018.22216796875,  2018.236083984375,\n                  2018.25,  2018.263916015625,   2018.27783203125,\n       2018.2916259765625, 2018.3055419921875, 2018.3194580078125,\n       2018.3333740234375,   2018.34716796875,  2018.361083984375,\n                 2018.375,  2018.388916015625,   2018.40283203125,\n       2018.4166259765625, 2018.4305419921875, 2018.4444580078125,\n       2018.4583740234375,   2018.47216796875,  2018.486083984375,\n                   2018.5,  2018.513916015625,   2018.52783203125,\n       2018.5416259765625, 2018.5555419921875, 2018.5694580078125,\n       2018.5833740234375,   2018.59716796875,  2018.611083984375,\n                 2018.625,  2018.638916015625,   2018.65283203125,\n       2018.6666259765625, 2018.6805419921875, 2018.6944580078125,\n       2018.7083740234375,   2018.72216796875,  2018.736083984375,\n                  2018.75,  2018.763916015625,   2018.77783203125,\n       2018.7916259765625, 2018.8055419921875, 2018.8194580078125,\n       2018.8333740234375,   2018.84716796875,  2018.861083984375,\n                 2018.875,  2018.888916015625,   2018.90283203125,\n       2018.9166259765625, 2018.9305419921875, 2018.9444580078125,\n       2018.9583740234375,   2018.97216796875,  2018.986083984375],\n      dtype='float32', name='year'))</pre></li></ul></li><li>Attributes: (17)VARIABLE :Ocean Surface CurrentsDATATYPE :1/72 YEAR IntervalDATASUBTYPE :unfilteredGEORANGE :20 to 420 -80 to 80PERIOD :Jan.01,2018 to Dec.26,2018year :2018description :OSCAR Third Degree Sea Surface VelocityCREATION_DATE :03:39 30-Jan-2019version :2009.0source :Gary Lagerloef, ESR (lager@esr.org) and Kathleen Dohan, ESR (kdohan@esr.org)contact :Kathleen Dohan (kdohan@esr.org) or John T. Gunn (gunn@esr.org)company :Earth &amp; Space Research, Seattle, WAreference :Bonjean F. and G.S.E. Lagerloef, 2002 ,Diagnostic model and analysis of the surface currents in the tropical Pacific ocean, J. Phys. Oceanogr., 32, 2,938-2,954note1 :Maximum Mask velocity is the geostrophic component at all points + any concurrent Ekman and buoyancy componentsnote2 :Longitude extends from 20 E to 420 E to avoid a break in major ocean basins. Data repeats in overlap region.history :Wed Sep 18 14:18:38 2024: ncks -4 -o oscar_vel2018.nc4 oscar_vel2018.ncNCO :netCDF Operators version 5.2.8 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco, Citation = 10.1016/j.envsoft.2008.03.004)</li></ul> In\u00a0[8]: Copied! <pre>group = zarr.group(store=store, overwrite=True)\ngroup\n</pre> group = zarr.group(store=store, overwrite=True) group Out[8]: <pre>Group(_async_group=&lt;AsyncGroup &lt;icechunk.IcechunkStore object at 0x7efd807b8920&gt;&gt;)</pre> In\u00a0[9]: Copied! <pre>import time\n\nfor var in oscar:\n    print(var)\n    tic = time.time()\n    group.create_array(\n        name=var,\n        shape=oscar[var].shape,\n        chunk_shape=(1, 1, 481, 1201),\n        fill_value=-1234567,\n        dtype=oscar[var].dtype,\n        data=oscar[var],\n        exists_ok=True,\n    )\n    print(store.commit(f\"wrote {var}\"))\n    print(f\"commited; {time.time() - tic} seconds\")\n</pre> import time  for var in oscar:     print(var)     tic = time.time()     group.create_array(         name=var,         shape=oscar[var].shape,         chunk_shape=(1, 1, 481, 1201),         fill_value=-1234567,         dtype=oscar[var].dtype,         data=oscar[var],         exists_ok=True,     )     print(store.commit(f\"wrote {var}\"))     print(f\"commited; {time.time() - tic} seconds\") <pre>u\nD2YNJWWTKW6DY8ECPJZG\ncommited; 43.68043661117554 seconds\num\nV0RSK39P1EXKB37F6Z10\ncommited; 44.08490180969238 seconds\nv\nJNDCHT5MF2MWRHYY8Q1G\ncommited; 61.78669619560242 seconds\nvm\nGAKXY70VJ2NQ3ANMEE10\ncommited; 55.72252893447876 seconds\n</pre> In\u00a0[14]: Copied! <pre>store.ancestry()\n</pre> store.ancestry() Out[14]: <pre>[('GAKXY70VJ2NQ3ANMEE10',\n  'wrote vm',\n  datetime.datetime(2024, 9, 27, 1, 44, 32, 21912, tzinfo=datetime.timezone.utc)),\n ('JNDCHT5MF2MWRHYY8Q1G',\n  'wrote v',\n  datetime.datetime(2024, 9, 27, 1, 43, 36, 291827, tzinfo=datetime.timezone.utc)),\n ('V0RSK39P1EXKB37F6Z10',\n  'wrote um',\n  datetime.datetime(2024, 9, 27, 1, 42, 34, 501948, tzinfo=datetime.timezone.utc)),\n ('D2YNJWWTKW6DY8ECPJZG',\n  'wrote u',\n  datetime.datetime(2024, 9, 27, 1, 41, 50, 428929, tzinfo=datetime.timezone.utc)),\n ('7YA9K51RW5N422R8YAPG',\n  'Repository initialized',\n  datetime.datetime(2024, 9, 27, 1, 40, 21, 969915, tzinfo=datetime.timezone.utc))]</pre> In\u00a0[3]: Copied! <pre>import zarr\nfrom icechunk import IcechunkStore, StorageConfig\n\n# TODO: catalog will handle this\ns3_storage = StorageConfig.s3_from_env(\n    bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\"\n)\n</pre> import zarr from icechunk import IcechunkStore, StorageConfig  # TODO: catalog will handle this s3_storage = StorageConfig.s3_from_env(     bucket=\"icechunk-test\", prefix=\"oscar-demo-repository\" ) In\u00a0[4]: Copied! <pre>store = IcechunkStore.open_existing(\n    storage=s3_storage,\n    mode=\"r\",\n)\nstore\n</pre> store = IcechunkStore.open_existing(     storage=s3_storage,     mode=\"r\", ) store Out[4]: <pre>&lt;icechunk.IcechunkStore at 0x7fc0580983e0&gt;</pre> <p>Look at history</p> In\u00a0[5]: Copied! <pre>store.ancestry()\n</pre> store.ancestry() Out[5]: <pre>[('GAKXY70VJ2NQ3ANMEE10',\n  'wrote vm',\n  datetime.datetime(2024, 9, 27, 1, 44, 32, 21912, tzinfo=datetime.timezone.utc)),\n ('JNDCHT5MF2MWRHYY8Q1G',\n  'wrote v',\n  datetime.datetime(2024, 9, 27, 1, 43, 36, 291827, tzinfo=datetime.timezone.utc)),\n ('V0RSK39P1EXKB37F6Z10',\n  'wrote um',\n  datetime.datetime(2024, 9, 27, 1, 42, 34, 501948, tzinfo=datetime.timezone.utc)),\n ('D2YNJWWTKW6DY8ECPJZG',\n  'wrote u',\n  datetime.datetime(2024, 9, 27, 1, 41, 50, 428929, tzinfo=datetime.timezone.utc)),\n ('7YA9K51RW5N422R8YAPG',\n  'Repository initialized',\n  datetime.datetime(2024, 9, 27, 1, 40, 21, 969915, tzinfo=datetime.timezone.utc))]</pre> In\u00a0[7]: Copied! <pre>root_group = zarr.open_group(store=store)\n</pre> root_group = zarr.open_group(store=store) In\u00a0[8]: Copied! <pre>root_group.members()\n</pre> root_group.members() Out[8]: <pre>(('um',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/um shape=(72, 1, 481, 1201) dtype=float64&gt;),\n ('u',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/u shape=(72, 1, 481, 1201) dtype=float64&gt;),\n ('v',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/v shape=(72, 1, 481, 1201) dtype=float64&gt;),\n ('vm',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/vm shape=(72, 1, 481, 1201) dtype=float64&gt;))</pre> In\u00a0[9]: Copied! <pre>root_group.members()\n</pre> root_group.members() Out[9]: <pre>(('v',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/v shape=(72, 1, 481, 1201) dtype=float64&gt;),\n ('vm',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/vm shape=(72, 1, 481, 1201) dtype=float64&gt;),\n ('u',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/u shape=(72, 1, 481, 1201) dtype=float64&gt;),\n ('um',\n  &lt;Array &lt;icechunk.IcechunkStore object at 0x7fc0580983e0&gt;/um shape=(72, 1, 481, 1201) dtype=float64&gt;))</pre> In\u00a0[16]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt In\u00a0[36]: Copied! <pre>plt.imshow(root_group[\"u\"][20, 0, :, :], cmap=mpl.cm.RdBu_r, vmin=-0.5, vmax=0.5)\nplt.gcf().set_size_inches((9, 5))\nplt.colorbar(location=\"bottom\", orientation=\"horizontal\", shrink=0.5, aspect=30)\n</pre> plt.imshow(root_group[\"u\"][20, 0, :, :], cmap=mpl.cm.RdBu_r, vmin=-0.5, vmax=0.5) plt.gcf().set_size_inches((9, 5)) plt.colorbar(location=\"bottom\", orientation=\"horizontal\", shrink=0.5, aspect=30) Out[36]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7fbf1db37e30&gt;</pre>"},{"location":"icechunk-python/notebooks/demo-s3/#xarrayzarricechunk-on-s3","title":"Xarray/Zarr/Icechunk on S3\u00b6","text":"<p>You will need to run this notebook in a <code>conda</code> environment created from <code>environment.yml</code>.</p>"},{"location":"icechunk-python/notebooks/demo-s3/#create-a-new-zarr-store-backed-by-icechunk","title":"Create a new Zarr store backed by Icechunk\u00b6","text":"<p>This example uses a S3 store</p>"},{"location":"icechunk-python/notebooks/demo-s3/#real-data","title":"Real data\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-s3/#write-to-icechunk","title":"Write to icechunk\u00b6","text":""},{"location":"icechunk-python/notebooks/demo-s3/#open-store","title":"Open store\u00b6","text":""},{"location":"icechunk-python/notebooks/memorystore/","title":"Memorystore","text":"In\u00a0[1]: Copied! <pre>import icechunk\nimport zarr\n</pre> import icechunk import zarr <p>Lets create an in-memory icechunk store</p> In\u00a0[2]: Copied! <pre>store = icechunk.IcechunkStore.create(\n    storage=icechunk.StorageConfig.memory(\"\")\n)\nstore\n</pre> store = icechunk.IcechunkStore.create(     storage=icechunk.StorageConfig.memory(\"\") ) store Out[2]: <pre>&lt;icechunk.IcechunkStore at 0x10c191110&gt;</pre> <p>Ok! Lets create some data!</p> In\u00a0[4]: Copied! <pre>group = zarr.group(store=store, overwrite=True)\ngroup\n</pre> group = zarr.group(store=store, overwrite=True) group Out[4]: <pre>Group(_async_group=&lt;AsyncGroup &lt;icechunk.IcechunkStore object at 0x10c191110&gt;&gt;)</pre> In\u00a0[5]: Copied! <pre>air_temp = group.create_array(\n    \"air_temp\", shape=(1000, 1000), chunk_shape=(100, 100), dtype=\"i4\"\n)\nair_temp\n</pre> air_temp = group.create_array(     \"air_temp\", shape=(1000, 1000), chunk_shape=(100, 100), dtype=\"i4\" ) air_temp Out[5]: <pre>&lt;Array &lt;icechunk.IcechunkStore object at 0x10c191110&gt;/air_temp shape=(1000, 1000) dtype=int32&gt;</pre> In\u00a0[6]: Copied! <pre>async for key in store.list():\n    print(key)\n</pre> async for key in store.list():     print(key) <pre>zarr.json\nair_temp/zarr.json\n</pre> In\u00a0[7]: Copied! <pre>air_temp[:, :] = 42\n</pre> air_temp[:, :] = 42 In\u00a0[8]: Copied! <pre>air_temp[200, 6]\n</pre> air_temp[200, 6] Out[8]: <pre>array(42, dtype=int32)</pre> <p>Now that we have set the values, lets commit</p> In\u00a0[10]: Copied! <pre>snapshot_id = store.commit(\"Initial commit\")\nsnapshot_id\n</pre> snapshot_id = store.commit(\"Initial commit\") snapshot_id Out[10]: <pre>'SVSF6Y1CR9DX61D4MHFX6WB4JG'</pre> <p>Okay now we can change the data</p> In\u00a0[11]: Copied! <pre>air_temp[:, :] = 54\n</pre> air_temp[:, :] = 54 In\u00a0[12]: Copied! <pre>air_temp[200, 6]\n</pre> air_temp[200, 6] Out[12]: <pre>array(54, dtype=int32)</pre> <p>And we can commit again</p> In\u00a0[13]: Copied! <pre>new_snapshot_id = store.commit(\"Change air temp to 54\")\nnew_snapshot_id\n</pre> new_snapshot_id = store.commit(\"Change air temp to 54\") new_snapshot_id Out[13]: <pre>'XGSEWCBDBRT4VE69BXC2JPV9MC'</pre> <p>Cool, now lets checkout the original snapshot and see if the value is 42 again</p> In\u00a0[14]: Copied! <pre>store.checkout(snapshot_id=snapshot_id)\nair_temp[200, 6]\n</pre> store.checkout(snapshot_id=snapshot_id) air_temp[200, 6] Out[14]: <pre>array(42, dtype=int32)</pre>"},{"location":"icechunk-python/notebooks/version-control/","title":"Version Control with Icechunk","text":"In\u00a0[1]: Copied! <pre>import zarr\nfrom icechunk import IcechunkStore, StorageConfig\n</pre> import zarr from icechunk import IcechunkStore, StorageConfig In\u00a0[2]: Copied! <pre>store = IcechunkStore.create(\n    storage=StorageConfig.memory(\"test\")\n)\nstore\n</pre> store = IcechunkStore.create(     storage=StorageConfig.memory(\"test\") ) store Out[2]: <pre>&lt;icechunk.IcechunkStore at 0x11acf7cd0&gt;</pre> <ol> <li>Why not checkout main by default?</li> <li>Why can I create snapshots on the <code>None</code> branch</li> </ol> In\u00a0[3]: Copied! <pre>root_group = zarr.group(store=store)\n</pre> root_group = zarr.group(store=store) In\u00a0[4]: Copied! <pre>root_group.attrs[\"attr\"] = \"first_attr\"\n</pre> root_group.attrs[\"attr\"] = \"first_attr\" In\u00a0[5]: Copied! <pre>first_commit = store.commit(\"first commit\")\nfirst_commit\n</pre> first_commit = store.commit(\"first commit\") first_commit Out[5]: <pre>'51MXCR5RTNGPC54Z7WJG'</pre> In\u00a0[6]: Copied! <pre>dict(root_group.attrs)\n</pre> dict(root_group.attrs) Out[6]: <pre>{'attr': 'first_attr'}</pre> In\u00a0[7]: Copied! <pre>root_group.attrs[\"attr\"] = \"second_attr\"\nsecond_commit = store.commit(\"second commit\")\nsecond_commit\n</pre> root_group.attrs[\"attr\"] = \"second_attr\" second_commit = store.commit(\"second commit\") second_commit Out[7]: <pre>'45AE3AT46RHZCZ50HWEG'</pre> In\u00a0[8]: Copied! <pre>store.snapshot_id\n</pre> store.snapshot_id Out[8]: <pre>'45AE3AT46RHZCZ50HWEG'</pre> <p>Here's where we are:</p> In\u00a0[9]: Copied! <pre>store.snapshot_id, dict(root_group.attrs)\n</pre> store.snapshot_id, dict(root_group.attrs) Out[9]: <pre>('45AE3AT46RHZCZ50HWEG', {'attr': 'second_attr'})</pre> In\u00a0[10]: Copied! <pre>store.checkout(snapshot_id=first_commit)\nroot_group = zarr.group(store=store)\ndict(root_group.attrs)\n</pre> store.checkout(snapshot_id=first_commit) root_group = zarr.group(store=store) dict(root_group.attrs) Out[10]: <pre>{'attr': 'first_attr'}</pre> In\u00a0[11]: Copied! <pre>root_group.attrs[\"attr\"] = \"will_fail\"\nstore.commit(\"this should fail\")\n</pre> root_group.attrs[\"attr\"] = \"will_fail\" store.commit(\"this should fail\") <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[11], line 2\n      1 root_group.attrs[\"attr\"] = \"will_fail\"\n----&gt; 2 await store.commit(\"this should fail\")\n\nFile ~/Developer/icechunk/icechunk-python/python/icechunk/__init__.py:261, in IcechunkStore.commit(self, message)\n    255 async def commit(self, message: str) -&gt; str:\n    256     \"\"\"Commit any uncommitted changes to the store.\n    257 \n    258     This will create a new snapshot on the current branch and return\n    259     the snapshot id.\n    260     \"\"\"\n--&gt; 261     return await self._store.commit(message)\n\nValueError: store error: all commits must be made on a branch</pre> In\u00a0[12]: Copied! <pre>store.branch\n</pre> store.branch In\u00a0[13]: Copied! <pre>store.reset()\n</pre> store.reset() In\u00a0[14]: Copied! <pre>assert store.snapshot_id == first_commit\n</pre> assert store.snapshot_id == first_commit In\u00a0[15]: Copied! <pre>store.new_branch(\"new-branch\")\n</pre> store.new_branch(\"new-branch\") Out[15]: <pre>'51MXCR5RTNGPC54Z7WJG'</pre> In\u00a0[16]: Copied! <pre>store.branch\n</pre> store.branch Out[16]: <pre>'new-branch'</pre> In\u00a0[17]: Copied! <pre>root_group = zarr.group(store=store)\ndict(root_group.attrs)\n</pre> root_group = zarr.group(store=store) dict(root_group.attrs) Out[17]: <pre>{'attr': 'first_attr'}</pre> In\u00a0[18]: Copied! <pre>root_group.attrs[\"attr\"] = \"new_branch_attr\"\nnew_branch_commit = store.commit(\"commit on new branch\")\n</pre> root_group.attrs[\"attr\"] = \"new_branch_attr\" new_branch_commit = store.commit(\"commit on new branch\") In\u00a0[19]: Copied! <pre>store.checkout(branch=\"main\")\n</pre> store.checkout(branch=\"main\") In\u00a0[20]: Copied! <pre>store.snapshot_id == second_commit\n</pre> store.snapshot_id == second_commit Out[20]: <pre>True</pre> In\u00a0[21]: Copied! <pre>store.checkout(branch=\"new-branch\")\nstore.snapshot_id == new_branch_commit\n</pre> store.checkout(branch=\"new-branch\") store.snapshot_id == new_branch_commit Out[21]: <pre>True</pre> In\u00a0[22]: Copied! <pre>store.checkout(branch=\"main\")\n</pre> store.checkout(branch=\"main\") <p>TODO: use current snapshot_id by default?</p> In\u00a0[23]: Copied! <pre>store.tag(\"v0\", snapshot_id=store.snapshot_id)\n</pre> store.tag(\"v0\", snapshot_id=store.snapshot_id) In\u00a0[24]: Copied! <pre>store.tag(\"v-1\", snapshot_id=first_commit)\n</pre> store.tag(\"v-1\", snapshot_id=first_commit) In\u00a0[25]: Copied! <pre>store.checkout(tag=\"v-1\")\nstore.snapshot_id == first_commit\n</pre> store.checkout(tag=\"v-1\") store.snapshot_id == first_commit Out[25]: <pre>True</pre> In\u00a0[26]: Copied! <pre>store.checkout(branch=\"main\")\n</pre> store.checkout(branch=\"main\")"},{"location":"icechunk-python/notebooks/version-control/#version-control-with-icechunk","title":"Version Control with Icechunk\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#create-a-new-zarr-store-backed-by-icechunk","title":"Create a new Zarr store backed by Icechunk\u00b6","text":"<p>This example uses an in-memory store.</p>"},{"location":"icechunk-python/notebooks/version-control/#snaphotting","title":"Snaphotting\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#concepts","title":"Concepts\u00b6","text":"<ol> <li><code>store.commit</code> creates a snapshot of the data.</li> <li>Every snapshot is associated with a snapshot ID.</li> <li>Use the snapshot ID to time-travel within your data's history.</li> </ol>"},{"location":"icechunk-python/notebooks/version-control/#create-a-snapshot","title":"Create a snapshot\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#view-the-current-snapshot-id","title":"View the current snapshot ID\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#time-travel-to-a-snapshot","title":"Time-travel to a snapshot\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#snapshotting-is-only-allowed-at-the-tip-of-a-branch","title":"Snapshotting is only allowed at the tip of a branch\u00b6","text":"<p>TODO: need better error message</p>"},{"location":"icechunk-python/notebooks/version-control/#branching","title":"Branching\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#view-the-current-branch","title":"View the current branch\u00b6","text":"<p>TODO: why is this None</p>"},{"location":"icechunk-python/notebooks/version-control/#create-a-new-branch","title":"Create a new branch\u00b6","text":"<p>We will create a new branch starting at <code>first_commit</code></p>"},{"location":"icechunk-python/notebooks/version-control/#switch-branches","title":"Switch branches\u00b6","text":"<p>Use the <code>branch</code> argument of <code>checkout</code> to switch to a different branch</p>"},{"location":"icechunk-python/notebooks/version-control/#tagging","title":"Tagging\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#creating-a-new-tag","title":"Creating a new tag\u00b6","text":""},{"location":"icechunk-python/notebooks/version-control/#time-travel-to-a-tag","title":"Time-travel to a tag\u00b6","text":"<p>Pass the <code>tag</code> argument to <code>checkout</code></p>"}]}